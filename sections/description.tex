\section*{Project Description}

\iflater

\todo{GPG: Proposals should identify a computer and information
  science and engineering grand challenge and an agenda to tackle such
  a challenge.} \bcp{Use the word ``agenda''!}

\todo{GPG: proposals should explain why existing CISE core programs
  for Small and Medium size projects
  (\$600,000 over three years and \$1,200,000 over four years,
  respectively) are inadequate to address the grand challenge. A
  strong rationale must be provided that explains why a budget of this
  magnitude is required to carry out the proposed work.}

\todo{GPG: The proposal must explicitly identify the participating
  CISE core programs it covers and make the case for why the challenge
  is within the scope of one or more of these participating core
  programs.} \bcp{I put it in the Project Summary -- it's a bit
  clunky, but I think it serves.  And no one can say that we didn't
  satisfy the requirement!}

\fi
Testing plays a vital role in modern software development,
contributing crucially to robustness, security, and overall software
quality\iflater\bcp{Would love to expand that a little.  Readers
  really need to be convinced that testing is important enough to keep
sinking research money into.}\fi; it also represents
a significantly fraction of the cost of developing
software\iflater{Numbers? one more sentence about costs would
  be good.}\fi{}.

\todo{Incorporate something like this (from John):
  \begin{itemize}
  \item ``Software bugs are
  notoriously commonplace; a 2002 report for Congress estimated their
  cost to the US economy at almost \$60 billion per year—around
  0,6\% of GDP. Software developers
  devote around half their effort to testing their software, in order
  to detect and eliminate most bugs before the software is
  released. The same report estimated that improved techniques for
  testing could save the US \$22 billion per year.''
  \item ``In 2002, the National Institute of Standards and Technology
  estimated the total cost of software errors to be almost \$60 billion
  per year, in the US alone11. It follows that improved testing
  techniques have the potential to save society very large sums; the
  same report estimated that better testing infrastructure could save
  \$22 billion of those \$60 billion, corresponding to 0.2\% of the
  entire GDP of the USA at that time. Since then the volume of
  software, and our dependence on it, has only grown. How much of
  these savings can be realised using property-based testing is, of
  course, a matter for speculation—but clearly, the problem PBT
  addresses, of improving software quality, is critically important to
  society as a whole."
  \item The report: \url{https://www.nist.gov/system/files/documents/director/planning/report02-3.pdf}
  \item Title of the report: The Economic Impacts of Inadequate
  Infrastructure for Software Testing
  \end{itemize}
}

Testing comes in many styles---unit testing, integration testing,
performance testing, stress testing, accessibility testing,
etc.---supported by many sorts of tools, with even more advanced tools
and techniques continually being developed, studied, and applied.

One such technique, {\em property-based testing} (PBT), has been
enthusiastically taken up by the functional programming research
community since its introduction in Haskell's QuickCheck
library~\cite{ClaessenHughes00} and is beginning to make
serious inroads beyond academia.
%
PBT is sometimes glossed as ``formal specification without formal
verification'': a developer characterizes the desired behavior of
some component in the form of executable {\em
  properties}, which are simply Boolean-valued
functions\iflater\bcp{that...}\fi. The code is
then validated against these properties by running it many times
with a large number of automatically generated test inputs.
%
PBT tools thus give programmers an efficient way of testing their
code's behavior with a comprehensiveness that is often not
possible with alternative testing methods. \iflater\bcp{Why not with
  fuzzing??}\fi

PBT's combination of rich, high-level specifications with mostly
automatic validation has been shown to be effective at identifying subtle
bugs in a wide variety of settings, including telecommunications
software~\cite{arts2006testing}, replicated
file~\cite{MysteriesOfDropbox2016} and key-value
stores~\cite{Bornholt2021}, automotive software~\cite{arts2015testing}, and a range
of other real-world systems~\cite{hughes2016experiences}. With at
least basic support
now available in pretty much every major programming language\iflater \hg{But not
enough! And not consistent}\fi, PBT has
begun to make significant inroads in the broader software
industry.  The developers of the Hypothesis PBT library in
Python, for example, estimate its user community at around half a
million~\cite{ZacPersonalCommunication}.  \iflater\bcp{There's a book
  about ScalaCheck!  Let's try to estimate the size of the user
  community.  And we should get in touch with those people!}\fi

\todo{More evidence of impact:
  \begin{itemize}
  \item In 2010, Koen Claessen and John Hughes received the ACM
  SIGPLAN ``Most Influential Paper of ICFP 2000'' award for their
  paper introducing QuickCheck. Today, this paper has over 1500
  citations on Google Scholar---the most cited ICFP
  paper ever (by a factor of 2), according to ACM's digital library.
  \end{itemize}
}

\newcommand{\participant}[1]{{P#1}}
% \newcommand{\participant}[1]{{\bf P#1}}

\amh{This is a deficit-focused pitch---focusing on the problems that need to be 
solved with PBT---rather than an asset-focused pitch---focusing on how we need 
new paradigms for PBT. Can we shift the focus of this to stating one big way in 
which innovation in PBT is necessary?}
Given all this momentum, one might wonder whether the research community
has already discovered and addressed all of the challenges that might limit PBT's adoption
in the broader software industry.  Sadly, it has not:
In an ongoing need-finding study with users of OCaml's QuickCheck testing tool
at Jane Street Capital, we have found
consistent enthusiasm for PBT---participants called it it
``obviously valuable'' (Participant \participant{1}),
built their own libraries for it when standard ones were not available
(\participant{8},
\participant{21}), and suggested that ``everyone'' at the company should use it
(\participant{20})---but participants also described {\em usability} issues that
limit PBT's impact on the broader software community, and thus on the safety of
critical software systems.

% Solving these issues
% requires advances in both technical foundations and
% tool design.\iflater\bcp{The last part of that sentence kind of
%   duplicates the first part of the next...}\fi

Taming these usability challenges, and making PBT dominant technique for
demonstrating software correctness, demands insights from both the
programming languages (PL) and
human-computer interaction (HCI) communities.  Chasins et
al.~\cite{chasins_pl_2021} have argued that a research methodology
combining PL and HCI hits a ``sweet spot'' where need-finding techniques identify
current pain points and motivate concrete tools that help programmers write
safe, correct programs. We wholeheartedly agree.
Indeed, the team behind this proposal is uniquely positioned to advance the
state of the art in PL and HCI: PI Head recently co-founded
a new HCI group at the University of Pennsylvania and specializes in interactive
programming environments, while PI Pierce has
published widely on PL topics including PBT.

\iflater
\bcp{Explain why it's grand... :-)  Specifically, explain why the
  resources of a Large grant are needed to address the challenge (why
  small or medium grants would not do it).}
\todo{Webinar: Proposals must be clear about why this kind of scale is
  required for the research to succeed.}
\discuss{Why do we need four PhD students---why can't this agenda be
  accomplished with fewer?  Why do we need an engineer?}
\fi

We propose a comprehensive, interdisciplinary research and
engineering program on
{property-based testing}, bringing the combined power of PL and HCI to
bear to accelerate PBT's
transition into practice.  The planned research,
engineering, and education activities can be grouped into five main
themes: \bcplater{This already feels too detailed, and we still need to write
blurbs for all the other tasks we've added in the past few days.  But
for now I think we should just go ahead and add stuff, and come back
later to presenting a grander summary.}
\begin{enumerate}[noitemsep]
% \begin{itemize}[noitemsep]
% \begin{enumerate*}
\item We will establish a solid \emph{foundation} for HCI-informed research on PBT,
starting from our ongoing need-finding study. We will confirm
this study's findings and further explore usability of PBT via
two survey studies and an observation study
of PBT users {\em in situ} (\sectionref{sec:foundation}).
%
\item We will help developers efficiently define and tune random input
\emph{generators} with novel techniques for generating values
satisfying given preconditions, for test input mutation, and for
example-based generator tuning---all building on a novel
``reflective'' approach to generation (\sectionref{sec:gen}).
%
\item We will empower developers to \emph{specify} properties with new
tools for checking temporal properties over internal program states
and for model-based testing of modularized code and for authoring
properties, plus a comprehensive review of PBT practice to help
students and developers identify situations where PBT is likely to be
most useful (\sectionref{sec:spec}).
%
\item We will build novel tools for {\em interacting} with properties
and generators---tools to help developers locate the causes of test
failures and understand how to improve the distributions of randomly
generated data to be more representative and
comprehensive\iflater\bcp{Blah.}\fi{} (\sectionref{sec:val}).
%
\item We will support the {\em diffusion} of PBT tools and
methodologies from academia into industry through targeted engineering
and education efforts---in particular, supporting and
improving open-source PBT projects, building a
comprehensive IDE for PBT, and developing materials for an
undergraduate computer science curriculum centered around PBT
(\sectionref{sec:diffusion}).
% \end{enumerate*}
% \end{itemize}
\end{enumerate}
\iflater\bcp{Do we need to write explicitly about how education is key not only
  to the diffusion area?  Or is this fine this way?}\fi
Success in all these efforts will require complementary tools from PL and
HCI research. HCI approaches will be used to concretely define
developers' needs, design the user interface to tools, and evaluate
their success, while PL techniques will be used to develop powerful
domain-specific languages and type-based automation to improve the PBT
process\proposecut{, as well as to formally validate these tools}.

The scale of this project---two PIs, four PhD students, and a staff
software engineer---is essential to its success, due to (1) its
tightly integrated research agenda, requiring expertise across PL and
HCI to make significant advances in four connected areas, and (2) its
ambitious goals for broad impact on industrial
practice, requiring the full-time attention of a software engineer.
%
A detailed sketch of responsibilities and timeline can be found in the
Management and Coordination supplement; briefly,
one PhD student will build on our existing user studies to
develop a clear foundational understanding of
PBT from the perspective of usability and human factors, two students
will develop ``back end'' technologies to support usable
PBT workflows, and the fourth student will leverage these technological
advances to build ``front end'' IDE support for PBT. The
research engineer will focus on transferring these
students' research products into an industrial context, improving and supporting
open-source PBT projects with theoretically informed ideas and tools. None of these
projects stands on its own; rather, each supports and informs the
others, together achieving broad and significant impact on software
development practice.
%
The Management and Coordination supplement also describes the metrics
we will use to monitor and measure the success of the various project
tasks.

\forreaders{Please tell us whether you feel convinced, at this point,
  that we have articulated a compelling ``grand challenge'' and an agenda to
  address it.}

\forreaders{Also, please take an especially critical look at the
  Management and Coordination supplement and at what we say
  about coordination and management here in the main project
  description.  NSF cares a lot about this!}

Our plans for Broadening Participation in Computing, described in an
accompanying document, are focused in two areas: (1) expansion of an
existing NSF-supported program that brings undergraduates from
underrepresented groups to Penn for summer research experiences in
programming languages, and (2) increasing diversity in the TA roster
for CIS 1200, Penn's introductory computer science course.

\medskip

The rest of this Project Description describes our research and
tech transfer agenda in detail.  Sections
\sectionref{sec:orientation} and \sectionref{sec:motivation} supply
background on PBT and present the preliminary findings from our
ongoing study at Jane Street that motivate the project.
%
Sections \sectionref{sec:foundation} through
\sectionref{sec:diffusion} outline our plans in each of the
themes listed above.
Section \sectionref{sec:broader-impacts}
discusses the Broader Impacts of the project, and
Section \sectionref{sec:prior} summarizes our prior
NSF-supported work on PBT.

\iflater
\bcp{Somewhere in the intro, I think we should put a snazzy
  infographic of the whole project...}
\hg{I can give that a try, you know I like snazzy infographics. Once we're
settled on the story I'll see what I can do}
\fi

\iflater
\todo{GPG: ``Does the plan incorporate a mechanism to assess
  success?''  We need to talk about this explicitly!}
\fi

\SIMPLESECTION{Orientation: Property-Based Testing}{sec:orientation}

% We begin in this section with some background on PBT and preliminary
% results from our ongoing study at Jane Street

% \subsectionstar{Property-Based Testing}
% \subsectionstar{Orientation: Property-Based Testing}
%
PBT%
% , famously popularized by Haskell's QuickCheck library
~\cite{hughes2007quickcheck}
is software testing method where
executable functions are used as partial
specifications of a component under test. For example, a developer might
write the following property for an \lstinline{insert}
function on binary search trees, taking an arbitrary tree \texttt{t}
and an integer
\texttt{x} as parameters. If the original tree
is a BST, then it should remain
a BST after the insertion of \texttt{x}:
\begin{lstlisting}
  prop_insert_correct x t  =  (is_bst t ==> is_bst (insert x t))
\end{lstlisting}
In general, a property is a function that
accepts a generated
test as input \iflater\hg{``generated test input''? I try to avoid
  calling the inputs ``tests''}\fi
and evaluates to \lstinline{True} if the test passes and
\lstinline{False} otherwise.
% This one, ``\verb|prop_insert_correct|'',
% checks that an insert operation on a binary search tree preserves the
% binary ordering of the tree.
Given a property, the PBT tool generates a
large number of inputs and
checks that the property yields \lstinline{True} for each one; any input
that causes the property to fail is reported as a {\em counterexample}.

Our research agenda focuses mostly on {\em random}
generation~\cite{hamlet1994random}, the dominant approach in PBT
(though many of our tools would also be applicable to alternatives
like enumerative test-case
generation~\cite{DBLP:conf/haskell/RuncimanNL08, leancheck}).  The
surprising effectiveness of random generation can be attributed to the
``combinatorial nature'' of large test cases---the fact that bugs can
often be exposed by a test input that embodies some specific combination
of features, independent of whatever other features may be
present.  For example, a bug might be triggered by a particular
sequence of API calls in a particular order, even when
these are interleaved with other API calls. As a result, testing with
large random inputs often exposes issues much faster than exhaustively
enumerating small inputs.  Techniques like swarm
testing~\cite{groce2012swarm} can further amplify this effect.

To apply PBT to a system or component, the developer first defines one
or more properties that they expect it should always satisfy. Then
they supply {\em random input generators} for the values that the
properties take as input---these are sometimes written by hand, but
often they can be automatically generated, e.g., from the input
type. Next, they check their properties against many generated inputs,
using a test harness provided by their PBT tool. And finally, if
counterexamples are discovered, they inspect them to determine the
source of the bug.  Each of these steps presents opportunities\iflater\bcp{weak!}\fi{} to
improve the user experience of PBT, as we describe in the next
section.

\smallskip

Why go to the trouble of PBT, rather than the more straightforward
example-based testing that is standard across the software industry?
First and foremost because a component can be tested be much more
thoroughly with a property plus many automatically generated examples
than with a small number of examples written out by hand.
% \proposecut{As mentioned
% previously, PBT has an impressive track-record uncovering bugs that other
% approaches had failed to
% find~\cite{arts2006testing,hughes2014mysteries,
% Bornholt2021,arts2015testing,hughes2016experiences}.}
But
PBT is more than just thorough---it is also more general than example-based
testing. For example, Wrenn et al.~\cite{wrenn2021using} observe that example-based testing
of programs whose correctness conditions are {\em relational} (e.g.,
topologically sorting a graph, which might
produce any of a number of correct results) is impossible to do
faithfully; a property-based specification is a better choice in
such cases.
PBT is also
an obvious choice if
the developer already has some semi-formal
specification in mind---for example if they are implementing behavior from an RFC or
other design document---because it provides a clear connection between the
specified behavior and the implementation.
Finally, the
properties required for PBT can also serve as documentation:
participants in our need-finding study (\participant{5}, \participant{21})
talked at length about properties being an ideal way to communicate what a
program is supposed to do.

\iflater\bcp{Talk about why fuzzing is not the answer.}\fi

With all these advantages, one might hope to find PBT on every
software developer's testing toolbelt.  But PBT raises some challenges
as well, as we shall see next.

\SIMPLESECTION{Motivation: A Formative Study of PBT in Industry}{sec:motivation}
%
Our research agenda is strongly informed by preliminary findings from
our in-progress need-finding study at Jane Street.  The purpose of this
study is to understand the usability shortcomings and research
challenges that must be addressed to boost adoption of PBT in
industry. The study data consists of thirty semistructured interviews
with developers who use PBT and maintainers of PBT tools.

Jane Street is
an attractive setting for this study for several reasons.  First and most
importantly, PBT is
already well established at Jane Street, so there is a large
population of people with well-informed opinions on its benefits and
challenges. Additionally, Jane Street famously builds much of of its
software in OCaml, a mostly functional programming language with
a well-engineered PBT library. This unified
ecosystem ensures that developers have access to mature PBT tools,
experience using them in collaborative settings,
and awareness of language-level abstractions necessary
for advanced usage.

As of February 2023, the full complement of thirty interviews has been
completed at Jane Street and a preliminary round of qualitative
analysis is underway; full-scale analysis will begin later in the year.
Findings from the study will be disseminated in a submission to a
top-tier software engineering conference.  We also carried out a
smaller pilot study among Hypothesis users to prepare for the
full-scale one at Jane Street; its results were presented at the
2022 HATRA workshop~\cite{goldstein_problems_2022}.

\subsectionstar{Usability Challenges} The final product of
the ongoing study will be a fine-grained, qualitative description of how
Jane Street developers use PBT, what they need from it, and how the
research community can
help improve it.  While full analysis of the data remains
to be completed, a number themes are already clear, and these form the
backbone of the present proposal.

\newcommand{\proptheme}[1]{{\color{nord-orange} \em #1}}
\newcommand{\gentheme}[1]{{\color{nord-green} \em #1}}
\newcommand{\evaltheme}[1]{{\color{nord-purple} \em #1}}
\newcommand{\edutheme}[1]{{\color{nord-frost4} \em #1}}
A first set of
themes revolves around \proptheme{\normalfont \bf specifications}, i.e.,
properties.
% and the kinds of programs in which they choose to test them.
Since
PBT is often described as a lightweight formal method, one
might imagine that a common challenge would be coming up with the
specifications of desired program behavior. Indeed, in our earlier pilot
study~\cite{goldstein_problems_2022}, some respondents indicated just that:
developers with less experience with PBT
sometimes struggled to \proptheme{Imagine
Properties} or to understand what properties to test (Pilot-\participant{1},
Pilot-\participant{3},
Pilot-\participant{4}, Pilot-\participant{5}).
By contrast, Jane Street developers on the whole reported
little difficulty finding
properties. Rather, most developers applied PBT in
\proptheme{High-Leverage Scenarios} where properties were already
available or straightforward to invent. In the words of participant~9
(henceforth \participant{9}), PBT is particularly easy to apply when
one has ``a really good abstraction with a complicated implementation.''
When asked to speculate, several developers (\participant{3}, \participant{15},
\participant{20}, \participant{22}) guessed
that 80--100\% of Jane Street
developers write programs like this, where properties are easy to find and
PBT is relatively easy to apply. This suggests that an effective way to
boost PBT in industry would be to provide
educational materials and documentation that highlight
real-world applications where it is are a natural
fit. \iflater\bcp{Readers may wonder why we are leading with education
in the motivation for a research proposal.  Is this an argument for
moving the specification section to after generation?}\fi

We also heard \proptheme{Opportunities for Better
Leverage} of specifications, where PBT is not easy {\em yet}, but
could be with a bit more research effort. For example, developers in
both studies (Pilot-\participant{4--6} and Jane Street \participant{7}) complained
that PBT was difficult when code was poorly abstracted.  Further,
more than three
quarters of study participants had used a particular approach to PBT commonly
called \proptheme{Model-Based Testing}.  \participant{3}, an author of PBT tools
at Jane Street, considered better automation and tooling around model-based
testing to be one of the most significant ways improve PBT usability.

A different set of themes concerned the \gentheme{\normalfont \bf generation} of
random inputs for PBT. Developers spoke
highly of the \gentheme{Derived Generators} that can be automatically
inferred from
the OCaml type system (\participant{5} called OCaml's tools for this
``[expletive] amazing'' and \participant{30} called them a ``game changer'').
These generators are already quite good, but they could be better: participants
identified deficiencies both small (e.g., API quirks) and large (e.g.,
derived generators
cannot enforce semantic preconditions).

When derived generators
failed, participants fell back to \gentheme{Bespoke Generators}, which
are far more flexible but proportionally more time-consuming to
build. For example, \participant{20} successfully used a bespoke
generator for XML documents to find significant bugs in their code,
but reported spending ``at least a day'' writing it.
Improving the abstractions available for authoring bespoke generators would
greatly improve the usability of PBT.
%
When a generated input turns out to be a {\em counterexample} that triggers
a property violation, the developer will need to inspect that
counterexample to find
and fix the root problem. Developers often implemented code for
\gentheme{Shrinking} counterexamples to discover smaller,
simpler inputs that trigger the same bug.  \participant{8}
and \participant{21}, who each implemented their own PBT libraries, both
incorporated shrinkers as key components. But
shrinkers need to be customized to particular kinds of data to be most
effective, and they can be time-consuming to build; several
developers
(\participant{16}, \participant{20} \participant{21}, \participant{30})
described constructing shrinker as an opaque and difficult process.
% Ideally, developers would have tools that make the process of implementing
% shrinkers fast and simple.

A further set of themes concerned the
\evaltheme{\normalfont \bf interaction} between developers and their
testing environments---especially the processes they use
for \evaltheme{Evaluating the
Effectiveness} of their tests. Many
wished for
better ways to evaluate their generators and properties, including
feedback on code coverage (\participant{9} and \participant{25}),
mutation
testing~\cite{papadakis_mutation_2018}, and help understanding the
distribution of randomly generated
inputs (\participant{10}, \participant{16}, \participant{16}). Problematically,
while many developers admitted they would benefit from better tools for
evaluating their tests, many seemed to
\evaltheme{Implicitly Trust the Infrastructure} that they did use. \participant{14} actually shipped broken code because
they did not realize their generator had missed important input
examples.
\iflater
\hg{More themes here, now that we're in infrastructure?}
\fi

Finally, our experience with these studies and with our own prior
research products suggests
that significant engineering and pedagogical effort is needed to amplify
PBT's \edutheme{\normalfont \bf diffusion} into the broader community.  Developers in both studies
(Pilot-\participant{1}, Pilot-\participant{4}, JS \participant{3} and
\participant{11}) reported a dearth of \edutheme{Documentation and Examples} for
learning about PBT; rectifying this will help newcomers get started. PBT is also not traditionally
taught in computer science curricula: making it readily available to
developers will required expanded
\edutheme{Classroom Education}. Finally, existing tools for PBT are
inconsistent and under-maintained; if PBT is to become mainstream, we
need to offer \edutheme{Support and Guidance for Open-Source
  Communities}\iflater\bcp{Don't like this title}\fi{} to help our
work and ideas permeate the PBT world.

\iflater
\discuss{How's this? It's a little forced and under-supported, but I think this is good for now.}
\fi

\iflater
\bcp{Needs a rewrite for the new structure... Section 7 is missing!
  Or maybe we can just drop this paragraph?!}%
%
We discuss plans for work that addresses \proptheme{specification
  themes} in \sectionref{sec:spec}, \gentheme{generation themes} in
\sectionref{sec:gen},
  \evaltheme{validation themes} in \sectionref{sec:val}, and \edutheme{education
  themes} in \sectionref{sec:ed}.  To support all of this work,
\sectionref{sec:foundation} describes plans for a further round of
studies to deepen our understanding of PBT needs and opportunities
across a wide range of industrial settings.
\fi

\iflater\bcp{Not sure any more how I feel about the colors.  They are
  not really used in the rest of the document (in particular, we took
  them out of the Management and Coordination figure)...}\fi

% \iflater
% \amh{Let's foreshadow other findings that
% we will publish that are outside of the scope of this proposal, but which might
% be motivating to other researchers, including: developers could benefit from
% additional automated support for deriving generators from types, effectiveness
% budget (i.e., deciding how much time to spend on running PBT tests versus fuzz
% tests), integration with unit testing frameworks, and exploring the use of
% properties as a source of documentation.}
% \bcp{Not sure where is the right place for this discussion (or if
%   there will be space)...}
% \fi

\SECTION{Foundation}{Understanding Needs and Opportunities%
\pagebudget{1}}{sec:foundation}

\iflater\todo{Mention how time-consuming these studies are (give some
  numbers and details).  We've allocated one whole PhD student to this
  aspect of the work.}\fi

The final results from the Jane Street user study\iflater\bcp{We call
  it a user study here, a need-finding study above, and I think a
  foundational study somewhere.  Let's rationalize this.}\fi{} will
paint a clear
picture of the benefits and challenges of PBT in the specific context
of Jane Street and other organizations with similar characteristics.  But to
fully understand the potential impact of PBT across the software
industry---and the factors that may limit its adoption---we need to
cast a wider net.
%
In this section, we describe three planned studies that aim
to produce a comprehensive, actionable
agenda for the latter stages of this project, and beyond.  We propose two
written surveys, one to assess the generality of these needs and obstacles
and one to identify potential for adoption of PBT tools
(\sectionref{sec:survey}), plus an observation study to understand
particular tasks involved in PBT, so as to guide the design of new
interactive tools
(\sectionref{sec:observations}).

\SUBSECTION{Cognitive theory of PBT}{sec:cogtheory}{1}{2}{HCI
  Theory}{PhD 1}{Everyone}{Head}
%
% Theories in software engineering include information foraging theory; Ko and
% Myers' framework for debugging failures, the attention investment model,
% beacons, and the notion of schemas/plans. The role that theory plays in tool
% development is that it allows us to reason about what influences user effort, 
% explicitly indicating constructs that lead to effort spent, and identifies
% as components that can be improved. Our theory will describe how a system
% to be adequately assessed, what it means for a system to be inadequately
% assessed, and how a system comes to be inadequately assessed. We will call
% the "untested system" theory.
%
\underconstruction{\ifnext Write me Andrew\fi
  \begin{itemize}
    \item Theories in programming are useful for communicating what is
      challenging about programming, and for generating ideas for what can be
      done to overcome those challenges. Consider, for instance, the information
      foraging theory developed by Burnett, Fleming, and others. This project
      has led to numerous innovative tool-design efforts to support navigation within code.
    \item What would a theory of property-based testing look like?
    \item Constructs would include the specification, the generator, the result,
      and the system under test, and the programmer's plan.
    \item The key relationship between constructs would be the interaction
      between the specification, generator, and the system to generate a result.
      They use this result to come up with a plan.
      Complexity in the specification, generator, and system each lead to
      challenges in developing and executing a plan.
    \item Key hypotheses relate to how changes in the specification,
      generator, and system can lead to ability to formulate, execute, and
      revise a plan. (Write out a couple example hypotheses).
    \item We will develop a preliminary theory and hypotheses in the first two
      years, refining it on the basis of findings that come from the
      interventions (i.e., usability studies) conducted in later years with
      various PBT tools, existing and experimental.
    % \item How do people think about PBT?
    % \item Issues of trust, confidence in the system, tradeoffs, etc.
  \end{itemize}
}

\SUBSECTION{Generalizability of the Jane Street findings}%
   {sec:survey}{2}{3}{HCI Theory}{PhD 1}{}{Head}
%
Preliminary findings from the Jane Street study already reveal a number of
opportunities to improve tools for property-based testing. To identify
others and to better understand which are most
critical, we will conduct two
surveys with broader samples of developers. These surveys aim to
(1) determine which obstacles observed in the interview
study\iflater\bcp{sigh: another term :-)}\fi{}
represent widely experienced pain points and
(2) understand the potential benefits of better tools for the
software industry as a whole.

% \TASK{Validation survey}{1}{1}{Who?}
The main survey
aims to confirm (or disconfirm!) that
the things we are learning from Jane Street generalize to other settings.
We will
ask developers which of the issues we found at Jane Street are
ones they have also encountered, which are most severe
in their experience, and what
other issues they have encountered.
To provide
clear usage scenarios to guide research, respondents will
be asked to write brief anecdotes elaborating on the
most severe issues they remember.  Other questions will assess how
heavily respondents depend on specific features of their PBT tools
that may be enabled by their
ambient programming environment and language (e.g., whether their
language supports Haskell-like typeclasses or OCaml-style
metaprogramming, both of which are used to good effect by
PBT tools in those languages).

Respondents will
be recruited broadly, from three sources: (1)
users of the Python Hypothesis framework; (2) Jane Street, aiming for
a broader set of developers in the interview study; and (3)
word of mouth and via announcements on the PIs' Twitter and Mastodon
accounts, various mailing lists, and discussion boards for developer
conferences---e.g., ``Yow!'', where PI Pierce spoke this
year~\cite{Pierce:Yow22}.
% Types and types-announce mailing lists. They are mailing
% lists for the types programming. We will distribute announcements among
% attendees of conferences like ICFP or ``Yow!'' whom have
% a cumulative thousands of programmers with functional
% programming experience.

% \TASK{Impact survey}{2}{2}{Who?}
A second, more speculative, written survey later in the project will investigate
how broadly PBT may {\em eventually} be able to reach.  To get a sense
of this, we will
survey ``proximal'' users of PBT---developers who do not use PBT
currently but who might find it particularly
useful.
\proposecut{Particular attention will be given to those whose work requires
writing validation code and public APIs with some definition of types\bcp{Huh?}.}
We
will recruit a broad sample of participants from varied settings
(professional, open source, educational) by working with our industry contacts
and recruiting over social media.
%
\proposecut{This survey will be more
difficult to ``get right'' than the first, and its design will
accordingly evolve over time as we gain a
better understanding of situations where PBT is most effective
(see \sectionref{sec:whento}), but we believe it can provide invaluable
feedback to PBT researchers about how far their work can reach.}
\iflater \hg{We should be more confident with this task---we're asking for
plenty of money, let's do it.} \fi

% \SUBSECTION{Usability of PBT Frameworks}%
\SUBSECTION{Dimensions of PBT infrastructure design}%
  {sec:frameworks}{3}{4}{HCI Theory}{PhD
1}{PhD 3}{Head}
%
Anecdotes related in interviews do not, in themselves, provide enough
information to inform the design of effective tools.  We will need a more
fine-grained understanding of the design trade-offs inherent in PBT frameworks
in order to extend and improve those frameworks.

Of course, comparing all frameworks in all languages pairwise is
infeasible and unhelpful; instead, we will identify several major dimensions along which PBT
frameworks differ and evaluate how differences along those dimensions impact
usability. For example, some frameworks, like OCaml's QuickCheck, lean heavily
into automating random data generation, whereas Haskell's Hedgehog framework is designed
to be more manual in the hopes of making users more intentional about their
testing; which of these approaches is preferable? Similarly, some frameworks
implement properties as \proposechange{basic predicates in the host
  language}{arbitrary host-language functions}, while others provide
a rich domain-specific language of properties; do such property
languages make a positive difference?

To facilitate answering these questions, we will design and implement a minimal,
modular PBT framework that can be tuned along the dimensions we want
to study. \iflater\bcp{Can we give a bit more detail about what this
  language will be like?  How is it related to the ``Reference
  implementation'' idea we talk about elsewhere?}\fi
We will then give users testing tasks and measure the speed, effectiveness, and
confidence with which they performed their testing. In this way, we will
generate design recommendations that we can use as evidence to help the
communities standardize on usable designs.

\SUBSECTION{PBT interaction models}{sec:observations}{4}{5}{HCI
   Theory}{PhD 1}{}{Head}
%
Our higher-level studies will also not answer granular questions that might
inform more traditional programming tool design, for example:
(1) How much time
are participants willing to devote to
creating a property or tuning a generator when in the
middle of a programming task? (2) How much space is available on a developer's
screen (amidst other tools like code editors and terminals) for
new
tools? and (3) What are developers' current strategies for solving the
problems they describe---e.g., what representations of generated data
currently seem most helpful
for programmers trying to understand the distributions of data generated by a
generator?

% \TASK{Observation study}{2}{2}{Who?}
As a basis for designing tools with a substantial novel interface
component (see \sectionref{sec:val}), we will observe developers
undertaking PBT tasks. These observations will allow us to evolve
singular anecdotes from the interviews into a more comprehensive picture of
the kinds of designs that will help programmers.

\SECTION{Generation}{Better Tools for Random Inputs\pagebudget{3}}{sec:gen}
%
Our Jane Street study also highlighted several \gentheme{generation themes}.
Indeed, generators were regularly cited as one of the most challenging
aspects of PBT: the existing tools related to random generation are varied
and powerful, but
they are not especially usable.  In this section, we propose {\em reflective
generators}, an abstraction for random generation that we have been
experimenting with at Penn, as a way of unbundling generator
functionality, exposing levers for
automation that enable a number of new approaches to generator
tooling (\sectionref{sec:reflective}). In particular, reflective
generators enable new
approaches to shrinking and fuzzing
that reduce developer effort and improve testing
effectiveness.

\subsectionstar{Context: What Makes Random Generation Hard}{}{}
As discussed in the Orientation section above, PBT relies heavily on {\em random data
generators}.  Checking a property of the system or component under
test with many randomly generated inputs gives confidence that the property
holds---provided the inputs are ``interesting enough.''
%
However,
many properties that developers want to test have {\em preconditions}
(a.k.a.{} {\em validity conditions} or {\em input constraints}) that
restrict the set of inputs that are interesting for testing. This comes up often
when testing data structures with invariants that must hold in order to apply
the operations being tested. Testing such properties can be problematic, since
many preconditions are difficult to satisfy randomly; if the developer is not
careful, they may waste most of their time budget generating and
discarding invalid inputs.
%
Of course, some fraction of the testing budget should be spent on
ill-formed or nonsensical inputs, but not too much, since these will not
exercise much of its functionality; most tests should be well-formed
(though perhaps unusual) representatives of the sorts of inputs the
system is designed to process.
%
Worse, even with validity accounted for, there are
more insidious ways for generators to under-perform---for example, by
generating many overly similar inputs while ignoring large parts of
the input space.

% The PBT literature partially addresses these concerns, with solutions solving
% one problem at the expense of another and trading off between programmer effort
% and generator power.

Existing approaches to these issues
fall on a spectrum from automatic to manual. The automatic approaches use
various proxies for validity and general ``interestingness'' of
inputs: some, like {\em
fuzzers}~\cite{afl-readme}, try to optimize readily available metrics like code
coverage, others ask users to provide their own metrics~\cite{loscher2017targetedpbt}, and
of course some use machine learning to infer proxies for
validity~\cite{godefroid2017learn, DBLP:conf/icse/ReddyLPS20}. These approaches
are easy to apply and can yield good distribution coverage, but they are rarely
sufficient for testing properties with complex preconditions. Slightly more
manual approaches are based on declarative representations of validity
conditions: for preconditions that are primarily structural, {\em grammar-based
fuzzing} provides a compelling solution~\cite{godefroid2008grammar,
holler2012fuzzing, veggalam2016ifuzzer, wang2019superion,
srivastava2021gramatron}, and for more complex, semantic preconditions,
SMT-solvers~\cite{dewey2017automated, LuckPOPL,
steinhofel2022input} can be used to automatically seek out valid
inputs. These tools are
much better at satisfying easy to moderately complex properties and
much less good at very complex or very ``sparse'' properties. The semi-automatic
category also includes tools for {\em example-based tuning}, a process that
improves realism of inputs by mimicking user-provided
examples~\cite{soremekun2020inputs}; these tools can generate
realistic inputs, but they are again limited in the preconditions they can
satisfy.

\subsectionstar{Context: Monadic and Free Generators}{}{}
The most manual---and most flexible---solutions use hand-built
generators, written in a convenient domain-specific language (DSL).
In  Haskell, where PBT was
first popularized, such DSLs are commonly implemented using {\em
monads\/}~\cite{moggi1991notions}, an elegant design pattern for
expressing effectful (in this case, random and stateful) computations
in a pure, stateless underlying
language such as Haskell. While monadic DSLs are not actually
needed to express generators in
impure languages, some libraries (e.g., in OCaml) still choose to use monadic
abstractions for their generator DSLs.

Monadic generators can implement random data producers of arbitrary complexity
(e.g., for Haskell
programs~\cite{palka_testing_2011}): they are strictly more expressive than
representations like grammar-based generators.  Yet monadic generators are
syntactically constrained in a way that isolates the probabilistic code and
prevents usage errors (like passing the wrong random seed around). As we will
see, the constrained nature of monadic generators also makes them perfect
candidates for sophisticated programmatic manipulation and interesting
generalizations.

To further improve monadic generators, it helps to re-frame generators as {\em
  parsers of randomness}. A generator
operates by making a series of random choices; equivalently, we can think of
it as being {\em given} some random sequence of choices and simply following
those choices to produce a value. This shift of perspective has been
used as the basis for a few implementations of PBT
tools~\cite{maciver2019hypothesis, dolan2017testing}; we were the
first to make it
formal in our paper, {\em Parsing Randomness}~\cite{goldstein2022parsing}.
%
This paper introduced {\em free generators}, which generalize the standard
monadic generator abstraction, demonstrate a formal link between parsing and
random generation, and enable new algorithms that improve
generation modulo validity constraints. Free generators look very
similar to standard monadic generators, but under the hood they behave more like
generator ``plans'' or syntax trees.%
\footnote{\normalsize For experts: Free generators are implemented using {\em freer
monads}~\cite{kiselyov2015freer}, which have been used to great effect in recent
years to capture the structure of effectful computations
(cf.~Interaction Trees~\cite{old:xia2019interaction}). Freer monads represent
monadic computations syntactically by reifying the monad operations
(\lstinline{return} and \lstinline{>>=}) as data constructors. Critically, this
is all implemented within the language (no macros or AST
manipulation). See \cite{goldstein2022parsing} for more details on the
free generator representation.}
In {\em Parsing Randomness} we proved that any free
generator can be interpreted
either as a standard monadic generator or as a source of
random choice sequences plus a parser over those sequences, thus formalizing the
relationship between generators and parsers.

\underconstruction{Add a short summary of the subsections that follow.}

\SUBSECTION{Theory of reflective generators}{sec:reflective}{1}{1}{PL
  Theory}{PhD 2}{}{Pierce}
\iflater
\bcp{I actually wonder whether all three reflective subsections might
  better be collapsed into a single one...}
\hg{I don't mind, as long as it doesn't make PhD 2 feel thin}
\hg{Also, BCP can you read the future work of the reflective paper at some
point and let me know if you think the synthesis angle belongs here
somewhere?}\bcp{Will do.  Adding it might give us an additonal ``PL
Theory'' task.}\bcp{(After reading...) I like it!}%
\fi
Building on the free generator work described above, we are exploring
a powerful generalization based on
monadic generators that can be run {\em backward}.
%
If a generator can be seen as parsing a sequence of choices into a
value, then running the
generator backward should take that value and produce a sequence of choices that
would generate it.  With this in mind, we have identified a
novel class of {\em reflective
generators}---an extension of monadic generators that can be run backward to
``reflect'' on the choices that they might have made to generate a
given test. The machinery
that makes reflective generators work is rather complex,%
\footnote{\normalsize Reflective generators are both monads and {\em
    partial profunctors},
implementing bidirectional programming in the style of Xia et
al.~\cite{xia2019composing}. This approach to bidirectional programming is
related to lenses~\cite{foster2009bidirectional}, but it hides much of the
complexity of bidirectional program composition in the bind operation of the
monad. The result is an elegant programming experience where both directions of
the computation can be written at once, in a type-safe way.}
but, like free generators, their syntax remains close to that of normal
monadic generators.

Note that the backward direction of a reflective generator is not the
same as just remembering the choices it makes as it goes (which is
already done~\cite{maciver2019hypothesis,
  hatfield-dodds_hypofuzz_nodate}). For one thing, a reflective
generator can reflect on choices for inputs it did not actually
produce---all that's required is that it {\em could} have produced
them.  For another, the choices can be structured in different ways (as bit
strings, higher-level choice sequences, choice trees, etc.) depending
on how much information is needed about the dependencies between different
choices.

Reflective generators are interesting because
bidirectional nature makes them useful for more than just generation!
We are exploring two other applications, which we describe
next.

\medskip

{\em Validity-Preserving Mutation.}
Many automated testing algorithms
(especially fuzzing algorithms~\cite{afl-readme}) {\em mutate} values
so as to explore
the behavior of the program in a space ``around'' those
values. Unfortunately, mutation can be
tricky in scenarios where values are subject to complex validity
constraints, since mutation often produces invalid values. Reflective
generators can help here: We can (1) reflect on the choices that lead
to a particular
value, (2) mutate those choices, and (3) re-run the generator with the new
choices, {\em correcting any that
would lead to an invalid value on the fly.} Figure~\ref{fig:mutation}
illustrates how this
algorithm can mutate a binary search tree while maintaining validity,
using no BST-specific code beyond the reflective generator itself.
\begin{figure}[t]
  \centering
  \includegraphics[width=.6\textwidth]{assets/mutate-diagram.pdf}
  \vspace{-2mm}
  \caption{Validity-preserving mutation of a binary search tree, maintaining the
  BST invariant. Mutating the root node from 6 to 3 invalidates the
  4 in the left-hand subtree; the generator 4 with a new random label,
1, then throws away its left subtree (because 1 is the
minimum element of the label range) and relabels 5 to 2.}\label{fig:mutation}
\end{figure}

\medskip

{\em Example-Based Tuning.} Earlier we pointed out that good generators
produce ``realistic'' inputs; one way to ensure this is to tune the generator so
it produces values that are similar to some user-supplied values deemed
realistic. Existing tools make good use of this example-based approach to
tuning~\cite{soremekun2020inputs}, but they do not work with generators as
powerful as monadic generators. We implement a similar algorithm using
reflective generators: we can (1) again, reflect on the choices that lead to a
set of realistic values, and (2) run the generator with {\em new choice weights}
informed by the choices that we saw.

Both of these applications have the potential to significantly improve
testing effectiveness---example-based tuning helps users generate more
realistic inputs, while validity-preserving mutation enables more
automated approaches to improving generator distributions---and we get
{\em both} by upgrading our generators to reflective ones. We
will see additional use cases for reflective generators in the
following sections, and discuss tools that leverage them later in the proposal.

\smallskip

We began thinking about the basic concept of reflective generators during a
currently running NSF project (described in \sectionref{sec:prior});
to date, we have built a
prototype implementation and drafted a preliminary progress report
laying out their basic theory~\cite{Frohlich2022}; we are working now
on a larger conference submission on these results.  Significant details of
both theory and pragmatics remain to be investigated, however; in particular,
while our exploratory performance measurements are promising, a
thorough empirical evaluation is still needed; this forms part of the work during the first
year of the current proposal.
\bcp{The relation between what's done and what's not done could be
  clearer.  I've just gone ahead and said we plan to submit a paper fairly
soon; we should say one more sentence about what will be in the paper,
and we should say more explicitly what
will not be in the paper but is needed to achieve the goals of the
present project.}

% \amh{Is there a way to make this status report make the success of
% this project sound more assured (if we think it is)? Some of the
% later projects in the proposal depend at least in part on this
% working.}\bcp{Tried tweaking the wording a bit.  I think it's OK.}

% \SUBSECTION{Reflective Shrinkers}{sec:shrinking}{3}{4}{PL
%   Practice}{PhD 2}{Engineer}{Who?}
%
\iflater\bcp{Make sure the lowest-level paragraph headers are all
  formatted the same.}\fi
\smallskip{\em Reflective shrinkers.}
Reflective generators' ability to run backward means that they can be used as
a tool for analyzing and manipulating the structure of
generated
inputs.  For example, we can use them to
implement validity-preserving {\em
shrinking} of values to find smaller counterexamples and speed up debugging
without no effort from the user. On its face, this feels similar to
validity-preserving mutation: Can we reflect on choices, shrink these
choices, and
then re-run the generator with the smaller choices? Likely yes! But there are
complications.

Shrinkers need to be more careful than mutators to keep shrunk values faithful
to the original. When mutating, it is generally fine if the mutated value is
occasionally quite different from the original, since the mutator is
trying many values and any that catch a bug are equally good. But a rogue
shrinker runs the risk of confusing the developer. For example, if a
reflective shrinker accidentally creates a visually larger
value---for example, because the relationship between choice sequence length and
output value size is not monotonic---then shrinking might actually make it more
difficult to understand a failing test case. Additionally, if a
reflective shrinker's shrunk choices
produce values that are very different from the original, they may all
avoid triggering the bug, misleading the shrinking algorithm into thinking that
the original value is already as shrunk as possible and causing it to
report a confusing counterexample.

% \TASK{Foundations for reflective shrinking}{2}{2}{Who?}
To establish reflective shrinkers as a solution to
validity preserving-shrinking, several important
properties need to be established: for some class of reflective
generators, the associated reflective shrinkers (1) never produce larger values
with fewer choices and (2) always produce shrunk values that are ``close'' to
the original. Formalizing sub-classes of well-behaved reflective generators and
notions of ``shrinker closeness'' should also yield
other potential applications of reflective generators and shrinkers.

The shrinking tools that come out of this work will use reflective generators to
automatically shrink inputs in an validity-preserving way. They will be able to
shrink values that they did not generate (e.g., counterexamples that come from
production), and they will give strong guarantees about the subtleties of the
shrinker behavior. We hope that these tools will mean that developers no longer
need to be intimidated by the idea of shrinkers: they will simply work in the
background, simplifying debugging.

\smallskip{\em Reflective fuzzers.}
% \SUBSECTION{Reflective Fuzzers}{sec:fuzzing}{2}{3}{PL Practice}{PhD 2}{Engineer}{Who?}
{\em Fuzzers} like AFL~\cite{afl-readme} are based on principles
similar to the
ones behind PBT: in particular, they use randomization to exercise many
program behaviors. Fuzzers are popular because they are both capable and
inherently easy to use. The developer need only point the fuzzer at an
executable binary and
wait. But, without help, fuzzers are not very good at finding
bugs when properties have complex preconditions.
Our ultimate goal is a unification of PBT and fuzzing tooling that combines the
powerful automation potential of reflective generators with the usability of
fuzzers.

There are a few existing projects that try to get the best of both worlds by
combining PBT and fuzzing.
For example, the FuzzChick library in Coq~\cite{OLDlampropoulos19fuzzchick}
uses code coverage as guidance for PBT, and the HypoFuzz library uses a
similar approach in Python~\cite{hatfield-dodds_hypofuzz_nodate}. These projects
are demonstrably powerful, but neither benefits from the years of expertise
poured into industrial-strength fuzzers; Crowbar, on the other hand,
does~\cite{dolan2017testing}. Crowbar uses
AFL~\cite{afl-readme}, one of the best-established
fuzzers, to generate random bit-strings that are later parsed into program
inputs. Crowbar does require more user effort than standard fuzzing techniques,
but the coverage guidance means that careful tuning is often not required to get
good testing performance.

% \TASK{Build a relective fuzzer}{2}{3}{who?}
We admire Crowbar, but think the idea can be pushed further by building
a variant of Crowbar on reflective generators.
We will start with a classic fuzzing setup, which tries to make the
system under test
crash by passing it a variety of semi-random inputs. Normally, the fuzzer is
``working against'' the parser, in the sense that the parser's job is to reject
invalid inputs and the fuzzer's job is to get past it.  Crowbar's
generators avoid this adversarial relationship because they are
monadic, and can satisfy parer constraints
parser by construction; ours will also be monadic, but also {\em reflective}.

Why a reflective generator? First, we should clarify why any kind of monadic
generator is preferable to grammar-based alternatives. Monadic generators can
straightforwardly generate context-free structures, so they can generate data
satisfying a strictly larger set of constraints than grammar-based ones.
But the beauty of a monadic
generator is that it can be made far more powerful, incrementally, as the
developer's testing needs change. They can start off thinking that they
need only consider the structure of their inputs, but later add more
semantic guarantees (using the extra flexibility of the monadic
presentation) if they find that their generator is wasting too much time
producing invalid values.

A compelling benefit of reflective generators in this scenario is that
their backward interpretation can be used to help seed the fuzzer.
Modern fuzzers often
ask the user for a number of {\em seeds}, input examples that the fuzzer can start from,
to ensure that the fuzzer does not spend ages exploring
inputs that have no hope of exercising the interesting parts of the
system under test. Normally these seeds are easy enough
for the user to write down, since they are simply program inputs, but if
we instead ask the fuzzer to generate sequences of {\em choices}, then
it becomes much more
error-prone (and tedious) to produce seeds by hand.  This is one great use for a
backward interpretation. The user can write down their seeds---either as values
in the program, or as text that can be parsed by the program's parser---and then
the reflective generator can reflect on the choices that produce those seeds.

The reflective generator also provides validity-preserving mutation. Guided by
heuristics, the system can opt to supplement the fuzzer's mutation schedule with
mutations that are obtained by the reflective generator's validity-preserving
mutation (which is more targeted than the mutators provided by the fuzzer). We
expect this to have a significant impact on performance, especially in contexts
where preconditions are relatively sparse and therefore hard for AFL to mutate
correctly.

\SUBSECTION{Usabilility of reflective generators}{sec:reflectiveusable}{2}{3}{HCI
  Practice}{PhD 2}{PhD 3}{Pierce}
Reflective generators are a powerful abstraction, but they are unavoidably a bit
more complex than standard QuickCheck generators. The current design of the
reflective generator language is a best guess at what users will find most
usable (based on our own experience using it) but guessing is far from enough to
ensure usability.

We will evaluate and re-design the surface syntax of reflective generators with
the help of real users.  First, we will recruit a small group of users, teach
them to use reflective generators via a short introductory ``README,'' and ask
them to implement a number of PBT generators inspired by both the research
literature and real-world examples. We will observe the users to identify points
of friction, interviewing them to ensure a clear understanding of their
impressions. With this information in hand, we will identify potential changes
to the API and test those changes with a new group of users. In this way, we
will ensure that both the problems we tackle and the solutions we settle on are
relevant to potential users.

Ultimately, we hope that this process results in a much more usable language for
reflective generators that is simple enough to be used by even non-expert PBT
practitioners.

\SUBSECTION{Generator automation}{sec:genauto}{4}{5}{PL Theory}{PhD 2}{}{Pierce}
%
Even with an ergonomic and learnable language for reflective generators, the
less code the user has to write the better. Our user studies were very clear
that thinking about generators slows users down and makes PBT more challenging.
But we should not compromise: any automation should help users obtain {\em high
quality}, {\em reflective} generators for use throughout the testing process.

As a first step, we will adapt existing tools for type-based generator
automation to work with reflective generators~\cite{noauthor_lysxia_nodate}.
These generators are only useful for testing properties with easily satisfiable
preconditions, but they are a good starting point for many developers. Beyond
that, we consider automation techniques that will help developers interactively
construct reflective generators that maintain complex preconditions.

Some users will have a standard QuickCheck generator that they would like to use
in situations that require a reflective generator. We plan to assist those
users with tools that automatically synthesize the backward annotations
required to make the generator bidirectional. We will experiment with both
conflict-driven program synthesis~\cite{feng_program_2018} or solver-aided
synthesis~\cite{torlak_growing_2013} to see which more successfully generates
annotations for realistic generators. Both approaches have a clearly defined
notion of success: reflective generators have notions of soundness and
completeness that can be tested via PBT, so each candidate set of annotations
can be quickly and efficiently evaluated.

Going one step further, we will also explore techniques that can synthesize an
entire reflective generator from whole cloth. There are a variety of existing
tools that try to do this for standard generators: Lampropoulos et al. provides
two solutions, one based on on inductive relation specifications of
preconditions~\cite{lampropoulos2017generating} and the other based on an
extended language for properties~\cite{beginners-luck}, and Steinh\"ofel et al.
infer a generator from constraints using
Z3~\cite{steinhofel_input_2022,de_moura_z3_2008}. Unfortunately, each of these
tools falls short of solving the problem of automated constrained generation in
general, and none of them include the tools necessary to produce reflective
generators, so there are significant opportunities to advance this space.

\SUBSECTION{Generator benchmark suite}{sec:benchmarks}{1}{2}{Tech
  Transfer}{PhD 2}{PhD 1}{Pierce}
%
\iflater\todo{(Re)write me Harry and Benjamin}\fi
\iflater\todo{Make sure this feels different from Leo's CAREER}\fi
%
The many papers in the PBT literature demonstrate effectiveness with case
studies, showing that certain bugs in certain systems are caught more quickly
with one too over another. For theoretical advances, this is often sufficient
to demonstrate that the paper is worth publishing, but this kind of evaluation
can be hard to interpret from the perspective of a would-be user. With all of
the new approaches to generation that we are proposing in this document, and
considering our goals around usability, we want to do better.

We will to develop and popularize a robust empirical evaluation framework for
generators and other PBT techniques. Our first contribution will be an
infrastructure for easily and extensibly running experiments.  By ``easily,'' we
mean that we will take on the burden of collecting data and analyzing the
results, exposing to the user library functions for their particular
instantiations as needed. We will evaluate a given tool based on (1) the degree
to which it is able to achieve high code coverage quickly, and (2) the speed
with which it finds bugs that have been pre-seeded in example programs. By
``extensibly,'' we mean that in addition to the two languages (Haskell and
OCaml/Coq), multiple frameworks (QuickCheck, SmallCheck, QuickChick, etc.), and
numerous workloads that we plan to support on release, we will design the
infrastructure so that users can easily add new things along each dimension.

Our second contribution will codify a library of case-studies and examples as
{\em benchmarks for PBT}. Similar suites of benchmarks already exist in the
fuzzing literature~\cite{hazimeh_magma_2021}, but those benchmarks are not
organized around the particular challenges that PBT tools face. In particular,
few of the benchmarks deal with the kinds of complex preconditions that PBT
tools are built to handle. We want to establish a set of challenging tasks that
can serve as a north star for future improvements to PBT generators and
bug-finding strategies (including our own!).

Designs for this project are currently being discussed with Leonidas
Lampropoulos and his group at the University of Maryland. PI Pierce has a long
history of successful projects with Prof.
Lampropoulos~\cite[etc.]{LuckPOPL,goldstein2021dojudgeatest,lampropoulos_coverage_2019,Lampropoulos&18,OLDlampropoulos19fuzzchick}.

\iflater\hg{Is this everything? Probably not...}\fi

\SECTION{Specification}{Widening the On-Ramp\pagebudget{2}}{sec:spec}
To widen the on-ramp to using PBT, developers need help envisioning and writing
specifications. Our pilot study suggested that developers struggle to write
properties about their programs, sometimes due to poorly abstracted code and
other times simply because they fail to see what properties are available to
test. We address the first of these issues in \sectionref{sec:outpurprop} and
the second in \sectionref{sec:interactive}.
%
\underconstruction{We've recently
  moved some sections around -- need to mention them here. }
%
In
\sectionref{sec:counter} we describe a planned tool to help migrate
counterexamples found by property-based tests into conventional regression tests.

That said, a notable finding of the Jane Street study has been that widening the
on-ramp to PBT may be easier if we can point developers towards particularly
high-leverage use-cases for PBT. In \sectionref{sec:whento} we discuss our plans
to catalog and communicate these use-cases for developers. Then, in
\sectionref{sec:automating} we show a way to automate a specific high-leverage
scenario: model-based testing.

\SUBSECTION{Properties over program traces}{sec:outpurprop}{2}{3}{PL
  Practice}{PhD 4}{UGrad/MS 1}{Pierce}
% With current PBT tools, properties can only be expressed over single modules.
% This causes problems when code is organized with functionality crossing module
% boundaries.
A common pain point for developers in our formative research
was that code may not be organized in a way that is conducive to property-based
testing. As a unit testing technology, PBT requires ``units'' of software to
test with clear boundaries. As such, PBT cannot easily apply to programs with
poorly encapsulated global state, or software with leaky or complex abstraction
boundaries.

Could there be a class of PBT tools that supports checking properties of
software in cases where important functionality falls across module boundaries?
We will explore how a PBT tool can support the definition of properties over
cross-module event logs. To demonstrate the potential of such a tool, consider
the case of a developer we interviewed, \participant{7}. P7 was testing
a system, which we call ``\lstinline{Inner},'' exemplifying this messy
abstraction
boundary. It was difficult to test \lstinline{Inner} because
its most interesting behavior arose only when interacting with its calling
component (which we call ``\lstinline{Outer}'').
\lstinline{Outer} took in simple inputs, and used these inputs to construct
complex inputs to \lstinline{Inner}. \lstinline{Inner} could not be tested with
realistic inputs without the complex apparatus of \lstinline{Outer} that
produced those inputs. The developer named this as a circumstance where a
PBT is difficult to apply, because the abstraction boundary between the
components is too fuzzy.

% \TASK{Tools for testing properties over logged values}{2}{2}{Who?}
We will develop a tool that allow developers to express properties that reach
into the state of multiple interacting software components. Take again the
example of \lstinline{Inner}. A solution to P7's problem could be to drive the
test through \lstinline{Outer}, and in doing so implicitly test
\lstinline{Inner}, monitoring runtime values within \lstinline{Inner}. Assuming
\lstinline{Inner} is a routine that processes a stream of messages with internal
values including a queue of processed items \lstinline{processed}, and an
\lstinline{overflow}, properties could include:

\begin{lstlisting}
prefix_of processed (next (processed))       (* 1 Processing is monotonic *)
msg.length < 100 ==> never (overflow = true) (* 2 Can process >= 100 bytes *)
eventually (processed.length = msg.length)   (* 3 whole message processed *)
\end{lstlisting}

Is it possible to check such behaviors with built-in assertion statements? We
posit that an assertion-based approach is non-ideal. On the one hand, assertions
would require nontrivial code to save previous values for properties like (1)
above, and likely involves brittle macros or meta-programming to remove such
auxiliary code at release time. Furthermore, it is difficult to imagine using
the assertion approach to checking properties (2) or (3) at all, because it
would require assertions cognizant of the ``end'' of a computation, information
that may not be available to the assertion statement.

Instead, we propose tooling where properties can be defined over logged values.
The first key component of the tool is the invocation of
logging for specific values using lightweight annotations within modules like
\lstinline{Inner}. The second component is a language for writing temporal
properties over those capable of capturing concepts like \lstinline{next} or
\lstinline{never} from the example properties above. Notably, for such temporal
properties, the property language will require logical connectives not typically
found in PBT. We propose adapting linear temporal logic (LTL) to capture
temporality. We will not be the first to use LTL within PBT; the Quickstrom
framework~\cite{oconnor_quickstrom_2022} uses LTL properties
for specifying and testing graphical user interfaces. That said, considerable
finesse will be needed to adapt LTL to properties at the level of generality
envisioned above; this will be one of the major efforts of the proposed work.

Ultimately, this work will lead to a tool where properties are expressed in a
property language extended with LTL; the system compiles LTL formulae to
properties over log traces; the developer provides inputs to an outer system
\emph{containing} the system(s) of concern; and then the test harness will check
the given trace properties in real time.

\todo{Mention the ``Little Tricky Logics'' paper and say we need to
  consider what logic is really appropriate.}

\SUBSECTION{Mixed-initiative property
  specification}{sec:interactive}{3}{4}{HCI Practice}{PhD 4}{PhD 3}{Pierce}
\iflater\bcp{This is colored ``HCI Practice'' but listed under Education.
  That seems confusing.  Maybe move it to Specification?}
\fi
%
Even when a developer knows when properties are useful in the abstract,
they may still have trouble writing their first properties
for real projects. We plan to develop prototype interactive tools that help
programmers compose their first properties in an educational setting.

The vision of this work is to provide a mixed-initiative~\cite{ref:allen1999mixed}
tool, where a student and their code editor work together to arrive at a
meaningful property. Extending prior work on property extraction techniques (see for
instance~\cite{ref:ammons2002mining, ref:le2018deep, ref:claessen2010quickspec,
smith_discovering_2017}), the idea is to help students compose properties that
are not just accurate descriptors of the system, but significant in reflecting
important program behavior. We will build an interface on top of a property
extractor which allows students to write their first properties which supports a
student by allowing them to (1) identify areas of code likely to lead to adverse behavior;
(2) providing unit test cases that represent special cases of more general
properties; and (3) selecting attributes of data types in the source code, or
of input and output data in a debugging REPL, relevant to the property. These
features will allow a student to more rapidly map from behaviors they can
observe or specify in familiar ways, and see how those behaviors are cast into
the language of their property checker.
%
The tool will be designed for use in Penn's upper division Haskell
course (CIS 5520) and therefore implemented on top of
Haskell's property
generator tool, QuickSpec~\cite{ref:claessen2010quickspec}.

\SUBSECTION{Model-based properties for modules}{sec:automating}{3}{4}{PL Practice}{PhD
4}{UGrad/MS 2}{Pierce}
One finding that has surprised us from the Jane Street study is that
it is {very} common for developers to build (or already to have) a
{\em model
implementation} of the code they are testing and check that the two versions of
the same code produce the same results.  This is a well-documented approach to
PBT~\cite{hughes_experiences_2016}, but it is not supported as well as it could
be by existing tooling.
%
With this in mind, we plan to build comprehensive tooling to automate
model-based testing for systems built in languages with strong {\em
  module systems}, like OCaml~\cite{macqueen_modules_1984}.

Model-based testing is essentially trivial in the simplest case where
the component under test and the model are both pure functions.  But
when the code under test is a {\em
  collection} of functions
organized into a module, things get much more interesting. Testing even a simple
module requires orchestrating multiple calls to the different functions in
its signature. For example, testing the signature \lstinline{StringFns} (in
Figure~\ref{fig:sigs}) requires wiring up the functions in the
signature to satisfy their types: testing \lstinline{drop_n} requires a randomly generated
integer to know how much to drop, calling \lstinline{split} produces a pair of strings
that can each be arguments to future function calls, etc. We will build on prior
work~\cite{hughes_experiences_2016} to generate well-typed sequences of function
calls that can be used to compare module implementations.

\begin{figure}[t]
  \begin{minipage}{.45\textwidth}
\begin{lstlisting}
module StringFns : sig
  val reverse : string -> string
  val drop_n  : int -> string -> string
  val split   : string -> string * string
\end{lstlisting}
  \end{minipage}
  \qquad\qquad
  \begin{minipage}{.45\textwidth}
\begin{lstlisting}
module Set : sig
  type 'a t     val empty : 'a t
  val mem   : 'a t -> 'a -> bool
  val add   : 'a t -> 'a -> 'a t
\end{lstlisting}
  \end{minipage}
  \vspace{-2mm}
  \caption{Some module interfaces we would like to test
    automatically.}\label{fig:sigs}
\end{figure}

Testing modules gets even hairier when they contain {\em abstract types}, as is
the case with \lstinline{Set}. Now some of the types in the signature cannot be
generated on their own, so they must be built up from scratch (in this case by
calling \lstinline{create} and \lstinline{add}). The \lstinline{Set} signature is
also {\em polymorphic}, so a type must be chosen for \lstinline{'a} before
testing. (There is significant work on this
problem~\cite{hou_favonia_logarithm_2022}, but it is not solved in general.)

% \TASK{Foundations for testing modules}{2}{2}{Who?}
To solve these problems, we will take a foundational approach, breaking down
modules theoretically and developing automation heuristics from first
principles. This will probably\iflater\hg{Blah}\bcp{Yes.  It would help if we
  could actually name one or two.}\fi{} involve identifying a subset of
modules where automation is {\em not} possible; highlighting that subset will
provide useful context for future work that might aim to simplify these cases in
other ways (e.g., by putting a human in the loop).

The work in this project will provide a basis that others can build on
in other languages. Abstract and polymorphic signatures are features
of many other modularity mechanisms (e.g., Java interfaces, Rust
traits, Haskell type classes, etc.), so solutions we find in OCaml
should transfer. \ifnext \bcp{We are not really promising much here (and not
  giving much detail about what we do promise)... Can we beef it up?
  Maybe add another task?}
\hg{The first problem is that we just don't stick the landing here. We're
planning to build a ``turnkey'' solution to MBT, but we get bogged down in
details and never pop back up. I'm not sure I have any other tasks to suggest...
have any of our conversations with Ernest raised new design decisions /
theoretical concerns we haven't considered?} \fi

\iflater
\bcp{I wonder whether one of the attractions of model-based properties for real-world testing is that they more or less allow one to ignore the issue of "generating structures satisfying preconditions."  That is, if what you're doing to build an "interesting" value of some data structure or internal state of some stateful system is issuing calls through an API, then I would guess in most situations pretty much any sequence of API calls will be legal; some will be more interesting than others, of course, but at least you'll never build a structure that doesn't satisfy the preconditions of your operations -- your whole distribution will always fall within the space of well-formed structures.  (Or, if it observably doesn't, you'll have uncovered a bug.)
Could this help explain why it seems to be so prevalent, at least at
Jane Street?  Is there any way one could test whether this sort of
reasoning is an actual explanation for the observed prevalence?  Is
any of this worth mentioning here?}
\fi

\SUBSECTION{Automatic explanations for properties}{sec:understanding}{3}{5}{HCI
  Theory}{PhD 4}{}{Pierce}
%
\underconstruction{\ifnext \todo{Write me Andrew.} \fi
  \begin{itemize}
    \item You run across some properties in a program---what then?
    \item This has important implications for education, but also for engineers
    \item For sufficiently simple programs in sufficiently limited subset of
      languages, it is possible to develop rule-based methods for generating
      readable, understandable explanations (PythonTutor, Tutorons)
    \item Prior work has been done in converting from logics into natural
      language explanations (work from Sanjit Seshia student), and may also be
      applicable to the kinds of statements that appear in specifications.
    \item The key is to help people both understand what might be unfamiliar
      individual symbols (some PBT frameworks have their own DSL for specifying
      properties), as well as to understand the idea behind the test. The idea
      behind the test might best be shown by selecting simple examples of the
      test case succeeding and similar test cases of it failing next to the
      property.
    \item It might be possible to use LLMs as well to provide descriptions of
      the ``idea'' behind the test; PI Head is investigating this
      in a parallel project and if it works sufficiently well, can incorporate
      it into this project (particularly for frameworks like Hypothesis that
      have properties written in a language like Python that much resembles the
      code that the models were trained on).
    \item ...?
  \end{itemize}
}

\iflater\todo{Tweak the title when we understand the content. :-)}\fi

\todo{Cf. UIST 2015 paper relavant to this.  Mayur (sp?) et al.}

\SUBSECTION{Counterexamples as regression
  tests}{sec:counter}{5}{5}{HCI/PL Practice}{PhD 4}{}{Pierce}
%
% After testing has revealed a counterexample to one of their properties
% with their tests, the developer may wish to turn that failure into a
% regression test, to ensure that later
% changes to the code will not reintroduce the same failure.
One pain
point experienced
by informants in our interviews at Jane Street was that it requires considerable work to
transform a failure that was detected by their PBT tools into a
regression test, despite the fact that much of the work involved in doing so
felt mechanical.

% \TASK{A tool for turning counterexamples into tests}{2}{2}{who?}
We plan to develop usable tooling for transforming failed PBT tests into single
regression tests. What is important to note about this project is that creation
of regression tests is \emph{mostly}, but not entirely, mechanical. In reality,
the creation of regression tests will likely require judicious incorporation of
the developer's input at key decision points. This is particularly the case for
specifying acceptance criteria. For instance, consider a property that checks
that a list insertion function never produces an empty list. In the event of a
failure, a developer may want to produce a regression test checking the
exactness of the result on the failed input (e.g., checking that the insertion
produced a particular concrete list) rather than simply checking that the output
list is non-empty.  Writing a regression test may involve multiple such
choices, including whether to test for exact output, whether to test
intermediate results, and how to initialize inputs.

We will develop an interactive tool that assists developers in creating
regression tests from failed tests. The idea is to first develop
technology for generating sufficiently readable code for regression tests (using
approaches such as Daka et al.'s~\cite{ref:daka2015modeling}), and then provide
in-situ editing assistance along the lines of contemporary interactive
refactoring tools from the HCI
literature~\cite{ref:head2018interactive,ref:barik2016quick,ref:murphyhill2008refactoring,ref:lee2013draganddrop}.
For this unique task of transforming properties into regression tests,
our tool will \iflater\proposecut{provide the key features
  of}\hg{MINOR: Feel free to disagree,
but I think phrases like this are a bit of an anti-pattern. I'd rather just say
the tool will X}\fi keeping track of the failed input,
generating starter test code, substituting in correct expected values of the
output by executing corrected code, and supporting developers in rapidly
performing likely edits to regression tests by providing suggestions of
alterations to their code like the ability to generate general property checks
with precise equality checks\iflater\bcp{??}\hg{I also don't understand what this is
getting at}\fi. We also hope the results of this work
can inform the
design of other approaches to counterexample extraction, including one that is
proposed for the Hypothesis library in Python~\cite{maciver2019hypothesis}.

\SECTION{Interaction}{PBT at Users' Fingertips
 \pagebudget{3}}{sec:val}

In this section, we
describe a sequence of research efforts to design and evaluate usable developer
tooling for PBT. These projects will contribute new paradigms for assessing whether their generators are generating sufficient and
appropriate inputs (\sectionref{sec:evaluating_distributions}) and whether those
inputs sufficiently exercise their code (\sectionref{sec:tuning}),
and understand testing-provoked failures involving complex inputs
(\sectionref{sec:failures}).
%
\underconstruction{Mention 6.4 too.}%
%
These projects will all be pursued using HCI
methodology, integrating these tools into contemporary interactive development
environments. The success of tools will be evaluated in usability studies
comparing developers' effectiveness and efficiency in writing tests for their
software, and debugging problems discovered by their test suites.  PI Head will
guide tool development efforts, leveraging his
experience designing, developing, evaluating, and deploying novel programming
environments and
tools~\cite{ref:head2015tutorons,ref:suzuki2017tracediff,ref:head2017writing,ref:head2018when,ref:head2018interactive,ref:head2019managing,ref:head2020composing}.

\SUBSECTION{Evaluating data
  distributions}{sec:evaluating_distributions}{2}{3}{HCI Practice}{PhD
  3}{UGrad/MS 3}{Head}
%
In comparison to testing techniques like unit testing, PBT draws many
tests from some
random {\em distribution} over possible inputs, rather than writing down a few
concrete individual inputs. The success  of
PBT thus depends on the quality of this distribution---whether
most of its probability mass is on tests that are sufficiently
realistic and diverse. We plan
to develop tooling for
understanding distributions of input data and easily tuning them.

\begin{wrapfigure}{r}{0.58\textwidth}
  \centering
  \includegraphics[width=0.58\textwidth]{assets/gen-vis.pdf}
  \caption{An envisioned tool for evaluating data distributions.
    \todolast{put this near the text that refers to it}}\label{fig:gen-vis}
\end{wrapfigure}

We draw inspiration from related work in HCI that has sought to better expose
the shape of input data distributions including
machine learning datasets
(e.g.,~\cite{ref:hohman2019gamut} and
~\cite{ref:hohman2020understanding}) and sequences of program values
(e.g.,~\cite{ref:kang2017omnicode}).
PBT poses a unique challenge because values produced by generators are
programmatically generated and
can be of unbounded structural
complexity (e.g., lists, trees, and other algebraic data types,
sequences of API calls, ...).
Consider an
example from a participant in our formative study, who wanted to generate
realistic logs of input data, where each log entry included at least a timestamp
and an event type.
% \proposecut{Such values are not trivially plotted in conventional
% visualizations, and it would be prohibitive to review individual examples if the
% logs are sufficiently long.}
Ideally, a developer would be able to answer
questions like: Are the generated log inputs long enough? Are the even sequences
realistic? This setting requires new kinds of views of data, and tight
developer support for easily defining meaningful views of the data.

% \TASK{Tools for working with data distributions}{2}{3}{who?}
We will design and implement new interactive tools that provide rapid,
informative views of input data distributions. The tools will address the
challenges of visualizing generator distributions using a novel combination of
tailored, tried-and-true features for interactive programming environments.

First, our tools will support live, realtime displays of generated values.
Our first goal is to provide
instant, live~\cite{ref:tanimoto1990viva} feedback on the generators. Building
on the tradition of other live functional programming environments
(e.g.,~\cite{tool:lighttable,ref:omar2019live}),
% our environment will provide
% live feedback summarizing many inputs produced by a generator, rather
% than a single one; As testing proceeds,
our environment will sample inputs
output by the generator and
pipe them into data displays (Figure~\ref{fig:gen-vis}), which
will first and foremost show aggregate data views, including aggregate
statistics (Figure~\ref{fig:gen-vis}.2) and visualizations of the distribution
of key features of the data (Figure~\ref{fig:gen-vis}.3). Visualizations will be
generated according to simple recommendation rules, similarly to other recent
exploratory data visualization tools from
HCI~\cite{ref:lee2021lux,wongsuphasawat_voyager_2016,
wongsuphasawat_voyager_2017}. Unique to our project, features to visualize will
be based on awareness of common features of algebraic data types, and extensible
through lightweight user-written code. For example, consider the
\lstinline{log} type
described above. A developer might be interested in the log's
\lstinline{length}, field accessors like \lstinline{event_type}, \lstinline{id},
and \lstinline{timestamp}, filters like \lstinline{is_empty}, and even
aggregators like \lstinline{max_by}. These kinds of features can be
generated automatically for common data types.
% Then the tool will then use
% lightweight type-based program synthesis to compose and combine these functions
% to get features. It may choose to show the \lstinline{length} of a log, but also
% the \lstinline{max_by (fun l -> length l.payload)} (the maximum payload length),
% and even pairs of features like these (which could be viewed as a two
% dimensional feature).

Second, the data displays will be easily extensible, via lightweight customization hooks. If
there are features that the user notices should be extracted, but that the
system cannot come up with itself (e.g., \lstinline{ids_unique}) the user can
write it themselves in companion code alongside their property specifications;
the interface will automatically load those features into the display.

Third, the interface will make it possible to drill down into
individual inputs from a list of samples that can be
filtered by selecting marked visualizations (e.g., choose a bar
of length ``10'' to preview individual inputs with that length). One
challenge will be to provide suitable representations of complex inputs that
will be easy to understand. Some general solutions might be to
pretty-print inputs, provide interactive object browsers like those available
in JetBrains~\cite{tool:jetbrains}, and/or allow a developer to explore an
object using a built-in REPL. Additionally, we will produce DOT
graph~\cite{ellson_graphviz_2002} representations of common kinds of inputs
(i.e., lists, trees) that will provide an at-a-glance understanding of
larger inputs (Figure~\ref{fig:gen-vis}.4).

Finally, our tools will provide live feedback on the input
distribution in the form of in-situ coverage feedback. Like other HCI
prototypes that have shown which lines of code are currently
executing in a running program~\cite{ref:brandt2010rehearse,
  ref:oney2009firecrystal, ref:burg2013record}, ours will be
able colorize code in the editor on the basis of
how frequently each line has been executed while running tests.

The result of these experiments will be the first live tool for generator
feedback and analysis; in the next section, we take it one step
farther to consider how interactive tools can help a developer not
just understand input data distributions, but more directly change
them.

\SUBSECTION{Tuning data distributions}{sec:tuning}{3}{4}{HCI Practice}{PhD
3}{UGrad/MS 4}{Head}
%
Tuning generator distributions is a challenging task, often requiring
significant trial-and-error---changing generator parameters and hoping
that the input data distribution changes in the right way. Approaches
like the one in \sectionref{sec:reflective} can help, but manual tuning
is still needed in many cases.
Developers need higher-level ways to describe the kinds of inputs they.

We will design tools to support the direct tuning of input data distributions
through manipulation of generated inputs and data distribution
visualizations. This work will build on the foundation outlined in
\sectionref{sec:evaluating_distributions}. Inspired by recent tools in
the PL+HCI literature
for bidirectional manipulation of programs and their
outputs~\cite{ref:hempel2019sketch, ref:kery2020mage,
  ref:omar2012active, ref:omar2021filling},
we explore reflective generators' potential to support tuning.

% \TASK{Tuning by filtering}{3}{4}{who?}
The first, general-purpose method for tuning input data distributions will be to
define filters on input data by manipulating aggregate data displays.
For instance, developers will be able to select a range of values from a bar
chart showing input data features and then request that all values generated
within that range are discarded before testing. This approach is flexible,
though it is notably coarse-grained;
it does not influence the implementation of the underlying generator, and
therefore can only go so far in influencing the kinds of values that are
generated.

% \TASK{Tuning by reflection}{3}{4}{who?}
A second idea is to leverage
reflective generators (\sectionref{sec:reflective}) to change the underlying
generator parameters. Developers will be able to interact with a visualization,
and then the the reflective generator will map those interactions
back to choices
in the generator (e.g., what value to place in a node, or the number of nodes to
produce, etc.), which can then be made more or less frequently. We envision
building tools that show the generator code
side-by-side with visualizations, and where parameter choices in the generator
can update live as the data distribution is manipulated in the visualizations.
Furthermore, developers will be able to interact with individual data points,
expressing that they would like to see more inputs like one that has already
been generated, or that they would like an input similar to a generated input,
but different in a way that they have demonstrated. Together, this tool and the
tool for visualizing generated data distributions will have a synergistic effect
in improving developers' ability to understand and ultimately achieve more
realistic, comprehensive data distributions for their testing.


\SUBSECTION{Interactive shrinking and
  debugging}{sec:failures}{3}{4}{HCI Practice}{PhD 3}{PhD 2}{Head}
%
One of the challenges in using PBT is understanding why
a given counterexample triggers a bug.  We will design interactive
tools to help in this process.

% \TASK{Interactive Shrinking}{3}{4}{who?}
First, we will design tools that build on
shrinkers~\cite{hughes_quickcheck_2007,arts_shrinking_2014} to help developers
understand counterexamples. Automatic shrinking, even when done via reflective
shrinkers as we discuss in \sectionref{sec:reflective}, can be opaque, and
shrinkers often make suboptimal decisions that an individual developer might be
able to make better.
Drawing inspiration from approaches in recent HCI literature that support
interactive code reduction through iterative, incremental
experimentation~\cite{ref:lim2018ply,ref:head2018interactive,ref:holmes2012systematizing,ref:hibschman2016telescope},
we will design aids for rapid, incremental, interactive shrinking of complex
inputs into simpler inputs. The key feature we will develop is the ability to
shrink inputs semi-automatically by pruning the input's contents and structure in an interactive
object viewer, similar to the kinds of object viewers available in contemporary
debuggers like in the JetBrains IDE~\cite{tool:jetbrains}.
The interactive shrinker will display the \emph{valid} ways
in which an input can be pruned, but the choice of exactly what to prune will be
up to the human. We will rely on reflective generators to provide the insights
into which parts of the structure correspond to choices that can be pruned, and
these pruning points will be made
visible in the object viewer. As a developer prunes the input, they will receive
continuous feedback as to whether it still causes a test to fail or not.
Developers will ultimately be able to
reduce complex counterexamples into simpler ones that are
easier to reason about when looking for bugs.

% Developers will also be alerted if the execution path
% through the code has changed as a result of shrinking the input. In some
% circumstances, this will indicate an undesirable change in the shrunken input,
% and in other cases, such changes in execution paths may be permissible (e.g., if
% the change in the input has led to a reduction in the number of cycles through a
% loop).

% \amh{Could we indicate on top of an object explorer which
% parts of the structure, if changed, would end up leading to
% a different test result, or preserving it?}
% \hg{To a point... You'd run into exponential blowup pretty much immediately, but
% depending on the scale of the input it could be doable}

% Other related projects:
% Whyline~\cite{ref:ko2009finding}.

% \TASK{Interactive debugging from failing tests}{3}{4}{who?}
In addition to developing novel interaction techniques for simplifying inputs,
we will also develop systems for helping developers locate code that, if
changed, would resolve the failure. Rather than explicitly encoding
relationships between generated outputs and their dependencies on
code~\cite{ref:ko2009finding}, we will instead help the developer
understand where the execution paths of a given counterexample diverges from
successful yet very similar inputs. Leveraging the parametric nature of the
reflective generators, we will generate inputs in a space ``around'' a
counterexample and identify which ones no longer cause a failure.  Then, we will
execute the program up to the point where the traces of the programs begin to
diverge, and we will drop the programmer into a debugging environment where
they can query the state of the program and step through the remainder of the
execution. PI Head has prior work designing debugging tools that help
programmers understand trace divergences in an educational
settings~\cite{ref:suzuki2017tracediff}; the work of this project would be to
bring this technology into professional programming environments where traces
for similar inputs are abundant.

\SUBSECTION{Dashboards for long-running tests}{sec:dashboards}{4}{5}{Tech Transfer}{PhD
3}{Engineer}{Head}
%
\underconstruction{ \iflater\todo{Write me Andrew?}\fi
\begin{itemize}
  \item Given the random nature of tests, it is not guaranteed that the same
    software, when tested twice, will have the same results.
  \item Views that focus on specific, known test cases may then be easier for
    developers to understand quickly than those reporting summary statistics.
  \item Or, dashboards should bring together signals from both random samples
    of inputs, as well as known samples.
\end{itemize}
}
\iflater \hg{I copied a comment into here---can we use any of that writing for this
section?}\fi
% \ifdraft
% \SUBSECTION{Increasing Assurance with More Tests}{sec:more}{3}{4}{Andrew}{}{}

% The informants in our interview study admitted to heuristic
% approaches to determining how many tests to run. Many set
% rigid cutoffs such as one-thousand tests, or one minute's
% worth of tests. One reason for these heuristics was that
% developers did not want to lock up their computer's
% computational resources, or delay their other development
% tasks, by waiting for their property-based tests to
% complete. As one of our informants pointed out, this could
% lead to circumstances where bugs were not discovered until
% property-based tests were checked into the continuous
% integration system, when tests were finally conducted at a
% sufficient volume to generate failing inputs \amh{@HG can
% you fact check my paraphrase of this informant's input?}\hg{Technically this was
% a pilot study informant, but I think it's OK}.
% Unfortunately, when failures are detected in continuous
% integration, developers no longer have the context to
% address those problems easily, because they have likely
% moved on to other tasks.

% We believe that, if appropriately designed, a developer's
% PBT tools can help developers get the best of both
% world: they can both increase assurance that their software
% is correct by running more property-based tests, while not
% locking up their computer's resources. We propose to extend
% in-editor testing tools to permit options where developers
% can configure their PBT test drivers to continue testing
% software in the background by default, without requiring a
% developer's explicit input. These tools will scan a
% developer's environment for property-based tests, divide
% time proportionally between the various tests, and capture
% the results in a digested form. To reduce the likelihood of
% breaking a developer's focus, errors will be marked through
% subtle annotations in the code (e.g., icons in the line
% margins) when one has been detected. Developers can also
% request alerts that can report failures instantaneously,
% should they wish to address any discovered failures right
% away.

% \hg{This section needs honing; right now it's just what I wrote in my thesis
% proposal, and it's too high-level} \amh{Is this a user
% interface project devoted to figuring out ways of invoking
% and keeping tabs on long-running tests during solo
% programming? Or is it a project around proposing better
% software engineering process that gives the right amount of
% time to long-running PBT tests?}
% \hg{Good question. I kind of naively hoped it was a UI project that both helped
% users invoke and keep tabs of tests in the background AND pushed them towards
% software engineering processes that gave PBTs appropriate space to operate. But
% now that I think about it that seems like a lot at once. Honestly I think the
% cultural shift in SE (potentially backed by some large-scale software tools) is
% the more impactful angle, but I don't know if we can argue that we know how to /
% want to do that}
% \bcp{Delete the section?}
% \amh{I tried this section again, trying to spell out in more
% detail my updated conception about what would be useful and
% magical about this tool. Can someone else take a look at it
% and make the decision of whether to remove the section?}
% \hg{I think it's better, but it's still pretty vague and flat. I wish there was
% a specific example of an interaction that I could say ``oh yeah, why don't I
% have that?'' As written, it sort of feels like the devil is in the details and
% we didn't actually give an details}

% % It is likely that PBT tools could play a role in improving this state of
% % affairs. For example, one could take inspiration from some theorem
% % provers~\cite{berghofer2004random} and create a system in which properties are
% % checked locally but in the background, as the programmer works on other things.
% % This avoids waiting time while potentially being less frustrating than running
% % in CI, since bugs would likely be found while the programmer still had the code
% % ``paged in.'' Alternatively, one might design a PBT system with CI in mind,
% % providing automated features for deferring property failure notifications until
% % a specified time or turning failing properties into unit tests that can be saved
% % for future testing.

% \fi

\SECTION{Diffusion}{Advancing Open Source PBT Tooling}{sec:diffusion}

\forreaders{This section collects all the tasks that involve either
  primarily engineering or primarily education.}

\underconstruction{Needs an introduction.}

\SUBSECTION{Tyche: An IDE for PBT}{sec:ide}{2}{5}{Tech
  Transfer}{Engineer}{PhD 3}{Pierce}
\amh{Maybe this belongs at the beginning of this section, and the tools that
come before would be components of \tyche{}}.
The ideas in this section highlight a clear opportunity for PBT tools that live
in and take advantage of the programmer's code editor. We have shown tools that
facilitate more intentional workflows, interact with and improve the
programmer's generator code, assist in the test-authoring and debugging
processes, and provide visibility into long-running testing campaigns---all
without requiring the programmer to context-switch out of their normal workflow.

But there is one more step needed to complete the picture: these tools will be
integrated into a single, extensible IDE for PBT that we call \tyche. The \tyche{}
extension to Visual Studio Code will include all of the projects from this
section, providing tools that help programmers through various stages of the PBT
process. And it will also be a home for new innovations. Future tools that we
or others in the community see as important improvements to PBT workflow will
also be included over time.

Ultimately, we think \tyche{} will become an invaluable assistant in the PBT
process, making it easier for developers to get started and succeed.
\iflater \hg{This is a weak ending} \fi

% \bcp{If we're going to keep this as a top-level theme, we'll need to
%   integrate it into the motivation early on (i.e., give it a color,
%   talk about how the Jane Street study shows we need it, ...).  Many
%   of the specific items fit just fine into the sections we've got (and
%   I moved them there).  But I wonder whether a top level ``Diffusion''
%   or ``Integration'' theme might be helpful for the story, and as a
%   catch-all for the rest.  Thoughts?}

% \hg{I think I like the idea of a section where we can talk about what the
% engineer will do without having to shoehorn it into others' projects. E.g., the
% engineer will theoretically be making changes to Hypothesis based on findings
% from Jane Street, reflectives, etc. but it's hard to talk about open-source
% maintainence in those sections. It will be difficult to justify diffusion based
% on the JS study, but it'll be a no-brainer if we justify it from the perspective
% of the ``grand challenge'' of establishing a cohesive, multi-language ecosystem
% for usable PBT}\bcp{Yes.  I think as we reshape and scale up the
% overall structure, we need to move away from the JS study as providing
% the whole architecture for the proposal: it was a nice conceit for a
% smaller project, but this one reaches too far beyond.  The grand
% challenge is better motivation for the whole thing.}

\SUBSECTION{Nurturing the PBT ecosystem}{sec:nurturing}{1}{5}{Tech
Transfer}{Engineer}{PhD 1}{Pierce}

\underconstruction{\todo{Write me someone}
  \begin{itemize}
    \item Besides supporting the PhD students, we want the engineer to increase
    adoption of PBT broadly by working in the open source community
    \item Many existing PBT tools are unmaintained or need more support
    \item We'll also have a number of design recommendations and fresh ideas
    that should be implemented across many languages and frameworks
  \end{itemize}
}

\SUBSECTION{Beyond research software}{sec:engineersupport}{3}{5}{Tech
Transfer}{Engineer}{}{Pierce}

\underconstruction{
  \begin{itemize}
    \item The engineer will support the PhD projects with design input and
    longer-term maintenance.
    \item Goal is to avoid the bane of research software that never
    actually makes it into widespread usage.
  \end{itemize}
}

% \SUBSECTION{A Minimal Implementation of
%   PBT}{sec:refkey}{3}{4}{Tech Transfer}{Engineer}{}{Who?}


% \SUBSECTION{A Minimal Implementation of
%   PBT}{sec:refkey}{3}{4}{Tech Transfer}{Engineer}{PhD 1}{Who?}

% \SUBSECTION{Reflection in the Wild}{sec:reflpython}{2}{3}{PL
% Practice}{Engineer}{PhD 2}{Who?}

% \SUBSECTION{Extensible Tools}{sec:extensibility}{3}{5}{Tech
% Transfer}{Engineer}{PhD 3}{Who?}
\ifnext
The engineer will provide support, design input, etc. to the students as they
produce research artifacts. When the projects are done, the engineer will
continue to maintain and grow those software projects.

{\em A Minimal Implementation of PBT.}
\todo{They support PhD 1 by...}

{\em Reflection in the Wild.}
\todo{They support PhD 2 by...}

\bcp{We can mention Python if needed, but let's try to keep our aims a
bit broader than just Python.}

{\em Maintaining and Extending Tyche.}
\todo{They support PhD 3 by...}

Building hooks into tools for extension and research experimentation...
\fi

% \SECTION{Education}{Advancing PBT in the Broader Culture\pagebudget{1}}{sec:ed}
% %
% Our goal is to make PBT not only usable but {\em
% used} in settings where it offers benefit; such a goal requires advances in PBT
% \edutheme{education} for
% new and experienced programmers alike. In this section, we describe
% our plans to develop
% course materials for instructors who want to integrate PBT into
% undergraduate data structures courses (\sectionref{sec:1210}),
% %
% tooling to help students write their first properties
% (\sectionref{sec:interactive}),
% %
% and course materials for instructors to integrate PBT into
% early-stage computer science curriculum (\sectionref{sec:1210}).


\SUBSECTION{``When to Specify
  It!''}{sec:whento}{3}{3}{Education}{Faculty}{Everyone else}{}
%
Our first pilot study with users of PBT suggested that developers struggle to
come up with specifications that are worth
testing~\cite{goldstein_problems_2022},
but
our follow-up interviews at Jane Street suggest that the developers there
rarely have the same problem. It seems that the Jane Street developers had
a tacit
understanding of ``no-brainer'' situations where PBT was an obvious
choice---situations where properties were easy to find and where PBT provided
much more thorough testing than other common techniques---and they limited their
use of PBT to these situations.
While experienced PBT users do also apply it in
high-cost / high-benefit situations, we believe
focusing on easier cases is the best way to drive adoption.

We aim to produce authoritative resources that help developers understand
high-leverage situations for PBT. We will begin with an effort
tailored to the academic community: a survey paper with the
aspirational title ``When to Specify It!'' in homage to John Hughes'
widely-viewed tutorial ``How to Specify It!''. This survey will
document the range of high-leverage scenarios identified in our formative
research, including cases like
\begin{enumerate*}[label=(\arabic{enumi})]
\item ``these two functions (e.g., a parser and a printer) should round-trip,''
\item ``this data structure (e.g. a set, map, etc.) should obey algebraic laws,''
\item ``this stateful module should uphold an invariant,''
\item ``these two programs should behave the same'' (e.g., because one
is an optimized version of the other),
and
\item ``this program should not crash.''
\end{enumerate*}
This list will be further expanded with our follow-up surveys with broader
communities of developers~(\sectionref{sec:survey}), a comprehensive
review of case studies that appear in
the academic literature, and an examination of open source projects across a
variety of different software ecosystems using PBT (e.g.,
QuickCheck/Haskell, Hypothesis/Python, Quickcheck/OCaml).

Building on the strong foundation of the survey, we will distill our findings
into media directly tailored for developers. First, we will write up
approachable developer documentation in partnership with our industry
collaborators, to be read among the first resources of any developer
documentation on PBT tools. We will work with our industry collaborators to
disseminate this documentation in blogs and incorporate it into tool
manuals. Then, we will present the findings as a talk at a
major professional developer conference.

\SUBSECTION{PBT for
  undergraduates}{sec:1210}{1}{5}{Education}{Faculty}{Engineer}{}
\iflater\bcp{Should this be led by BCP, or both?  Say explicitly.}\fi
%
Education can act as a force multiplier to make technologies more usable.
When tools or techniques become standards in undergraduate curricula,
students help bootstrap adoption of those tools and techniques in industry. We will
integrate PBT into some of the earliest coursework that undergraduate students
take, targeting introductory level data structures
curriculum. As noted in \sectionref{sec:whento}, testing well-defined data
abstractions is one of the high-leverage scenarios for using PBT, making data
structures a powerful anchor for demonstrating the power of PBT. Building on the
foundation of recent PBT instruction in data structures
courses~\cite{wrenn2021using,nelson2021automated}, we will integrate PBT into
Penn's CIS 1210 course on data structures. We
will evolve the curriculum to emphasize themes in PBT that have arisen from our
formative research to help students identify powerful circumstances for
using PBT, including methods for writing great generators, understanding
generators, considering properties as documentation, and expanding one's
thinking around the scenarios where PBT is suitable
(see~\sectionref{sec:whento}). We will evaluate the impact of these new
instructional approaches on students' ability to leverage PBT in final projects,
disseminating our findings in publications at computer science research venues.
The instructional materials we develop will be made public for instructors at
other institutions to adapt into their own curricula.

\amh{Add evaluation of new contributions to the curriculum, with plans to
publish findings in computing education research venues.}

\immediate\closeout\workplanfile
% \SIMPLESECTION{Plan of Work\pagebudget{.7}}{sec:plan-of-work}

% \bcp{I've moved the text that was here to a separate supplemental
%   document, but we'll probably want to say something short in the main
%   project description (either here or, better, toward the top) about
%   synergy and coordination, with a pointer to that document.}

% \begin{figure}[ht]
%   \centering
%   \vspace*{-1in}
%   \hspace*{-.4in}\includegraphics[width=1.1\textwidth]{assets/workplan.pdf}
%   \vspace*{-1.3in}
%   \caption{Plan of work.}\label{fig:workplan}
% \end{figure}

% \vspace*{-.4in}


\SIMPLESECTION{Broader Impacts%
 \pagebudget{.5}}{sec:broader-impacts}

\noindent{\bf Impact on industry.} The first area of broad impact from
the proposed work will be ...  \underconstruction{a paragraph about
  tech transfer.  This and education (in the broad sense---developers
  and students) are equal parts of our broad impact story.} \smallskip

\noindent{\bf Educational Benefits.}
%
The second area of broad impact will be the
development of educational materials. Educational activities will be
coordinated with the rest of the work, and they are integral to the
project's overall goal of making PBT a standard testing methodology,
readily available to every industrial developer.  Specific pedagogical
threads within the project are described in~\sectionref{sec:ed}.

As an auxiliary goal around education, broadly construed, we will also work to
publicize and communicate the benefits of PBT in the broader computer science
research community. As part of these efforts, we will write an article on PBT
for the Communications of the Association for Computing Machinery
(CACM). \iflater\bcp{Mention this article more prominently in the rest
of the document.  Also, promise a CS in Education paper.}\fi

\smallskip
\noindent{\bf Mentoring and Diversity.}
%
The majority of requested funding will support formative research
experiences and mentoring for graduate students. We
also plan to work with undergraduate and masters students during this project;
they, too will benefit from the research experience. Each graduate
student will have leadership responsibility for multiple facets of the
project, including co-supervising interested undergraduate and masters
researchers.

The PIs will recruit students for this project with a mind towards making
the research area reflective of diversity in the US, and Pennsylvania specifically.
The PIs have had made inroads broadening participation of women in their
groups (1/3 of Pierce's direct Ph.D. mentees are women, as are 2/3 of PI
Head's). That said, a particular area of focus going forward will be on increasing representation of
other underrepresented groups, including Black and Latinx students. Pierce (along with Penn colleagues Zdancewic and Weirich)
was recently informed that he will likely receive funding for an
NSF REU program that will involve 24 undergraduates (8 per
year for three years), selected specifically with an eye to diversity;
some will certainly work on this project. See the appendix on Broadening Participation in Computing for more
details.

\smallskip
\noindent{\bf Benefits to Society.}
%
The project's goals will also be served by open-source distribution of the tools
built during the project. Key systems will be engineered and documented to a standard
that makes them immediately useful to engineers and students: the projects around
improved generation and shrinking, PBT over logs, and evaluating data
distributions will be particular targets for widespread
dissemination. The remaining projects will be published under a permissive
open source license to serve as models for similar tools for other programming languages and environments.

The project also represents an excellent opportunity for strengthening
collaborations between university researchers and industrial advocates of PBT.  Our
ongoing user study at Jane Street has been carried out with the
enthusiastic support of their developers and management, and we hope
to continue using them as a testbed for prototypes of the tools we
will build.  Similarly, we are in active discussions with the
developers of the Hypothesis testing tool for Python\todolast{citation?}, who are keenly
interested in the results of the studies in
\sectionref{sec:survey}.  We plan to establish connections with the
developers and user communities of other popular PBT tools such as
ScalaCheck\todolast{citation?}.

Longer term, better testing means better software.  As software
systems have grown to the gigantic scale seen today, good testing
methodologies and tools (unit testing tools, test-first design
methods, etc.) have come to play an ever more crucial role.  Adding a
powerful new testing tool to programmers' arsenals will further boost
this part of the development process, leading to software of every
sort that is more robust, more reliable, and less expensive to build.


% \proposecut{See the Collaboration Plan for
% more.}

% The Project Description must contain, as a separate section within the narrative, a section labeled ``Broader
% Impacts of the Proposed Work". This section should provide a discussion of the broader impacts of the proposed
% activities. Broader impacts may be accomplished through the research itself, through the activities that are
% directly related to specific research projects, or through activities that are supported by, but are complementary to
% the project. NSF values the advancement of scientific knowledge and activities that contribute to the
% achievement of societally relevant outcomes. Such outcomes include, but are not limited to: full
% participation of women, persons with disabilities, and underrepresented minorities in science, technology, engineering, and
% mathematics (STEM); improved STEM education and educator development at any level; increased public
% scientific literacy and public engagement with science and technology; improved well-being of individuals in
% society; development of a diverse,globally competitive STEM workforce; increased partnerships between
% academia, industry, and others; improved national security; increased economic competitiveness of the United
% States; and enhanced infrastructure for research and education.

\SIMPLESECTION{Results from Prior NSF Support\pagebudget{.5}}{sec:prior}

% If any PI or co-PI identified on the project has received NSF funding (including any current
% funding) in the past five years, in formation on the award(s) is required,
% irrespective of whether the support was directly related to the proposal or not.
% In cases where the PI or co-PI has received more than one award (excluding amendments),
% they need only report on the one award most closely related to the proposal. Funding includes not just salary
% support, but any funding awarded by NSF. The following information must be provided:\\

\emph{\underline{PI Pierce}}: (NSF 1955565) ``Collaborative Research:
SHF: Medium: Bringing Python Up to Speed'' (\$437,999,
7/2020--6/2023), with co-PIs Michael Hicks (Maryland) and Emery Berger
(Amherst).
The project aimed to dramatically increase the performance and
correctness of applications written in Python by developing novel
techniques for performance analysis, optimization, run-time systems,
property-based random testing, concolic execution, and program
synthesis. It developed both
novel performance analysis tools and optimizations and novel automatic
testing frameworks. These were largely tailored to and implemented for
Python, but applicable in other, similar languages.
%
{\bf Intellectual Merit.} The project involved work on both
performance measurement (mostly at Amherst and Maryland) and PBT (mostly at Penn
and Maryland).  Specific threads of work involving Penn included
building an early
version~\cite{Frohlich2022} of the Reflective Generators described in
\sectionref{sec:reflective}, carrying out the pilot study of PBT in Python
mentioned in the motivation section
above~\cite{goldstein_problems_2022}, and building on the idea of
freer monads from functional programming to develop ``free
generators,'' which unify parsing and
generation~\cite{goldstein2022parsing}, presented a principled
automatic testing framework for application-layer
protocols~\cite{Li2021:MBToNA}, and developed and released a freely
available mutation testing framework for Python, called {\tt
  pytest-mutagen}~\cite{pytestmutagen}, and applied ideas from
combinatorial testing, a widely studied testing methodology, to modify
the distributions of random test-case generators so as to find bugs
with fewer tests~\cite{DBLP:conf/esop/GoldsteinHLP21}.
%
{\bf Broader Impacts.} Project results and open-source software
products are being used to increase the
performance and correctness of Python applications.
Educational impact has included training both graduate and
undergraduate students, including a female PhD student at Penn, Jessica
Shi.
%
{\bf Publications (involving Penn):} \cite{Frohlich2022,DBLP:conf/esop/GoldsteinHLP21,
  goldstein2022parsing, goldstein_problems_2022, Li2021:MBToNA}.
{\bf Research Products (from Penn)} \cite{pytestmutagen}.

\smallskip

\emph{\underline{PI Head}} has not previously received NSF support.

% \SUBSECTION{Proposed Study}{}{}
% The Project Description should provide a clear statement of the work to be undertaken and must include:
% objectives for the period of the proposed work and expected significance; relation to longer-term goals of the PI's
% project; and relation to the present state of knowledge in the field, to work in progress by the PI under other
% support and to work in progress elsewhere.
%
% The Project Description should outline the general plan of work, including the broad design of activities to be
% undertaken, and, where appropriate, provide a clear description of experimental methods and procedures.
% Proposers should address what they want to do, why they want to do it, how they plan to do it, how they will
% know if they succeed, and what benefits could accrue if the project is successful. The project activities may be
% based on previously established and/or innovative methods and approaches, but in either case must be well
% justified. These issues apply to both the technical aspects of the proposal and the way in which the project may
% make broader contributions.

\iflater
\section*{Stuff to think about once the document stabilizes}

\todo{GPG: To increase the diversity of the reviewer pool, CISE actively encourages each proposer to include a list of suggested reviewers (including email addresses and organizational affiliations) whom they believe are especially well qualified to review the proposal and are not conflicted with project personnel. Suggestions for reviewers from groups underrepresented in computing are especially encouraged. Proposers should follow the guidance in PAPPG Chapter II.D.1.}

\todo{GPG: Issues of fairness, ethics, accountability, and
  transparency (FEAT) are important considerations for many core
  topics in computer and information science and engineering. In
  projects that generate artifacts ranging from analysis methods to
  algorithms to systems, or that perform studies involving human
  subjects, PIs are encouraged to consider the FEAT of the outputs or
  approaches.  (We should touch on this point somewhere.)}

\todo{Does it make sense to include something about separation logic,
  or more generally about ``generators with difficult preconditions''?
 Could be speculative, as long as it's not vacuous.}

\bcp{I see quite a few little issues in the citations.  Let's make
  sure we give that a quick polishing pass.}

\bcp{We could / should read some past funded NSF Large proposals for
  ideas about how people talk about their work.}

\todo{Remember to turn off all the ``forreaders'' stuff.}

\todo{Make sure the intro to each section accurately reflects what's
  in the section, and in which order.}

\todo{Every section (including the broader impacts and each of the
  research sections) needs to explicitly address how we will measure
  success...  Check that we've done this.}

\discuss{What about related work???  Eeek.}

\fi

\bigskip \forreaders{Don't miss all the supplemental documents beyond
  the bibliography that we'd love to hear your comments on...}

\discuss{Get letters from Leo and Hila.}