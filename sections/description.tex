\section*{Project Description}

\iflater\todo{Remember to not use any URLs in the project description!
  (They are encouraged in the references.)}\fi

Testing plays a vital role in modern software development,
contributing crucially to robustness and overall quality---as well as
to overall development costs.
%
It comes in many styles---unit testing, integration testing,
performance testing, stress testing, accessibility testing,
etc.---supported by many sorts of tools, with yet more advanced tools
and techniques continually being developed, studied, and applied.

One such technique, {\em property-based testing} (PBT), has been
enthusiastically taken up by the functional programming research
community since its introduction in the Haskell QuickCheck
library~\cite{ClaessenHughes00}, and is beginning to make
serious inroads beyond academia.
%
PBT is sometimes glossed as ``formal specification without formal
verification'': a developer characterizes the desired behavior of
some piece of code at a high level in the form of executable {\em
  properties}, which are simply Boolean-valued functions. The code is
then validated against these properties by running it over and over
with a large number of automatically generated test cases.
%
PBT tools thus give programmers an efficient way of testing their
code's behavior with a comprehensiveness that is often not
possible with alternative tools.

PBT's combination of rich, high-level specification with easy, mostly
automatic validation has proved effective at identifying subtle
bugs in an impressive variety of settings, including telecommunications
software~\cite{arts2006testing}, replicated
file~\cite{hughes2014mysteries} and key-value
stores~\cite{Bornholt2021}, automotive software~\cite{arts2015testing}, and a range
of other real-world systems~\cite{hughes2016experiences}. With support
now available in all major programming languages, PBT has
begun to make significant inroads in the broader software
industry. For example, the developers of the Hypothesis library for
Python estimate that the library is used by on the order of half a million
developers~\cite{ZacPersonalCommunication}.

\newcommand{\participant}[1]{{P#1}}
% \newcommand{\participant}[1]{{\bf P#1}}

With all of this momentum, one might wonder if the research community has
already
addressed all of the challenges that could limit PBT's adoption
in the broader software industry, but it
seems the answer is no.
In an ongoing need-finding study with users of OCaml's QuickCheck testing tool
at Jane Street Capital, we found
consistent enthusiasm for PBT---participants called it it
``obviously valuable'' (Participant \participant{1}),
built their own libraries for it when standard ones were not available in their
development context (\participant{8},
\participant{21}), and suggested that ``everyone'' at the company should use it
(\participant{20}). But we also found
challenging new research questions where advances in technical foundations and
tool design necessary to overcome present usability challenges and expand the
potential impact of PBT.

Our formative research suggests that a future of \emph{usable property-based
testing} depends on research addressing the challenge problems of helping
developers
%
(a) identify situations where a property-based approach is suitable for
testing,
%
(b) specify properties that cross module boundaries,\bcp{Not sure
  about this one.}
%
(c) write random generators for data structures with complex invariants,
%
(d) understand whether generated data distributions are sufficiently realistic
and comprehensive, and in turn tune the generated data distributions,
%
(e) understand individual test failures, and
%
(f) learn about effective PBT practice in the first place.
\amh{This feels redundant with the later bullet points. I'd propose we have this
simply state the high-level categories of needs from the project summary, and
save the specifics for the bullet points below.}\bcp{Agreed.}
\iflater\bcp{All that needs final tuning.}\fi

Solutions to these challenges require insights from both the
programming languages (PL) and
human-computer interaction (HCI) communities.  Chasins et
al.~\cite{chasins_pl_2021} argue that a methodology
involving
both PL and HCI hits a ``sweet spot'' where need-finding techniques identify
current pain points and lead to concrete tools that help programmers write
safe, correct programs. We wholeheartedly agree, and such a methodology is
central to this research.
The team behind this proposal is uniquely positioned to advance the
state of the art in PL and HCI: PI Head recently co-founded
a new HCI group at the University of Pennsylvania and specializes in interactive
programming environments, while PI Pierce has
published widely on PL topics including PBT.

We propose a comprehensive, interdisciplinary research program
bringing the combined power of PL and HCI to bear to accelerate PBT's
transition into practice.
\begin{itemize}[noitemsep]
\item We will establish a \emph{foundation} for HCI-informed research on PBT,
beginning with our ongoing need-finding study. We will confirm
its findings and further explore PBT's usability with
two\iflater\bcp{check!}\fi{} survey studies and observation studies
of PBT users {\em in situ} designed to lead to concrete tool designs
(\sectionref{sec:foundation}).
  \item We will empower developers to \emph{specify} properties with new approaches
to support the key use cases of properties that cross module boundaries and
those that represent complex model-based checks (\sectionref{sec:spec}).
  \item We will help developers efficiently define and tune test-case
\emph{generators} with novel techniques for generating precondition-satisfying
values, test input mutation, and example-based generator tuning, building
  on an ``reflective'' approach to generation (\sectionref{sec:gen}).
  \item We will support developers in rapidly \emph{validating} testing
  effectiveness with novel interactive programming tools that help programmers
locate the cause of a test failure and understand how to improve their generated
data distributions to be more representative and comprehensive
(\sectionref{sec:val}).
  \item We will propose an agenda\bcp{weak} for PBT \emph{education}, grounding this
  agenda in resources for teaching PBT practices in an undergraduate
  computer science curriculum, a tool to help students author
  properties, and a comprehensive review of PBT practice
  to help developers learn about diverse situations to which they should
  put properties to work to validate their software
  (\sectionref{sec:ed}).
\end{itemize}

Each of these efforts will deploy complementary tools from PL
and HCI research. HCI approaches will be used to concretely define developers' needs,
design the user interface to tools, and evaluate their success. PL approaches
will be used to develop PBT tools such as domain-specific languages and
type-based tools that are powerful and flexible, and to formally validate these
tools. We conclude the proposal with a concrete outline of our work plan
(\sectionref{sec:plan-of-work}) and discussion of the broader impacts of the
proposed work (\sectionref{sec:broader-impacts})

\sectionstar{Orientation: Property-Based Testing}

We next describe the basics of the PBT approach, with the aim of highlighting 
where research has unique leverage in improving tooling, processes, and 
education around PBT.

Popularized by QuickCheck~\cite{hughes2007quickcheck} in Haskell,
PBT is a form of random testing~\cite{hamlet1994random} where
users write executable functions which act as partial
specifications of a function under test. For example, a user might
write the following property for an \lstinline{insert}
function for a binary search tree (BST):\footnote{The function can be read as
follows:
Assume an arbitrary tree \texttt{t} and an integer
\texttt{x} to insert into that tree. If the original tree
is a BST (``\lstinline{is_bst t}''), then it should remain
a BST after the insertion of \texttt{x} (``\lstinline{is_bst (insert x t)}'').}
\begin{lstlisting}
  prop_insert_correct x t  =  (is_bst t ==> is_bst (insert x t))
\end{lstlisting}
As is the case with properties generally, this property is a function which 
takes in generated
test inputs and evaluates to \lstinline{True} if the test passes, and
\lstinline{False} otherwise.  This function, ``\verb|prop_insert_correct|'',
checks
that an insert operation on a binary search tree preserves the
binary ordering of the tree.
While properties like this one require
only a few lines to express, they can validate unlimited
input-output pairs.  Given a property like this one, the PBT tool generates a
large number of inputs and
checks that the property evaluates to \lstinline{True} for each input; any input
that causes the property to fail is reported as a {\em counterexample}.

This proposal focuses on {\em random}
generation~\cite{hamlet1994random}, in contrast to other
approaches like enumerative test-case
generation~\cite{DBLP:conf/haskell/RuncimanNL08, leancheck} and model
checking. Random generation is the dominant approach in PBT. Its
surprising effectiveness is often attributed to the
``combinatorial'' nature of larger test cases: bugs can be
exposed with test inputs that embody a specific combination of features,
independent of whatever other features are present. For example,
a bug may be triggered by some sequence of API calls in a
particular order,
regardless of whether they are
interleaved with other API calls. As a result, testing with large random
inputs often exposes the same issues as testing with exponentially more
enumeratively generated inputs.

From the perspective of a developer, applying PBT entails the following
steps. First, the developer defines one or more properties that should be true
of the program under test. Then, they design (or reuse, or tailor) {\em random
input generators} for the values that the properties take as input. Then, they
check the properties with generated inputs, using test drivers provided by their
PBT tools. And finally, if counterexamples are discovered, they make sense of
them and determine the source of the bug.
Each of these steps presents opportunities to improve the user experience of
PBT, as we describe in the next section.

\smallskip

Why go to the trouble of PBT, when more straightforward example testing is the
de-facto standard in the software industry? First and foremost, PBT has the
potential to be much more thorough than user-defined examples. As mentioned
previously, PBT has an impressive track-record uncovering bugs that other
approaches to testing had failed to find~\cite{arts2006testing,
hughes2014mysteries, Bornholt2021, arts2015testing, hughes2016experiences}. But
PBT is more than just thorough---it is actually more general than example-based
testing. Wrenn et al.~\cite{wrenn2021using} point out that example-based testing
of programs like that implement relations (e.g., topological sort, which may
produce one of a number of potentially correct results) is impossible to do
faithfully; a property-based specification is a much better choice. PBT is also
an obvious choice if formal properties have already been written, as is the case 
with tools like QuickChick~\cite{paraskevopoulou_foundational_2015}, which 
implements PBT in
the Coq proof assistant. Finally, PBT can act as superior documentation:
participants in our foundational study (\participant{5}, \participant{21})
talked at length about properties being an ideal way to communicate what a
program is supposed to do.

With so many potential advantages, it is clear that PBT should be a tool should 
be a tool that software developers turn to without hesitation for validating 
their software. We intend to make it that way.

\iflater\todo{Double-check that we've mentioned all relevant
  ``hgih-level'' related work somewhere.}
  \hg{My current decision is that Orientation should stay high-level RE PBT.
  Specifically, I think it shouldn't get into the weeds of generation
  techniques, including fuzzing. I added a bunch of references RE that later on
  in the context sections.}
  \fi

\sectionstar{Motivations from a Formative Study of PBT in Industry 
\pagebudget{2}}\label{sec:motivation}
%
The goals of this proposal are motivated by findings from an on-going
need-finding study we have been conducting in industry. The purpose of this 
study was to understand how the research community can make PBT more valuable 
for software developers. This study has consisted of semistructured interviews 
with 30 stakeholders in PBT, including both developers who have used PBT and 
developers and maintainers of PBT tools. The site of the study is Jane Street 
Capital. A number of aspects of Jane Street make it
an attractive setting for such a study.  Most importantly, PBT is
already well established at Jane Street, so there is a large
population of people with well-informed opinions on its benefits and
challenges. Additionally, Jane Street famously builds almost all of its
software in OCaml, a mostly functional programming language for which
there is a well-engineered PBT tool. This unified
ecosystem allows us to ensure that developers have access to mature PBT tools,
experience using them in collaborative settings,
and awareness of language-level programming abstractions necessary
to advanced usage.\footnote{Because our primary need-finding study to-date has 
taken place solely at Jane Street, our follow-up studies in 
\sectionref{sec:foundation} will engage with broader groups of developers using 
PBT to expand and generalize our findings.}

As of December 2022, the full complement of 30 interviews and a
preliminary round of qualitative analysis have been completed; full-scale 
analysis will begin in 2023.  A paper describing our findings will be submitted
to a top-tier software engineering conference.

\subsectionstar{Framework: Challenges Using PBT} Upon completing
our analysis, we will contribute a deep, qualitative description of how 
developers use PBT, what they need from it, and how the research community can 
help improve PBT tools. While full analysis of the data remains
to be done, several themes have arisen from our initial analysis
of interview session notes. These form the backbone of the present proposal.

\newcommand{\proptheme}[1]{{\color{nord-orange} \em #1}}
\newcommand{\gentheme}[1]{{\color{nord-green} \em #1}}
\newcommand{\evaltheme}[1]{{\color{nord-purple} \em #1}}
\newcommand{\edutheme}[1]{{\color{nord-frost4} \em #1}}
A first set of
themes revolves around the {\bf specifications} that developers test.
% and the kinds of programs in which they choose to test them.
Since
PBT is often described as a kind of lightweight formal method, one
might imagine that a central challenge would be coming up with the
right specifications. Indeed, an earlier pilot study of PBT users in
Python~\cite{goldstein_problems_2022} that we ran to prepare for the Jane Street
study\iflater\bcp{Is this the first
  mention of the pilot study?  If not, move this citation earlier and
no need for it here.}\fi{}
concluded just that.
  But at Jane Street itself
we actually heard very little about difficulties coming up with
properties; rather, most participants described applying PBT in
\proptheme{High-Leverage Scenarios} where properties were already
available or straightforward to invent. For example, \participant{9} pointed out
that PBT is particularly easy to apply when one has ``a really good abstraction
with a complicated implementation.''
Several participants (\participant{3}, \participant{15},
\participant{20}, \participant{22}), when asked to speculate, guessed
that 80--100\% of Jane Street
developers write programs like this, where properties are easy to find, and
where PBT is relatively easy to apply.  This suggests that an effective way to
approach education and documentation around PBT would be to focus on
``opportunistic'' applications of PBT as an easy on-ramp.

Our interviews also identified several \proptheme{Opportunities for Better
Leverage}---situations where PBT is not easy {\em yet}, but could be with a
little more research effort. For example, \participant{7} spoke about trying to
test a poorly abstracted program (which are notoriously resistant to PBT) by
writing properties about output it prints to the terminal; we plan to
operationalize this approach.
Furthermore, more than three quarters of study participants had used a
particular kind of PBT, commonly called
\proptheme{Model-Based Testing}.
\participant{3}, an author of PBT tools at Jane Street, considered better
automation and tooling around model-based testing to be one of the most
significant ways improve PBT and make it easier to pick up and use.

Another set of common themes in the interviews concerned the {\bf generation} of
random inputs for property-based testing. Many participants spoke
highly of the \gentheme{Derived Generators} that can be inferred from
the OCaml type system (\participant{5} called OCaml's tools for this
``[expletive] amazing'' and \participant{30} called them a ``game changer'').
These generators are already quite good, but they could be better: participants
identified deficiencies both small (e.g., API quirks) and large (e.g.,
derived generators
cannot enforce semantic preconditions). When derived generators
failed, participants fell back to \gentheme{Bespoke Generators}, which
are far more flexible but proportionally more time consuming to design and work
with. For example, \participant{20} found important bugs with a bespoke
generator for XML documents,
but reported spending ``at least a day'' writing it.
Improving the abstractions available for writing bespoke generators would
greatly improve both the experience of using PBT and its potential for
bug-finding.

If a generated input is deemed a {\em counterexample} for a property,
highlighting a bug in the code, it needs to be easy to use that input to
determine the root cause of the bug. The interviews made clear that
\gentheme{Shrinking}, the process of finding the smallest possible input that
provokes a given bug, is a critical part of the PBT process.  \participant{8}
and \participant{21}, who each needed to implement their own PBT libraries, both
made sure to include shrinkers in their implementations. Unfortunately,
shrinkers are often time-consuming to build, and many participants
(\participant{16}, \participant{20} \participant{21}, \participant{30}) talked
about it as an opaque or even magical process that they did not understand.
Ideally, shrinkers would be an invisible part of the PBT process that
automatically minimized counterexamples for developers.

Some more themes from the interview study concerned the {\bf validation} of
testing effectiveness. The developers' \evaltheme{Evaluation of Effectiveness}
often featured in our conversations, with some participants asking explicitly
for better ways to evaluate their generators and properties. There are many
forms such feedback could take, including code coverage (requested by
\participant{9} and \participant{25}), {\em mutation
testing}~\cite{papadakis_mutation_2018}, and especially input space coverage
measurements (requested by \participant{10}, \participant{16}, and
\participant{16}). Providing good tools that provide this feedback is critical:
though they asked for better feedback when prompted, many admitted to
\evaltheme{Implicit Trust in Infrastructure} that they had not thoroughly
analyzed---\participant{14} actually shipped broken code to production because
their generator missed important input examples. Tools in this space should
warn developers about under-performing testing without them needing to ask.

Finally, improving the usability of PBT requires a focus on {\bf education}. As
mentioned above, focusing on \edutheme{High-Leverage Scenarios} should lower the
barrier to entry for PBT and give developers patterns that they can match when
deciding if PBT is a good choice for a particular program. Additionally, even in
situations where properties are readily available, developers who are less
experienced than those at Jane Street struggle with \edutheme{Imagining
Properties}. In our pilot study~\cite{ref:goldstein2022some} in population of
Python developers, participants (Pilot-\participant{1}, Pilot-\participant{3},
Pilot-\participant{4}, Pilot-\participant{5}) consistently complained that they
could not see what properties to test, even when it might have been clear to
someone with more experience. Developers with all levels of experience
(including Pilot-\participant{1}, Pilot-\participant{4}, \participant{3}, and
\participant{11}) also complained about the dearth of \edutheme{Documentation
and Examples} available for PBT tools.

We discuss plans for work that addresses these themes below, \proptheme{specification
  themes} in \sectionref{sec:spec}, \gentheme{generation themes} in
\sectionref{sec:gen},
  \evaltheme{validation themes} in \sectionref{sec:val}, and \edutheme{education
  themes} in \sectionref{sec:ed}.

\iflater
\amh{Let's foreshadow other findings that
we will publish that are outside of the scope of this proposal, but which might
be motivating to other researchers, including: developers could benefit from
additional automated support for deriving generators from types, effectiveness
budget (i.e., deciding how much time to spend on running PBT tests versus fuzz
tests), integration with unit testing frameworks, and exploring the use of
properties as a source of documentation.}
\bcp{Not sure where is the right place for this discussion (or if
  there will be space)...}
\fi

% \todo{Debugging doesn't really fit here...}
\ifdesperateforspace
\bcp{ We could save a tiny bit of space by grouping all the
  ``We discuss'' sentences into a paragraph at the end.  But if space
  is OK, I think it reads well this way.}
  \fi

\SECTION{Foundation}{Understanding Needs and Opportunities \pagebudget{1}}{sec:foundation}

% \amh{This section is around double its page budget. One reason is that I am
% probably being too heavy-handed in trying to justify the methodological choices.
% Help me figure out what to prune!}\bcp{Did some pruning! :-)}

The final results from the in-progress user study should paint a clear
picture of the benefits and challenges of PBT in the specific context
of Jane Street and other organizations with similar characteristics.  But to
fully understand the potential impact of PBT across the software
industry---and the factors that may limit its adoption---we need to
cast a wider net.
%
In this section, we describe three planned studies, drawing on mixed
methods in the service of producing a comprehensive and actionable
agenda for future research, in this research project and beyond---two
written surveys, one to assess the generality of these needs and obstacles
and one to identify potential for adoption of PBT tools
(\sectionref{sec:survey}), and an observation study to understand
particular tasks involved in PBT to guide the design of new algorithms
and interactive tools (\sectionref{sec:observations}).

\iflater\bcp{Somewhere, we need to mention the letter of support from
  JS.  I guess it belongs in the section where we describe potential
  further collaborations with them.}\fi

\SUBSECTION{Generalizing the Jane Street Findings}{sec:survey}{1}{2}{Other}
\iflater\bcp{Plural or singular?}\fi

Our ongoing Jane Street study has already revealed a number of
opportunities to improve tools for property-based testing. To identify
others and to better understand which of these opportunities are most
important for the research community to explore, we will conduct two
surveys with broader samples of developers. These surveys aim to
(1) determine which obstacles observed in the interview study
represent widely experienced pain points with PBT tools and
(2) characterize the potential benefits of better tools to the
software industry as a whole.

\emph{Validation survey}. The main survey we plan to conduct aims to check that
the things we learned from Jane Street generalize more broadly.
Our interview study has revealed
a number of issues with existing PBT tools, but it is not clear
how broadly these issues are experienced by developers, or
their relative severity. To find out, we will
ask developers which of the issues we have identified are
ones they have experienced, and the severity of those issues
in their experience. Respondents will also be asked to report on
other issues they encountered in their use of PBT tools.
To provide
clear usage scenarios to guide future work, respondents will
be asked to write brief anecdotes elaborating on the
most severe issues they had.

Respondents will
be recruited broadly, from three sources. First, we will recruit
users of Hypothesis, a widely used Python-based PBT
framework
(see the attached letter of support).
Second, we will distribute the survey at Jane Street, hoping to reach
a broader set of developers than we were able to interview.
And third, we will recruit users from social media by posting
survey announcements from the PIs' Twitter and Mastodon
accounts, various mailing lists, and on discussion boards for conferences like
``Yow!'' where we have been invited to speak~\cite{noauthor_when_nodate}.
% Types and types-announce mailing lists. They are mailing
% lists for the types programming. We will distribute announcements among
% attendees of conferences like ICFP or ``Yow!'' whom have
% a cumulative thousands of programmers with functional
% programming experience.

\emph{Impact survey}. Another more speculative survey we hope to conduct asks
how broadly PBT may eventually be able to reach.  To get a sense of this, we
survey ``proximal'' users of PBT---that is, developers who do not use PBT
currently, but who experience use cases where PBT methods would be particularly
useful.  Particular attention will be given to those whose work requires the
writing of validation code and public APIs with some definition of types. We
will recruit a broad sample of participants across development contexts
(professional, open source, educational) by working with our industry contacts
and by recruiting over social media. Admittedly, this survey seems more
difficult to ``get right,'' and the design will evolve over time as we gain a
better understanding of the situations where PBT is most effective
(\sectionref{sec:whento}\bcp{this got moved}), but if done right this could provide invaluable
feedback to PBT researchers about how far their work could reach.

\SUBSECTION{Observing PBT in Practice}{sec:observations}{3}{4}{Other}

Anecdotes related in interviews do not generally, in and of
themselves, provide enough information to inform the design of
effective tools---they beg questions like: (1) How much time
are participants willing to devote to
a task like creating a generator or debugging a counterexample when in the
middle of a programming task? (2) How much space is available on a developer's
screen (amidst other tools like code editors and terminals) for
interacting with new
tools? (3) What are developers' current strategies for solving the
problems they describe
(for example, what representations of generated data currently seem most helpful
for programmers trying to under the distributions of data generated by a
generator)?.  As a basis for designing tools with a substantial novel interface
component (see \sectionref{sec:val}), we will observe developers
undertaking the respective PBT tasks we
aim to support, on the order of a small handful of developers (i.e., 2--10
observation sessions). These observation sessions will allow us to evolve
singular anecdotes from the interviews into a more comprehensive picture of
what kinds of designs will be viable for helping programmers.

% \bcp{Again, we need to explain the expected contributions---what end
%   does ``characterizing obstacles'' serve.}

\smallskip

Together, the studies described in this section provide a firm foundation many
of the activities in this proposal, and for future research on PBT more
generally. We will produce a validated and prioritized set of developer needs
for PBT tools and a detailed set of requirements for how tools should be
(re)designed to best satisfy the most important of those needs.

\SECTION{Specification}{Widening the On-Ramp \pagebudget{2}}{sec:spec}
The projects in this section address the \proptheme{specification themes}
discussed in the Motivation section.  In \sectionref{sec:outpurprop} we describe
a plan to significantly expand the scenarios where PBT has leverage, improving
its applicability to poorly abstracted code. Then, in
\sectionref{sec:automating} we provide tools to apply PBT automatically in one
specific scenario: model-based testing.  Together, these projects will greatly
improve the on-ramp to PBT and clarify its place in the software industry.

\SUBSECTION{Properties Over Logs}{sec:outpurprop}{1}{2}{Harry}

\bcp{What about swapping this with the modules section, which seems
  better motivated by the JS findings?  (Was the log issue mentioned
  by multiple people, or just P7?  Or are we extrapolating from other
  things people said, about abstraction generally?)}

One common problem raised by Jane Street developers (and even more loudly
in our earlier pilot study~\cite{ref:goldstein2022some}) was
that code may sometimes not be abstracted
in a way that is amenable to testing. Indeed, any kind of
testing of software units---with PBT techniques or otherwise---requires
``units'' to test!  But PBT is
especially sensitive to issues like poorly encapsulated global state, which may
impact the repeatability of testing, and leaky or confusing
abstraction boundaries, which make it difficult to cleanly state
properties or specifications.

One Jane Street developer (\participant{7}) described their experience testing
a trading system (we'll call it System A) with messy abstraction boundaries. In
this case, System A was hard to test because it was primarily used as a
sub-component of a larger System B: System B would take in simple data, and then
at some point pass a large, complex data structure to System A, which would do
some work and hand results back to System B. PBT of System A was therefore hard,
because generating valid examples of the large data structure that System A
takes as input required deep knowledge of the internals and types of System B
that the developers of System A did not have access to, and because the
interface was too complex to write straightforward properties about.

One might imagine trying to test System A through System B by writing properties
over intermediate values in System A. One might imagine trying to write and
test properties like
\begin{lstlisting}
  sublist_of processed (next (processed))      (* 1. Processing is monotonic. *)
  msg.length < 100 ==> never (overflow = true) (* 2. Capacity at least 100 bytes. *)
  eventually (processed.length = msg.length)   (* 3. The whole message is processed. *)
\end{lstlisting}
which demonstrate that internal variables like \lstinline{processed},
\lstinline{msg}, and \lstinline{overflow} evolve correctly over time.
These properties could theoretically be tested without ever running System A on
its own: we can generate inputs to System B, have System B drive System A, and
simply monitor the variable values to check the desired properties.

The tricky part comes with how to actually implement this testing. One might
hope to do it through debug assertions, but that quickly becomes untenable.
Checking the property (1) with debug assertions would require noisy code to
saves the previous value every time \lstinline{processed} is updated, and the
programmer would need to use macros or some other kind of meta-programming to
remove that code when building in release mode. Even worse, there may be no way
to check properties (2) or (3) at all! Doing so would require making an
assertion at the ``end'' of the computation, which may not actually be possible
if System B is driving System A from the outside.

Instead, we propose a framework for writing properties over {\em logged values}
that can capture the interesting relationships between past and future variable
values that the above properties demand.  Concretely, we will insert lightweight
logging annotations and provide a language for writing properties over those
logs. In order to express concepts like \lstinline{next}, \lstinline{never}, and
\lstinline{eventually} the property language will need some logical connectives
that are not standard in PBT.  In particular, we need a temporal logic like {\em
linear temporal logic} (LTL) so that properties can capture a notion of time.

Using LTL with PBT is not unheard of---Quickstrom testing
framework~\cite{oconnor_quickstrom_2022} has already shown that LTL properties
can be used for specifying and testing graphical user interfaces---but LTL
properties at this level of generality have not yet been attempted. We plan to
compile the user's LTL formulae to properties over traces that respect the
structure of the log and ignore updates to irrelevant variables. Then, the
developer can provide inputs to some system containing the sub-system under
test, and the test harness will apply the trace properties in real time. In this
way, the user can get feedback about the invariants that should hold of the
intermediate values of their programs, making PBT available in a huge range of
programs where it was previously too difficult to apply.

\SUBSECTION{Models for Modules}{sec:automating}{2}{3}{Harry}
One finding that has surprised us from the Jane Street study is that
it is {very} common for developers to build (or already to have) a
{\em model
implementation} of the code they are testing and check that the two versions of
the same code produce the same results.  This is a well-documented approach to
PBT~\cite{hughes_experiences_2016}, but it is not supported as well as it could
be by existing tooling.
%
With this in mind, we propose comprehensive tooling to entirely automate
model-based testing for a large class of ML-style software
{\em modules}~\cite{macqueen_modules_1984}.

\begin{figure}[t]
  \begin{minipage}{.45\textwidth}
\begin{lstlisting}
module StringFns : sig
  val reverse : string -> string
  val drop_n  : int -> string -> string
  val split   : string -> string * string
end
\end{lstlisting}
  \end{minipage}
  \qquad\qquad
  \begin{minipage}{.45\textwidth}
\begin{lstlisting}
module Set : sig
  type 'a t     val empty : 'a t
  val mem   : 'a t -> 'a -> bool
  val add   : 'a t -> 'a -> 'a t
end
\end{lstlisting}
  \end{minipage}
  \caption{Some module interfaces we would like to test
    automatically.}\label{fig:sigs}
\end{figure}

Model-based testing is trivial in the simplest case (comparing two pure
functions), but if the code under test is a {\em collection} of functions,
organized into a module, things get more interesting. Testing even a simple
module requires a complex orchestration of calls to the different functions in
the module's signature. For example, testing \lstinline{StringFn} (in
Figure~\ref{fig:sigs}) correctly requires wiring up the functions in the
signature to satisfy their types: \lstinline{drop_n} needs a randomly generated
integer to know how much to drop, \lstinline{split} produces a pair of strings
that can each be arguments to future function calls, etc. We will build on prior
work~\cite{hughes_experiences_2016} to correctly generate graphs of function
calls that are well-typed and that can be used to compare module implementations.

Testing modules gets even hairier when they contain {\em abstract types}, as is
the case with \lstinline{Set}. Now, some of the types in the signature cannot be
generated on their own, so they must be built up from scratch (in this case by
calling \lstinline{create} and \lstinline{add}). The \lstinline{Set} module is
also {\em polymorphic}, so a type must be chosen for \lstinline{'a} before
testing. (There is significant work on this
problem~\cite{hou_favonia_logarithm_2022}, but it is not solved in general.)
To solve these problems, we will take a theoretical approach, breaking down
modules theoretically and developing automation heuristics from first
principles. This process will necessarily involve carving out some subset of
modules where automation is not possible, but highlighting that subset will
provide useful context for future work in PBT usability.

Finally, so far we have only talked about comparing implementations of the same
module signature, but what about implementations of {\em related} signatures? In
theory, it should be possible to automate test harnesses when two modules share
some subset of behaviors, or when the concrete types that they operate on are
related in some known way. We plan to work with users to understand the
important use-cases here and get the automation right.

With the prevalence of ML-derived languages used in practice today, and with ML
modules' similarities to other tools for defining interfaces, this project has
the potential to significantly increase the usability of model-based PBT.

Section \sectionref{sec:whento}\bcp{This got moved} incorporates the ideas from this section along
with many more into a comprehensive list of PBT patterns that developers can use
to decide if PBT is an easy choice for testing their code.

\SECTION{Generation}{Better Tools for Random Inputs \pagebudget{3}}{sec:gen}
%
The Jane Street study also highlighted several \gentheme{generation themes}.
\bcp{Rest of paragraph is weak.}
Generators were regularly cited
as one of the most challenging parts of the
PBT process, and many
asked for better tools for constructing generators that reliably address
their testing goals. In this section, we describe a series of projects
to develop new and better ways to express random data generators that give
developers more automatic ways to test their code more thoroughly.

\subsectionstar{Context: Why is Random Generation Hard?}%
As discussed in the Orientation, PBT relies heavily on {\em random data
generators}. The inputs produced by the generator exercise the developer's
properties and give confidence that they hold---provided the inputs are
interesting enough.
%
However,
many properties that developers want to test have {\em preconditions}
(or {\em validity conditions} or {\em input constraints}) that
restrict the set of inputs that should be used for testing. This comes up often
when testing data structures with invariants that must hold in order to apply
the operations under test. Testing such properties can be problematic, because
many preconditions are difficult to satisfy randomly; if the developer is not
careful they may waste most of their testing time generating and
discarding invalid inputs.
%
Worse, even among valid inputs, not all are equally realistic. Often generated inputs
feel fabricated, since they are not constructed with prior knowledge of the
kinds of inputs that the program is likely to see.
%
Worse yet, even with validity and realism accounted for, there are
other ways for generators to under-perform---for example, by
generating many overly similar inputs while ignoring large parts of
the input space.

% The PBT literature partially addresses these concerns, with solutions solving
% one problem at the expense of another and trading off between programmer effort
% and generator power.

Existing approaches to these issues
fall on a spectrum from automatic to manual. The automatic approaches use
proxies for validity and general ``interestingness'' of inputs: some, like {\em
fuzzers}~\cite{afl-readme}, optimize readily available metrics like code
coverage, others ask users to provide metrics~\cite{loscher2017targetedpbt}, and
naturally some use machine learning to infer proxies for
validity~\cite{godefroid2017learn, DBLP:conf/icse/ReddyLPS20}. These approaches
are easy to apply and can obtain good distribution coverage, but they are rarely
sufficient for testing properties with complex preconditions. Slightly more
manual approaches are based on declarative representations of validity
conditions: for preconditions that are primarily structural, {\em grammar-based
fuzzing} provides a compelling solution~\cite{godefroid2008grammar,
holler2012fuzzing, veggalam2016ifuzzer, wang2019superion,
srivastava2021gramatron}, and for more complex, semantic preconditions, some
have proposed using SMT-solvers~\cite{dewey2017automated, LuckPOPL,
steinhofel2022input} to automatically seek out valid inputs. These tools are
much better at satisfying up to moderately-complex properties, but some are
still out of reach. The semi-automatic
category also includes tools for {\em example-based tuning}, a process that
improves realism of inputs by mimicking user-provided
examples~\cite{soremekun2020inputs}; the existing tools successfully generate
more realistic inputs, but are again limited in the preconditions they can
satisfy.

\subsectionstar{Context: Monadic and Free Generators}
\iflater\hg{Not sure I love this transition... I liked it better when this sentence was above}\fi
The most manual, but also the most flexible, solutions use hand-written
generators, written in a convenient domain-specific language (DSL).
In  Haskell, where PBT was
first popularized, many such DSLs are implemented using {\em
monads\/}~\cite{moggi1991notions}, an elegant design pattern for
expressing effectful (in this case, random and stateful) computations
in a pure, stateless underlying
language. While monadic DSLs are not actually necessary to express generators in
non-pure languages, some libraries (e.g., in OCaml) still use monadic
abstractions to build their generator DSLs.

Monadic generators can implement random data producers of arbitrary complexity
(e.g., it is possible to write a monadic generator for Haskell
programs~\cite{palka_testing_2011}), so they are often more expressive than
other representations like grammar-based generators.  Yet monadic generators are
syntactically constrained in a way that isolates the probabilistic code and
prevents usage errors (like passing the wrong random seed around). As we will
see, the constrained nature of monadic generators also makes them the perfect
candidates for sophisticated manipulations and interesting generalizations.

In order to improve monadic generation along the dimensions listed
above, it helps to re-frame generators as {\em parsers of randomness}. A generator
operates by making a series of random choices, but we can equivalently think of
it as being provided some random sequence of choices and then simply following
those choices to produce a value. This perspective has been used in a few
implementations of PBT
systems~\cite{maciver2019hypothesis, dolan2017testing}; we made it
formal in our paper, {\em Parsing Randomness}~\cite{goldstein2022parsing}.

The paper introduces {\em free generators}, which generalize the standard
monadic generator abstraction, demonstrate a formal link between parsing and
random generation, and enable new algorithms for generation that improve
generation modulo validity constraints. Free generators are written the same
way as standard monadic generators, but they are more like generator ``plans''
or syntax trees.%
\footnote{For experts: Free generators are implemented using {\em freer
monads}~\cite{kiselyov2015freer}, which have been used to great effect in recent
years to capture the structure of effectful computations
(cf.~ITrees~\cite{old:xia2019interaction}). Freer monads represent
monadic computations syntactically by reifying the monad operations
(\lstinline{return} and \lstinline{>>=}) as data constructors. Critically, this
is all implemented within the language (no macros or AST
manipulation). See \cite{goldstein2022parsing} for more details on the
free generator representation.}
This means that a single free generator can be be {\em interpreted} in multiple
different ways. In {\em Parsing Randomness} we prove that any free generator can
be interpreted either as a standard monadic generator or as a source of
random choice strings with a parser over those strings; this formalizes the
relationship between generators and parsers.
\proposecut{Furthermore, since free generators
are uninterpreted syntax trees, they can be manipulated programmatically. Free
generators admit a version of a Brzozowski
derivative~\cite{brzozowski1964derivatives} that can be used incrementalize
generation. We showed that free generator derivatives enable an algorithm called
{\em Choice Gradient Sampling}, which uses repeated derivatives to guide a
generator to inputs that are more likely to be valid with respect to a given
precondition. This work is an important step towards better automation for
generators with complex preconditions.}

\SUBSECTION{Reflective Generators}{sec:reflective}{1}{2}{Harry}
% \bcp{Maybe this is also better described as proposed work?}\hg{Could be, but I
% worry that enough of the following projects rest on reflective generators that
% we want to be able to say ``don't worry, we actually have an idea what these
% will look like''}\bcp{That's fair enough.  I think we can finesse this
% issue though---e.g., by mentioning that we've been discussing this
% with Sam and Meng and have some concrete ideas how to proceed (and
% then giving a bit of technical meat, though of course there is not
% room for much).}
% \hg{Fine by me. How do you want to frame the reference to Sam and Meng?}
Continuing on from the free generator work, we have begun to look at even more
powerful generalizations of the standard monadic approach to random generation.
One such project concerns monadic generators that can be run {\em backward}.

If a generator parses a sequence of choices into a value, then running the
generator backward should take a value and produce a sequence of choices that
would produce that value. With this in mind, we propose {\em reflective
generators}, an extension of monadic generators that can be run backward to
``reflect'' on the choices that they made when producing an input. The machinery
that makes reflective generators work is quite complex,%
\footnote{Reflective generators are both monads and {\em
    partial profunctors},
implementing bidirectional programming in the style of Xia et
al.~\cite{xia2019composing}. This approach to bidirectional programming is
related to lenses~\cite{foster2009bidirectional}, but it hides much of the
complexity of bidirectional program composition in the bind operation of the
monad. The result is an elegant programming experience where both directions of
the computation can be written at once, in a type-safe way.}
but, like free generators, their syntax is still quite close to that of normal
monadic generators.

But reflective generators are useful for more than just generation!
We plan to explore (at least) the following two example applications.

{\em Validity-Preserving Mutation.}
Many automated testing algorithms
(especially fuzzing algorithms~\cite{afl-readme}) {\em mutate} values to explore
the behavior of the program in a space ``around'' those values. This can be
difficult in PBT scenarios, where values are subject to complex validity
constraints, since mutation often produces invalid values. Reflective
generators can help: we can (1) reflect on the choices that lead to a particular
value, (2) mutate those choices, and (3) re-run the generator with the new
choices {\em while correcting any choices that
would lead to an invalid value.} Figure~\ref{fig:mutation} shows the way this
algorithm is able to mutate a binary search tree, while maintaining validity,
using no BST-specific code beyond the reflective generator itself.

\begin{figure}[h]
  \centering
  \includegraphics[width=.7\textwidth]{assets/mutate-diagram.pdf}
  \caption{Validity-preserving mutation of a binary search tree, maintaining the
  BST invariant.}\label{fig:mutation}
  \hg{I can't seem to wrap it...}
\end{figure}

{\em Example-Based Tuning.} Earlier we pointed out that good generators
produce ``realistic'' inputs; one way to ensure this is to tune the generator so
it produces values that are similar to some user-supplied values deemed
realistic. Existing tools make good use of this example-based approach to
tuning~\cite{soremekun2020inputs}, but they do not work with generators as
powerful as monadic generators. We implement a similar algorithm using
reflective generators: we can (1) again, reflect on the choices that lead to a
set of realistic values, and (2) run the generator with {\em new choice weights}
informed by the choices that we saw.

Both of these applications have the potential to significantly improve
testing effectiveness---example-based tuning helps users generate more
realistic inputs, while validity-preserving mutation enables more
automated approaches to improving generator distributions---and we get
{\em both} by upgrading our existing generators to reflective ones. We
will see some additional use cases for reflective generators in the
following sections.

\SUBSECTION{Reflective Fuzzers}{sec:fuzzing}{2}{3}{Harry}
{\em Fuzzers} like AFL~\cite{afl-readme} use principles that are similar to the
ones behind PBT: they leverage randomized testing to quickly exercise as
many program behaviors as possible.  One might then expect that the fuzzing and
PBT share a significant amount of literature, but in reality they do not. Often
the communities seem to ``talk past'' one another.

We propose that the main thing separating the PBT and fuzzing communities is
simply a difference of {\em focus}. The fuzzing literature mostly talks about
fast and automatic ways to find critical (security) vulnerabilities in
programs---usually manifesting in the form of crash failures.  In contrast, PBT
researchers want effective ways to test semi-formal logical specifications of
their programs. Both areas of focus are important: fuzzing captures the ``80\%''
of cases catching high-profile bugs with minimal programmer effort, and PBT
gives a level of thoroughness that fuzzing does not claim to match. But there is
something unsatisfying when things are laid out this way. In particular, while
fuzzing and PBT focus on different testing problems, they face many of the same
technological hurdles. Both PBT and fuzzing need fast and effective ways to
generate random inputs that are valid for the systems that they are testing, and
neither community has truly settled on the ``right'' way to get there.

There is already some work that attempts to bridge the gap between PBT and
fuzzing. For example, the FuzzChick library in Coq~\cite{OLDlampropoulos19fuzzchick}
uses code coverage as guidance for PBT and the HypoFuzz library uses a
similar approach in Python~\cite{hatfield-dodds_hypofuzz_nodate}. These projects
are demonstrably powerful, but neither benefits from the years of expertise
poured into industrial-strength fuzzers; Crowbar
does~\cite{dolan2017testing}. Crowbar uses
AFL~\cite{afl-readme}, one of the best-established
fuzzers, to generate random bit-strings that are later parsed into program
inputs.  (We plan to use AFL++~\cite{fioraldi_afl_2020},
which has supplanted the original AFL project.)

\begin{wrapfigure}{l}{0.5\textwidth}
  \centering
  \includegraphics[width=.4\textwidth]{assets/fuzzing.pdf}
  \caption{Overview of gradually constrained fuzzing.}\label{fig:fuzzing-plan}
\end{wrapfigure}

Our system will start with a setup similar to Crowbar, but with a much greater
focus on the generators; in particular, we will use reflective generators to
gain significant testing power. The setup is shown in
Figure~\sectionref{fig:fuzzing-plan}. We start with a classic fuzzing setup, attempting
to make the system under test crash by passing it a variety of semi-random
inputs. Normally, the fuzzer is working against the parser, in the sense that
the parser's job is to reject invalid inputs and the fuzzer's job is to ``get
past the parser.'' The CFGs used by grammar-based fuzzers help a bit, but they
cannot generate inputs satisfying complex context-sensitive constraints. As in
Crowbar, we avoid this adversarial relationship and subsume grammar-based
generation with a powerful generator that is powerful enough to satisfy the
parser by construction; in our case, that generator is reflective.

Why use a reflective generator? First, we should clarify why any kind of monadic
generator is preferable to grammar-based options. Monadic generators can
straightforwardly generate context-free structures, and they can often do so
automatically with the help of type information~\cite{mista2019deriving}. If
this is all that the precondition requires, then there is no harm in using a
monadic generator, rather than a grammar-based one. But the beauty of a monadic
generator is that it can be made far more powerful, incrementally, as the
developer's testing needs change. The developer can start off thinking that they
need only consider the structure of their inputs, but they can later add more
semantic guarantees if they determine that their testing is ineffective.

Focusing on reflective generators specifically, one compelling benefit is that
their backward interpretation can be used to help seed the fuzzer.  Most fuzzers
ask for a number of {\em seeds}, input examples that the fuzzer can start from,
in order to ensure that the fuzzer does not spend ages exploring
inputs that have no hope of working out. Normally these seeds are easy enough
for the user to write down, since they are simply program inputs, but now that
we are asking the fuzzer to generate sequences of choices it becomes much more
error-prone (and tedious) to produce seeds by hand.  This is one great use for a
backward interpretation. The user can write down their seeds---either as values
in the program, or as text that can be parsed by the program's parser---and then
the reflective generator can reflect on the choices that produce those seeds.

The reflective generator also provides validity-preserving mutation. Guided by
heuristics, the system can opt to supplement the fuzzer's mutation schedule with
mutations that are obtained by the reflective generator's validity-preserving
mutation (which is more targeted than the mutators provided by the fuzzer). We
expect this to have a significant impact on performance, especially in contexts
where preconditions are relatively sparse and therefore hard for AFL++ to mutate
correctly.

Our ultimate goal is a grand unification of PBT and fuzzing generator tooling:
the fuzzing literature provides battle-tested heuristics for coverage-guided
generation, the PBT literature provides powerful tools like reflective
generators for refining generator performance incrementally, and both
communities benefit from generators with better distributions.

\SUBSECTION{Reflective Shrinkers}{sec:shrinking}{3}{4}{Harry}

A more modest application of reflective generators uses them to implement
validity-preserving {\em shrinking} of values to find smaller counterexamples
and speed up debugging. On its face, this feels similar to validity-preserving
mutation: Can we reflect on choices, shrink the choices, and then re-run the
generator with the smaller choices? Likely yes! But there are complications.

Shrinkers need to be more careful than mutators to keep shrunk values faithful
to the original. When mutating, it is often fine if the mutated value is
accidentally quite different from the original value, since the mutator is
trying many values and any that catch a bug are equally good. But a rogue
shrinker runs the risk of confusing the developer much more than they already
were. For example, if a reflective shrinker accidentally creates a visually larger
value, for example, because the relationship between choice sequence length and
output value size is not monotonic, the shrinker might actually make it more
difficult to find a bug. Additionally, if a reflective shrinker's shrunk choices
produce a values that are very different from the original, they may fail to
shrink at all, instead just stepping to values that are no longer
counterexamples and concluding that the original value is a local minimum.

In order to establish reflective shrinkers as a once-and-for-all solution to
validity preserving shrinking, we plan to formalize and prove the above
properties. Specifically, we want to show that, for some class of reflective
generators, the associated reflective shrinkers (1) never produce larger values
with fewer choices and (2) always produce shrunk values that are ``close'' to
the original. Formalizing sub-classes of well-behaved reflective generators and
notions of ``shrinker closeness'' will also give valuable insights into other
applications of reflective generators and shrinkers.

We mentioned in the Motivation section that participants in the study thought of
shrinking as magical, but currently the abstraction is too leaky for them to
be comfortable ignoring it.  Our hope is that eventually, having a reflective
generator on hand will mean that shrinking, even in the face of complex
preconditions, is {\em actually magic}, requiring no developer intervention and
simply making counterexamples smaller and easier to understand.

\SUBSECTION{Benchmarking}{sec:benchmarking}{1}{3}{Other}
\todo{Make sure this feels different from Leo's CAREER}
The many papers in the PBT literature demonstrate effectiveness with case
studies, showing that certain bugs in certain systems are caught more quickly
with one too over another. For theoretical advances, this is often sufficient
to demonstrate that the paper is worth publishing, but this kind of evaluation
can be hard to interpret from the perspective of a would-be user. With all of
the new approaches to generation that we are proposing in this document, and
considering our goals around usability, we want to do better.

We will to develop and popularize a robust empirical evaluation framework for
generators and other PBT techniques. Our first contribution will be an
infrastructure for easily and extensibly running experiments.  By ``easily,'' we
mean that we will take on the burden of collecting data and analyzing the
results, exposing to the user library functions for their particular
instantiations as needed. We will evaluate a given tool based on (1) the degree
to which it is able to achieve high code coverage quickly, and (2) the speed
with which it finds bugs that have been pre-seeded in example programs. By
``extensibly,'' we mean that in addition to the two languages (Haskell and
OCaml/Coq), multiple frameworks (QuickCheck, SmallCheck, QuickChick, etc.), and
numerous workloads that we plan to support on release, we will design the
infrastructure so that users can easily add new things along each dimension.

Our second contribution will codify a library of case-studies and examples as
{\em benchmarks for PBT}. Similar suites of benchmarks already exist in the
fuzzing literature~\cite{hazimeh_magma_2021}, but those benchmarks are not
organized around the particular challenges that PBT tools face. In particular,
few of the benchmarks deal with the kinds of complex preconditions that PBT
tools are built to handle. We want to establish a set of challenging tasks that
can serve as a north star for future improvements to PBT generators and
bug-finding strategies (including our own!).

Designs for this project are currently being discussed with Leonidas
Lampropoulos and his group at the University of Maryland. PI Pierce has a long
history of successful projects with Prof.
Lampropoulos~\cite[etc.]{LuckPOPL,goldstein2021dojudgeatest,lampropoulos_coverage_2019,Lampropoulos&18,OLDlampropoulos19fuzzchick}.
\iflater
\hg{Is this everything? Probably not...}
\fi

\SECTION{Validation}{Understanding Testing Effectiveness \pagebudget{3}}{sec:val}

One of the unique challenges in creating usable property-based testing is
providing adequate support for evaluating test results. Testing with properties
is fundamentally different from conventional unit testing tools in many ways.
First, it becomes a task in and of itself to understand individual inputs. This
because the inputs are not written by the developer, but rather generated
automatically, and furthermore, they can be of unbounded structural complexity.
Second, a developer needs to assess the quality of their tests. The quality of a
property-based test depends in part on the extent to which it covers execution
pathways through the code, though it also depends on the quality of the inputs.
Namely, are the inputs complex enough, and distributed in such a way that they
are likely to trigger practically important bugs? Third, developers need to
decide what to do with failed test cases, and may need to spend time migrating
failed test cases into regression suites. Finally, developers need to make
difficult decisions around how much computation time to budget for exercising
the property-based tests.

Each of these points of departure from conventional testing methodology
introduces new challenges into the testing process. And they require new
approaches to tool design to help developers reason about complex distributions
of complex inputs. In this section, we describe a sequence of research projects
we will undertake to bring about usable developer tooling for property-based
testing. These projects will contribute new paradigms for tools that help
developers understand program failures involving complex inputs
(\sectionref{sec:failures}), assess whether their generators are generating
sufficient, appropriate inputs (\sectionref{sec:evaluating_distributions}) and
whether those inputs sufficiently exercise their code
(\sectionref{sec:tuning}), and migrate failed property-based tests into regression
tests (\sectionref{sec:counter}).
% and decide on compute budgets for their
% property-based tests (\sectionref{sec:more}).
These projects will be pursued
using human-computer interaction methodology, integrating these tools into
contemporary development environments for functional programming. The result of
this work will be a comprehensive, innovative set of design primitives for
helping developers get work done in the challenge setting of reasoning about
tests with an overabundance of input-output examples.

% \amh{I think I want to organize this section chronologically around the
% following sequence of tasks: (1) understanding (and debugging) a single input;
% (2) understanding distributions of inputs; (3) tuning distributions of inputs;
% (4) integration of specific inputs into regression tests; (5) supporting good
% decisions around ``time budgets'' for PBT.}
% \amh{Some notes on framing from a prior meeting with HG and BCP: When we write
% about HCI-related parts of the projects, I think the right tack is to describe
% this as an area of HCI-oriented programming tools that merits additional
% exploration in tooling of the following types: X, Y, Z.  We will draw
% inspiration from adjacent areas, though the idea is to establish some of the
% first work in usable multi-example software testing.  This requires entirely new
% ways of expressing and reviewing tests, and could have
% potentially transformative impact within interactive programming systems in the
% long term. (Can we flesh out just how big a field we feel this is?)}

\SUBSECTION{Understanding Failures}{sec:failures}{3}{4}{Other}

With property-based testing, one of the challenges to evaluating test results is
simply understanding individual inputs, which may be complex and trigger bugs in
ways that are difficult to understand. We will design interactive tools that
make it easier for developers to understand the structure of inputs that trigger
failures, and to understand the nature of those failures. First, we will design
tools that help develoeprs simplify inputs into ones that are easier to
nderstand. One method of doing so is through automatic shrinking, a well-known
technique in the  PBT
literature~\cite{hughes_quickcheck_2007,arts_shrinking_2014}, and an area in
which we will innovate in this project (see \sectionref{sec:shrinking}).  Though
even with the advances proposed in this project, shrinking can be opaque, and
often make suboptimal decisions that an individual developer might be able to
make better.

Drawing inspiration from approaches in recent HCI literature that have supported
the interactive reduction of code through iterative, incremental
experimentation~\cite{ref:lim2018ply,ref:head2018interactive,ref:holmes2012systematizing,ref:hibschman2016telescope},
we will design aids for rapid, incremental, interactive shrinking of complex
inputs into simpler inputs. The key feature we will develop is the ability to
shrink inputs by pruning the input's contents and structure in an interactive
object viewer, similar to the kinds of object viewers available in contemporary
debuggers like in the JetBrains IDE~\cite{tool:jetbrains}. The key idea for
interactive shrinking will be to point out to developers the \emph{valid} ways
in which an input can be pruned. These valid options for pruning can be
collected as metadata for the input as they are generated by a reflective
generator; in other words, many choices that the generator makes correspond to
an aspect of the input that could be pruned. These pruning points will be made
visible in the object viewer. As a developer prunes the input, they will receive
continuous feedback as to whether it still causes a test to fail or not, to help
the developer determine whether the simplified input in fact continues to
trigger the same error. The end result will be that developers will be able to
reduce complex inputs into simpler ones that trigger failures, which will in
turn be easier to reason about when locating causes of failure in the code.

% Developers will also be alerted if the execution path
% through the code has changed as a result of shrinking the input. In some
% circumstances, this will indicate an undesirable change in the shrunken input,
% and in other cases, such changes in execution paths may be permissible (e.g., if
% the change in the input has led to a reduction in the number of cycles through a
% loop).

% \amh{Could we indicate on top of an object explorer which
% parts of the structure, if changed, would end up leading to
% a different test result, or preserving it?}
% \hg{To a point... You'd run into exponential blowup pretty much immediately, but
% depending on the scale of the input it could be doable}

% Other related projects:
% Whyline~\cite{ref:ko2009finding}.

In addition to developing novel interaction techniques for simplifying inputs,
we will also develop systems for helping developers locate code that, if
changed, should resolve the failure. Rather than explicitly encoding
relationships between generated outputs and their dependencies on
code~\cite{ref:ko2009finding}, we will instead take the approach of helping a
developer understand where the execution paths of a counterexample diverges from
a successful yet very similar input. Leveraging the parametric nature of the
reflective generators developed in \sectionref{sec:reflective}, we will generate
additional inputs that are very similar to the counterexample that are pass the
test, assuming that inputs that share most of their structure and concrete data
will trigger execution paths are also very similar themselves. Then, we will
execute the program up to the point where the traces of the programs begin to
diverge. Finally, we will drop the programmer into a debugging environment where
they can query the state of the program and step through the remainder of the
execution. PI Head has prior work designing debugging tools that help
programmers understand trace divergences in an educational
settings~\cite{ref:suzuki2017tracediff}; the work of this project would be to
bring this technology into professional programming environments where traces
for similar inputs are abundant by the nature of reflective generators.

\SUBSECTION{Evaluating Data Distributions}{sec:evaluating_distributions}{1}{2}{Other}

In comparison to testing techniques like unit testing, PBT focuses on testing
distributions of inputs, rather than individual inputs. The success of running
property-based tests depends on a developer's ability to create a distribution
of input data that is sufficiently realistic and comprehensive. One of the major
aims of our work is going to be to develop much-needed tooling around
understanding distributions of input data and more easily changing them. This
was a pain point of participants in our study, many of whom stated that they did
not know the kinds of input their generators were producing.

We draw inspiration from related work in HCI that has sought to better expose
the shape of input data distributions in the domain of machine learning in the time of models
that require massive datasets (e.g.,~\cite{ref:hohman2019gamut} and
~\cite{ref:hohman2020understanding}) and the sequence of values that programs
take during single executions (e.g.,~\cite{ref:kang2017omnicode}). What is unique
in our outlook is a focus on helping developers understand distributions of
complex inputs (e.g., lists, trees, and other algebraic data types). Consider an
example from a participant in our formative study, who wanted to generate
realistic logs of input data, where each log entry included at least a timestamp
and an event type. Such values are not trivially plotted in conventional
visualizations, and it would be prohibitive to review individual examples if the
logs are sufficiently long. Ideally, a developer would be able to answer
questions like, are the generated log inputs long enough? Are the even sequences
realistic?  This setting requires new kinds of views of data, and tight
developer support for easily defining meaningful views of the data.

\begin{wrapfigure}{r}{0.6\textwidth}
  \centering
  \includegraphics[width=0.6\textwidth]{assets/gen-vis.pdf}
  \caption{An envisioned tool for evaluating input data distributions.
  (1) A developer invokes the tool from controls that appear directly
  next to their property in the editor. They are supported in evaluating the
  input data distribution with (2) aggregate statistics describing the generated
  values, (3) visualizations of distributions of key features of the input, and (4)
  example values, pretty-printed or represented with DOT graphs. The developer can define additional
  features and views in their functional code.}\label{fig:gen-vis}
\end{wrapfigure}

To this end, we will design new interactive tools that provide rapid,
informative views of input data distributions. The tools will address the
challenges of visualizing generator distributions using a novel combination of
tailored, tried-and-true features for interactive programming environments.

First, the tool will support live, realtime displays of generated values.
Because property-based tests take some time to run, our first goal is to provide
instant, live~\cite{ref:tanimoto1990viva} feedback on the generators. Building
in the tradition of other live functional programming environments
(e.g.,~\cite{tool:lighttable,ref:omar2019live}), our environment will provide
live feedback on the many inputs associated with the code, rather than a single
input. As the test runs, our tool will sample inputs output by the generator and
pipe them into data displays (Figure~\ref{fig:gen-vis}). These data displays
will first and foremost show aggregate data views, including aggregate
statistics (Figure~\ref{fig:gen-vis}.2), and visualizations of the distribution
of key features of the data (Figure~\ref{fig:gen-vis}.3). Visualizations will be
generated according to simple recommendation rules, similarly to other recent
exploratory data visualization tools from
HCI~\cite{ref:lee2021lux,wongsuphasawat_voyager_2016,
wongsuphasawat_voyager_2017}. Unique to our project, features to visualize will
be based on awareness of common features of alebraic data types, and extensible
through lightweight user-written code. Consider the \lstinline{log} type
described above. A developer might be interested in the log's
\lstinline{length}, field accessors like \lstinline{event_type}, \lstinline{id},
and \lstinline{timestamp}, filters like \lstinline{is_empty}, and even
aggregators like \lstinline{max_by}. These kinds of features can be generated
automatically for the most common data types.
% Then the tool will then use
% lightweight type-based program synthesis to compose and combine these functions
% to get features. It may choose to show the \lstinline{length} of a log, but also
% the \lstinline{max_by (fun l -> length l.payload)} (the maximum payload length),
% and even pairs of features like these (which could be viewed as a two
% dimensional feature).

Second, the data displays will be easily extensible, providing lightweight hooks
for customizing aggregate data displays and previews. If
there are features that the user notices should be extracted, but that the
system cannot come up with itself (e.g., \lstinline{ids_unique}) the user can
write it themselves in companion code alongside their property specifications;
the interface will automatically load those features into the display.

Third, the interface will make it possible to drill-down into individual inputs.
In many cases, aggregate statistics will not be enough for understanding whether
the right kind of inputs are being generated. A developer will be able to access
sampled inputs from a list of samples. This list of samples can be interactively
filtered by selecting marks visualizations (e.g., a developer can choose a bar
for inputs of length ``10'' to preview individual inputs with that length). One
challenge will be to provide suitable representations of complex inputs that
will be easy to understand. The most general-purpose solution will be to
pretty-print solutions, provide interactive object browsers like those available
in JetBrains~\cite{tool:jetbrains}, and to allow a developer to explore an
object using a built-in REPL. Additionally, we will produce DOT
graph~\cite{ellson_graphviz_2002} representations of common kinds of inputs
(i.e., lists, trees) that will provide an at-a-glance understanding of inputs up
to dozens or hundreds of elements in length.

Finally, the tools will provide live feedback on completeness of the input
distribution in the form of in-situ coverage feedback. Like other HCI research
prototypes that have shown which lines of code are currently executing in a
running
program~\cite{ref:brandt2010rehearse,ref:oney2009firecrystal,ref:burg2013record},
developer's will be able to see lines of code in their editor colorized on the
basis of how frequently they have been executed while running the property-based
tests. This will let a developer decide whether their generator is executing the
paths in the code that they believe are particularly error-prone.
\iflater
As the project evolves, and once ``Bringing Fuzzing into Focus'' from
\sectionref{sec:fuzzing} is complete, we even plan to provide users with
visualizations of code coverage feedback.
\hg{This feels weak here, let's see how
the end of the section ends up looking}
\fi
When complete, this project will be the first live tool for generator
feedback and analysis; in the next section, we take it one step farther to
consider how interactive tools can help a developer not just understand input
data distributions, but more directly change them.

\SUBSECTION{Tuning Data Distributions}{sec:tuning}{3}{4}{Harry}

What should developers do if their property-based tests are testing the wrong
inputs? Tuning generators is a challenging task,
often requiring significant trial-and-error through an opaque process of
changing generator parameters and hoping that the input data distribution will
be updated in the right way. This process, however, often does not pay off.
Ideally, developers could describe the kinds of inputs they want directly,
rather than tweaking generator parameters and hoping it leads to the right kinds
of input data distributions.

We will design tools to support the direct tuning of input data distributions
through manipulation of generated inputs and input data distributions. This
work will build on the foundation developed in the prior project
(Section~\ref{sec:evaluating_distributions}). Inspired by recent tools in the PL/HCI literature
for bidirectional manipulation of programs and their
outputs~\cite{ref:hempel2019sketch,ref:kery2020mage,ref:omar2012active,ref:omar2021filling},
we explore the unique affordances of reflective generators in their ability to
support careful tuning of generators.

The first, general-purpose method for tuning input data distributions will be to
define filters on input data through interaction with aggregate data displays.
For instance, developers will be able to select ranges of values from a bar
chart showing input data features and then request that all values generated
within that range are discarded before testing. This is a particularly
compelling approach for its flexibility, though it is notably coarse-grained;
it does not influence the implementation of the underlying generator, and
therefore can only go so far in influencing the kinds of values that are
generated.

A second innovative method is to leverage the technology we have developed for
reflective generators (Section~\ref{sec:reflective}) to change the underlying
generators. For some features, as a developer manipulates marks in the data
distribution visulaizations, those manipulations can be mapped back to choices
in the generator (e.g., what value to place in a node, or the number of nodes to
produce, etc.). We envision building tools that show the generator code
side-by-side with visualizations, and where parameter choices in the generator
can update live as the data distribution is manipulated in the visualizations.
Furthermore, developers will be able to interact with individual data points,
expressing that they would like to see more inputs like one that has already
been generated, or that they would like an input similar to a generated input,
but different in a way that they have demonstrated. Together, this tool and the
tool for visualizing generated data distributions will have a synergistic effect
in improving dewvelopers' ability to understand and ultimately achieve more
realistic, comprehensive data distributions for their property-based tests.

\SUBSECTION{Counterexamples as Regression Tests}{sec:counter}{1}{1}{Other}

After a developer has identified a failure in their property-based tests, they
often wish to turn that failure into a regression test, to ensure that later
changes to the code will not reintroduce the failure. One pain point experienced
by informants in our interview study was that it requires considerable work to
transform a failure that was already detected by their PBT tools into a
regression test, despite the fact that much of the work involved in doing so
felt mechanical.

We plan to develop usable tooling for transforming failed PBT tests into single
regression tests. What is important to note about this project is that creation
of regression tests is \emph{mostly}, but not entirely mechanical. In reality,
the creation of regression tests will likely require judicious incorporation of
the developer's input at key decision points. This is particularly the case for
specifying acceptance criteria. For instance, consider a property that checks
that a list insertion function never produces an empty list. In the event of a
failure, a developer may want to produce a regression test checking the
exactness of the result on the failed input (e.g., checking that the insertion
produced a particular concrete list) rather than simply checking that the output
list is non-empty. The act of writing regression tests involve several such
choices, including whether to test for exact output, whether to test
intermediate results, and how to initialize inputs.

We will develop an interactive tool that assists developers in creating
regression tests from failed property-based tests. The idea is to first develop
technology for generating sufficiently readable code for regression tests (using
approaches such as Daka et al.'s~\cite{ref:daka2015modeling}), and then provide
in-situ editing assistance along the lines of contemporary interactive
refactoring tools from the HCI
literature~\cite{ref:head2018interactive,ref:barik2016quick,ref:murphyhill2008refactoring,ref:lee2013draganddrop}.
For this unique task of transforming property-based tests into regression tests,
our tool will provide the key features of keeping track of the failed input,
generating starter test code, substituting in correct expected values of the
output by executing corrected code, and then supporting developers in rapidly
performing likely edits to regression tests by providing suggestions to
alterations to their code like the ability to generate general property checks
with precise equality checks.

% \ifdraft
% \SUBSECTION{Increasing Assurance with More Tests}{sec:more}{3}{4}{Other}

% The informants in our interview study admitted to heuristic
% approaches to determining how many tests to run. Many set
% rigid cutoffs such as one-thousand tests, or one minute's
% worth of tests. One reason for these heuristics was that
% developers did not want to lock up their computer's
% computational resources, or delay their other development
% tasks, by waiting for their property-based tests to
% complete. As one of our informants pointed out, this could
% lead to circumstances where bugs were not discovered until
% property-based tests were checked into the continuous
% integration system, when tests were finally conducted at a
% sufficient volume to generate failing inputs \amh{@HG can
% you fact check my paraphrase of this informant's input?}\hg{Technically this was
% a pilot study informant, but I think it's OK}.
% Unfortunately, when failures are detected in continuous
% integration, developers no longer have the context to
% address those problems easily, because they have likely
% moved on to other tasks.

% We believe that, if appropriately designed, a developer's
% PBT tools can help developers get the best of both
% world: they can both increase assurance that their software
% is correct by running more property-based tests, while not
% locking up their computer's resources. We propose to extend
% in-editor testing tools to permit options where developers
% can configure their PBT test drivers to continue testing
% software in the background by default, without requiring a
% developer's explicit input. These tools will scan a
% developer's environment for property-based tests, divide
% time proportionally between the various tests, and capture
% the results in a digested form. To reduce the likelihood of
% breaking a developer's focus, errors will be marked through
% subtle annotations in the code (e.g., icons in the line
% margins) when one has been detected. Developers can also
% request alerts that can report failures instantaneously,
% should they wish to address any discovered failures right
% away.

% \hg{This section needs honing; right now it's just what I wrote in my thesis
% proposal, and it's too high-level} \amh{Is this a user
% interface project devoted to figuring out ways of invoking
% and keeping tabs on long-running tests during solo
% programming? Or is it a project around proposing better
% software engineering process that gives the right amount of
% time to long-running PBT tests?}
% \hg{Good question. I kind of naively hoped it was a UI project that both helped
% users invoke and keep tabs of tests in the background AND pushed them towards
% software engineering processes that gave PBTs appropriate space to operate. But
% now that I think about it that seems like a lot at once. Honestly I think the
% cultural shift in SE (potentially backed by some large-scale software tools) is
% the more impactful angle, but I don't know if we can argue that we know how to /
% want to do that}
% \bcp{Delete the section?}
% \amh{I tried this section again, trying to spell out in more
% detail my updated conception about what would be useful and
% magical about this tool. Can someone else take a look at it
% and make the decision of whether to remove the section?}
% \hg{I think it's better, but it's still pretty vague and flat. I wish there was
% a specific example of an interaction that I could say ``oh yeah, why don't I
% have that?'' As written, it sort of feels like the devil is in the details and
% we didn't actually give an details}

% % It is likely that PBT tools could play a role in improving this state of
% % affairs. For example, one could take inspiration from some theorem
% % provers~\cite{berghofer2004random} and create a system in which properties are
% % checked locally but in the background, as the programmer works on other things.
% % This avoids waiting time while potentially being less frustrating than running
% % in CI, since bugs would likely be found while the programmer still had the code
% % ``paged in.'' Alternatively, one might design a PBT system with CI in mind,
% % providing automated features for deferring property failure notifications until
% % a specified time or turning failing properties into unit tests that can be saved
% % for future testing.

% \fi

\bigskip
\bcp{Could this go at the top of the section (and be shorter)?}
The projects in this section must all be designed in concert with real users
evaluated in terms of their impact and usability. We will design these tools
using {\em human-centered design}: we will show prototypes to potential users,
get feedback, redesign, and repeat, until we are satisfied that our design will
solve users' problems effectively. When evaluating the designs, we will conduct
short user studies, comparing a users' experiences with our tools to their
performance on the same tasks without those tools. Designing and evaluating in
this way will increase our potential for impact and keep our work grounded in
the needs of real users.

\SECTION{Education}{Advancing PBT in the Broader Culture \pagebudget{1}}{sec:ed}

Our goal with this project is to make PBT not only usable but {\em
  used}; educational activities will play a crucial role.  This
section describes three specific threads of activity addressing pedagogical
challenges in different domains:
(1) course materials for instructors who want to integrate PBT into
undergraduate data structures courses,
%
(2) guidance for
industrial developers on how to identify low-hanging fruit, and
%
(3) a
tool for helping both developers and students write properties
interactively.

\SUBSECTION{PBT for Undergrads}{sec:1210}{1}{2}{Other}

\writeme{Write me.}

\begin{itemize}
\item One great way to increase industrial usage of PBT is to make it
part of the standard undergraduate curriculum.
\item There are several places where this could happen---compilers,
functional programming courses, intro courses---but we believe the
most promising place to start will be a data structures course.
(Partly because this has already happened for intro courses---cite
Brown folks, etc.)
\item Reasons:
  \begin{itemize}
  \item Data structures is often taken early in the curriculum, so
  habits can carry over to other courses even where PBT is not
  explicitly used.
  \item ``Obvious'' properties are often readily available (e.g.,
  algebraic specifications, inefficient reference implementations)
  \end{itemize}
\item Concrete plan:
\begin{itemize}
\item Create a model for how to do it by adding PBT to
the 1210 homework assignments (HW is of course the most important
place to put it) and creating some lecture material explaining it.
\item Write a cs-ed paper about how others can do it.
\end{itemize}
\end{itemize}

\ifdraft
\todo{Prior work~\cite{wrenn2021using,nelson2021automated} Shriram papers?}
\fi

\SUBSECTION{When to Specify It!}{sec:whento}{2}{3}{Harry}
PBT is often described as a lightweight formal method.  One
might therefore imagine that a central challenge of using PBT would be
coming up with specifications. Indeed, a preliminary interview
study before the larger Jane Street study seemed to indicate just
that~\cite{ref:goldstein2022some}. But at
Jane Street we actually heard very little about trouble
writing
properties; instead, most participants described applying PBT in scenarios when
properties were already available or quite easy to imagine. This suggests that,
while PBT educators should certainly teach developers how to
write properties, they should concentrate on
helping developers quickly recognize situations where PBT is a
natural fit because the properties are obvious!

The Jane Street study also gives us a solid start on a comprehensive list of
``no-brainer properties.''  For example:\bcp{These need to be written
  more intuitively (e.g., like point 2)}
\begin{enumerate*}[label=(1)]
\item code that is already formally or semi-formally specified,\bcp{This seems like a non-example?}
\item ``these two functions (e.g., a parser and a printer) should
round trip,'' \bcp{add participant numbers??}
\item pure data structures,
\item modules with invariants,
\item related versions of the same code,
\item programs that may fail catastrophically,
and
\item stateful APIs with well-understood contracts\bcp{e.g.?}.
\end{enumerate*}
These patterns cover a range of
scenarios that real software developers regularly find themselves in, but they
are likely not exhaustive. To flesh out the list, we will continue to
gather examples; specifically, we will include
questions about natural use-cases on the survey of the Hypothesis user
community described in~\sectionref{sec:survey}; we will also mine
research papers and experience reports in the literature,
examine open-source software projects, and leverage our connections with various
PBT communities (QuickCheck/Haskell, Hypothesis/Python, Quickcheck/OCaml, etc.)
to ensure that we have a comprehensive understanding of the best uses for PBT.

These results will be presented in a survey-style paper (with an
accompanying talk), tentatively entitled {\em When to Specify It!} in
homage to John Hughes's {\em How to Specify
  It!}~\cite{HowToSpecifyIt}, a famous tutorial on the many different
kinds of properties that can be written or a given piece of code.

% Clearly it is important to ask {\em How}, but our developer interviews
% suggest that it may be even more important to have a clear sense of {\em When}
% to use PBT. Our paper on {\em When to Specify It!} will combine examples from developer
% interviews with ones from our our years of experience studying and applying PBT
% into the definitive guide for the most impactful opportunities to apply PBT.

\SUBSECTION{Interactive Property Specification}{sec:interactive}{3}{4}{Other}
\bcp{I'm not sure any more that this belongs under Education.  It's
  pretty researchy, and it's not clear what the pedagogical angle is.
But maybe it can be revised to fit better here.}

\hg{New framing idea: We know because of pilot (and anecdotally) that imagining
properties is hard, esp for beginners. Tools exist to suggest properties, but
they're not usable. We want to make tools like QuickSpec usable for students so
they can apply PBT on their own without as much hand-holding. Evaluate on its
ability to suggest properties for the Penn CIS curriculum}

% \bcp{General comment for this section: It all seems good, but it is a bit vague
% and high level---I feel like technical readers will want a bit more meat.  Could
% we spice it up with a concrete example?} \amh{Agreed, I think a concrete
% example would be \emph{great} here. Maybe there is an example we can bring in
% from the QuickSpec paper~\cite{ref:claessen2010quickspec}}

% \bcp{make it shorter!}

% \amh{Examples of complex properties that might not be
% straightforward to implement. Algebraic properties of
% libraries.  Reversing a list twice comes up with the same
% list.  Properties of regular expressions. Maybe this would
% be best as an educational tool. For instance, helping
% students figure out how to test their code. Helping people
% modularize their existing systems (suggestions to remove
% depenedencies on Swing so that it is more testable, etc.).}

We also heard from Jane Street developers about a number of scenarios
where properties were {\em not} obvious but where PBT delivered
sufficient benefit that it was well worth taking the trouble to
discover them.  \bcp{Tune this to emphasize the cs-ed context.}  To
help developers see what to do in such scenarios, we plan to build a
tool that can work with developers to decide on a set of significant
properties to test.

Prior research has demonstrated that automated tools can extract
specifications of a program's behavior~\cite{ref:ammons2002mining,
  ref:le2018deep, ref:claessen2010quickspec}. We are interested here
in the non-trivial research problem of integrating such techniques
into usable developer tools. This will involve addressing several
challenges:
%
First, \textit{generated properties should be \underline{important}}. Any non-trivial
program can be characterized by an overwhelmingly large number of properties,
many of which describe only incidental aspects of the program's behavior that do
not need to be tested. How can tools produce those properties that developers
would want to have tested? We believe that this is a problem that can be best
solved with a mixed-initiative approach~\cite{ref:allen1999mixed}, where properties
are determined by judiciously incorporating both developer input and automated
techniques.
%
A developer could guide a specification mining tool to extracting relevant
properties through mechanisms such as (1) identifying regions of code that
are likely to lead to an adverse behavior such as an exception or a logical
error; (2) providing unit test cases that focus on a special case of a
generalized
property; and (3) indicating aspects of interest on input and output data during
exploration in a debugging REPL.
%
% Second,
% \textit{generated properties should be
% \underline{readable}}.
% , there are some variants about systems that may be so complex that
% they require significant comprehension time for users (i.e., those involving a
% large number of clauses). In such cases, a tool may way to generate simpler
% variants of properties first, and allow developers to refine them on their own.
%
Second, \textit{tools should help developers \underline{revise}
  properties}. If a generated property is too strict,
the programmer should be able to mark
a counterexample that was generated by the PBT tool as spurious. If
too relaxed, they should be able to provide a
counterexample that {\em should} be marked as a failure.
% In each of these cases, the
% property generator may have generate multiple properties for a developer to
% review, each of which may satisfy the refinements that a developer has provided.

We will develop this tool as
a VSCode extension. In our first iterations, we will generate
candidate properties using QuickSpec~\cite{ref:claessen2010quickspec}, a tool that
builds on Haskell's QuickCheck library to synthesize a series of plausible
properties about a given function. Future iterations will include
techniques from tools like
\todo{cite other quickspec follow-on}~\cite{smith_discovering_2017}.
%
PI Head has extensive experience with designing and developing tools
involving program
analysis~\cite{ref:head2018interactive,ref:head2019managing} and
program synthesis~\cite{ref:head2017writing} and with extending the
VSCode environment~\cite{ref:head2020composing}.

% I hope to work with the course staff of CIS 1210 to incorporate PBT into the
% curriculum. The ideal scenario would be to add a PBT thread throughout the
% course, giving students the tools to specify and test their code as they go.  I
% plan to follow the lead of others who have done similar things before (e.g., the
% PL folks at Brown University) to give students the best chance at incorporating
% PBT into their tool-set.

% There are a few important challenges that need to be considered in order for
% this to work out.  The curriculum is already quite full, so adding PBT likely
% means removing something else. I will need to work with the professors currently
% teaching the course to find room, but I expect that this process will be fairly
% difficult. It is also possible that adding PBT will actually make parts of the
% course {\em easier}, especially for students with some knowledge of logic and/or
% less well-developed unit testing instincts. Honestly I think this is a good
% thing, as it gives students more ways to succeed and it may re-enforce the value
% of PBT, but some may find this problematic.

\bcp{There must be more work on automatically generating
  specifications, etc...  Need more citations...}

\immediate\closeout\workplanfile
\SIMPLESECTION{Plan of Work \pagebudget{.7}}{sec:plan-of-work}

% \begin{figure}[ht]
%   \centering
%   \vspace*{-1in}
%   \hspace*{-.4in}\includegraphics[width=1.1\textwidth]{assets/workplan.pdf}
%   \vspace*{-1.3in}
%   \caption{Plan of work.}\label{fig:workplan}
% \end{figure}

% \todo{Flesh out the chart in PPT; maybe eventually re-draw it in some
%   nicer form.}

\begin{figure}[ht]
  \centering
\begin{ganttchart}[
      expand chart=\textwidth,
      y unit chart=.4cm,
      %   vgrid
    ]{1}{4}
% \gantttitle{Ongoing}{1}
  \gantttitle{Year 1}{1}
  \gantttitle{Year 2}{1}
  \gantttitle{Year 3}{1}
  \gantttitle{Year 4}{1}
  \input{main.workplan}
\end{ganttchart}
  \caption{Plan of work.  \iflater\bcp{It's too big probably.  :-(  We can
      trim it a little by removing the main section headings.  After
      that, is it interesting enough to justify the space?  I guess we
    could alternatively include ``(Years X to Y)'' in subsection
    titles}\bcp{Can squeeze one line at the top by hacking a bit.}\fi
   % \bcp{The timeline needs to be brought into correspondence with our
   %   actual expectations of timing, and explained.  Harry's projects
   %   should end by year 3 latest!  :-)}
   % \hg{How's that?}
   % \bcp{Hmm---now it looks like there's not much happening in year 4!
   % Maybe we need to remove this constraint.  Also, there's not much
   % blue at the moment, and I'm not certain how to talk about the
   % distinction between the two students' domains of work.  Can we try
   % to make it more coherent (even at the cost of some accuracy
   % wrt. our expectations about what you're going to do)?}
}\label{fig:workplan}
\end{figure}

\writeme{Write me.}


\SIMPLESECTION{Broader Impacts of the Proposed Work \pagebudget{.5}}{sec:broader-impacts}

\writeme{Write me.}

\begin{itemize}
\item more secure and robust software
\item development of a diverse,globally competitive STEM workforce;
\item increased economic competitiveness of the United States
\item improved STEM education and educator development at any level;
\item mention jessica for the benchmarking project, plus female
interns; note that Andrew has 2/3 female PhD students and BCP has one
\item CACM article
\item increased partnerships between academia, industry, and others
(JS, Hypothesis)
\item enhanced infrastructure for research and education (tool
building).
\end{itemize}

\paragraph*{Benefits to Society} \todo{increase software reliability,
  tech industry, ...}
%
All the tools we develop will be freely available, making the results
of our research directly useful to students, industrial software
engineers, and developers of similar tools targeting different
languages and programming environments.

\paragraph*{Educational Benefits}
Specific pedagogical threads within the project are described
in~\sectionref{sec:ed}.  Beyond these, more than
60\%\iflater\bcp{check}\fi{} of the requested funding will support
graduate students.  We also plan to work with undergraduate students
during this project; they, too will benefit from the research
experience. Each graduate student will have leadership responsibility
for multiple facets of the project, including co-supervising
interested undergraduate researchers.  See the Collaboration Plan for
more. \iflater\bcp{I guess we need to write one?}\fi


% The Project Description must contain, as a separate section within the narrative, a section labeled ``Broader
% Impacts of the Proposed Work". This section should provide a discussion of the broader impacts of the proposed
% activities. Broader impacts may be accomplished through the research itself, through the activities that are
% directly related to specific research projects, or through activities that are supported by, but are complementary to
% the project. NSF values the advancement of scientific knowledge and activities that contribute to the
% achievement of societally relevant outcomes. Such outcomes include, but are not limited to: full
% participation of women, persons with disabilities, and underrepresented minorities in science, technology, engineering, and
% mathematics (STEM); improved STEM education and educator development at any level; increased public
% scientific literacy and public engagement with science and technology; improved well-being of individuals in
% society; development of a diverse,globally competitive STEM workforce; increased partnerships between
% academia, industry, and others; improved national security; increased economic competitiveness of the United
% States; and enhanced infrastructure for research and education.

\SIMPLESECTION{Results from Prior NSF Support \pagebudget{.5}}{sec:prior}

% If any PI or co-PI identified on the project has received NSF funding (including any current
% funding) in the past five years, in formation on the award(s) is required,
% irrespective of whether the support was directly related to the proposal or not.
% In cases where the PI or co-PI has received more than one award (excluding amendments),
% they need only report on the one award most closely related to the proposal. Funding includes not just salary
% support, but any funding awarded by NSF. The following information must be provided:\\

\emph{\underline{PI Pierce}}: (NSF 1955565) ``Collaborative Research:
SHF: Medium: Bringing Python Up to Speed'' (\$437,999, 7/2020--6/2023), with co-PIs Michael Hicks (Maryland, now Amazon) and Emery
Berger (Amherst).
{\bf Publications (from Penn):}~\cite{DBLP:conf/esop/GoldsteinHLP21,goldstein2022parsing}.
{\bf Research Products (from Penn):} The {\tt pytest-mutagen} mutation
testing tool for Python~\cite{pytestmutagen}.

\emph{\underline{PI Head}} has not previously received NSF support.

% \SUBSECTION{Proposed Study}
% The Project Description should provide a clear statement of the work to be undertaken and must include:
% objectives for the period of the proposed work and expected significance; relation to longer-term goals of the PI's
% project; and relation to the present state of knowledge in the field, to work in progress by the PI under other
% support and to work in progress elsewhere.
%
% The Project Description should outline the general plan of work, including the broad design of activities to be
% undertaken, and, where appropriate, provide a clear description of experimental methods and procedures.
% Proposers should address what they want to do, why they want to do it, how they plan to do it, how they will
% know if they succeed, and what benefits could accrue if the project is successful. The project activities may be
% based on previously established and/or innovative methods and approaches, but in either case must be well
% justified. These issues apply to both the technical aspects of the proposal and the way in which the project may
% make broader contributions.

\iflater
\section*{More stuff to not forget :-)}

Unfunded collaborations: Any substantial collaboration with
individuals not included in the budget should be described in the
Facilities, Equipment and Other Resources section of the proposal (see
Chapter II.C.2.i) and documented in a letter of collaboration from
each collaborator. Such letters should be provided in the
supplementary documentation section of FastLane or Research.gov and
follow the format instructions specified in Chapter
II.C.2.j. Collaborative activities that are identified in the budget
should follow the instructions in Chapter II.D.3.  \bcp{Jane Street.
  And maybe we should ask John too?}

bpc.net - source materials for NSF BPC plans (they also vet plans).
  we should check that other universities are doing!

\fi
