\section*{Project Description}

Testing plays a vital role in modern software development,
contributing crucially to robustness and overall quality---as well as
to overall development costs.
%
It comes in many styles---unit testing, integration testing,
performance testing, stress testing, accessibility testing,
etc.---supported by many sorts of tools, with yet more advanced tools
and techniques continually being developed, studied, and applied.

One such technique, {\em property-based testing} (PBT), has been
enthusiastically taken up by the functional programming research
community since its introduction in the Haskell QuickCheck
library~\cite{ClaessenHughes00}, and is beginning to make
serious inroads beyond academia.
%
PBT is sometimes glossed as ``formal specification without formal
verification'': a developer characterizes the desired behavior of
some piece of code at a high level in the form of executable {\em
  properties}, which are simply Boolean-valued functions. The code is
then validated against these properties by running it over and over
with a large number of automatically generated test cases.
%
PBT tools thus give programmers an efficient way of testing their
code's behavior with a comprehensiveness that is often not
possible with alternative tools.

PBT's combination of rich, high-level specification with easy, mostly
automatic validation has proved effective at identifying subtle
bugs in an impressive variety of settings, including telecommunications
software~\cite{arts2006testing}, replicated
file~\cite{hughes2014mysteries} and key-value
stores~\cite{Bornholt2021}, automotive software~\cite{arts2015testing}, and a range
of other real-world systems~\cite{hughes2016experiences}. With support
now available in all major programming languages, PBT has
begun to make significant inroads in the broader software
industry. For example, the developers of the Hypothesis library for
Python estimate that the library is used by on the order of half a million
developers~\cite{ZacPersonalCommunication}.

\newcommand{\participant}[1]{{P#1}}
% \newcommand{\participant}[1]{{\bf P#1}}

With all of this momentum, one might wonder if the research community has
already
addressed all of the challenges that could limit PBT's adoption
in the broader software industry, but it
seems the answer is no.
In an ongoing need-finding study with users of OCaml's QuickCheck testing tool
at Jane Street Capital, we found
consistent enthusiasm for PBT---participants called it it
``obviously valuable'' (Participant \participant{1}),
built their own libraries for it when standard ones were not available in their
development context (\participant{8},
\participant{21}), and suggested that ``everyone'' at the company should use it
(\participant{20}). But we also found
challenging new research questions where advances in technical foundations and
tool design necessary to overcome present usability challenges and expand the
potential impact of PBT.

Addressing usability challenges in PBT requires insights from both the
programming languages (PL) and
human-computer interaction (HCI) communities.  Chasins et
al.~\cite{chasins_pl_2021} argue that a methodology
involving
both PL and HCI hits a ``sweet spot'' where need-finding techniques identify
current pain points and lead to concrete tools that help programmers write
safe, correct programs. We wholeheartedly agree, and such a methodology is
central to this research.
The team behind this proposal is uniquely positioned to advance the
state of the art in PL and HCI: PI Head recently co-founded
a new HCI group at the University of Pennsylvania and specializes in interactive
programming environments, while PI Pierce has
published widely on PL topics including PBT.

We propose a comprehensive, interdisciplinary research program on \emph{usable
property-based testing}, bringing the combined power of PL and HCI to bear to
accelerate PBT's transition into practice.
\begin{itemize}[noitemsep]
\item We will establish a \emph{foundation} for HCI-informed research on PBT,
beginning with our ongoing need-finding study. We will confirm
its findings and further explore PBT's usability with
two survey studies and observation studies
of PBT users {\em in situ} designed to lead to concrete tool designs
(\sectionref{sec:foundation}).
  \item We will empower developers to \emph{specify} properties with
  new tools for checking temporal properties over internal program states and
automating model-based testing of abstractions defined using OCaml's
module system (\sectionref{sec:spec}).
  \item We will help developers efficiently define and tune test-case
\emph{generators} with novel techniques for generating precondition-satisfying
values, test input mutation, and example-based generator tuning, building
  on an ``reflective'' approach to generation (\sectionref{sec:gen}).
  \item We will support developers in rapidly \emph{validating} testing
  effectiveness with novel interactive programming tools that help programmers
locate the cause of a test failure and understand how to improve their generated
data distributions to be more representative and comprehensive
(\sectionref{sec:val}).
  \item We will develop materials for PBT \emph{education}
  including an undergraduate computer science curriculum centered around PBT, a
  tool to help students author properties, and a comprehensive review of PBT
  practice to help students and developers learn about diverse situations where
  PBT is likely to be the most useful (\sectionref{sec:ed}).
\end{itemize}

Each of these efforts will deploy complementary tools from PL
and HCI research. HCI approaches will be used to concretely define developers' needs,
design the user interface to tools, and evaluate their success. PL approaches
will be used to develop PBT tools such as domain-specific languages and
type-based tools that are powerful and flexible, and to formally validate these
tools. We conclude the proposal with a concrete outline of our work plan
(\sectionref{sec:plan-of-work}) and discussion of the broader impacts of the
proposed work (\sectionref{sec:broader-impacts})

\subsectionstar{Orientation: Property-Based Testing}
%
We next describe the basics of the PBT approach, with the aim of highlighting
where research has unique leverage in improving tooling, processes, and
education around PBT.

Popularized by QuickCheck~\cite{hughes2007quickcheck} in Haskell,
PBT is a form of random testing~\cite{hamlet1994random} where
users write executable functions which act as partial
specifications of a function under test. For example, a user might
write the following property for an \lstinline{insert}
function for a binary search tree (BST):\footnote{The function can be read as
follows:
Assume an arbitrary tree \texttt{t} and an integer
\texttt{x} to insert into that tree. If the original tree
is a BST (``\lstinline{is_bst t}''), then it should remain
a BST after the insertion of \texttt{x} (``\lstinline{is_bst (insert x t)}'').}
\begin{lstlisting}
  prop_insert_correct x t  =  (is_bst t ==> is_bst (insert x t))
\end{lstlisting}
As is the case with properties generally, this property is a function which
takes in generated
test inputs and evaluates to \lstinline{True} if the test passes, and
\lstinline{False} otherwise.  This function, ``\verb|prop_insert_correct|'',
checks
that an insert operation on a binary search tree preserves the
binary ordering of the tree.
While properties like this one require
only a few lines to express, they can validate unlimited
input-output pairs.  Given a property like this one, the PBT tool generates a
large number of inputs and
checks that the property evaluates to \lstinline{True} for each input; any input
that causes the property to fail is reported as a {\em counterexample}.

This proposal focuses on {\em random}
generation~\cite{hamlet1994random}, in contrast to other
approaches like enumerative test-case
generation~\cite{DBLP:conf/haskell/RuncimanNL08, leancheck} and model
checking. Random generation is the dominant approach in PBT. Its
surprising effectiveness is often attributed to the
``combinatorial'' nature of larger test cases: bugs can be
exposed with test inputs that embody a specific combination of features,
independent of whatever other features are present. For example,
a bug may be triggered by some sequence of API calls in a
particular order,
regardless of whether they are
interleaved with other API calls. As a result, testing with large random
inputs often exposes the same issues as testing with exponentially more
enumeratively generated inputs.

From the perspective of a developer, applying PBT entails the following
steps. First, the developer defines one or more properties that should be true
of the program under test. Then, they design (or reuse, or tailor) {\em random
input generators} for the values that the properties take as input. Then, they
check the properties with generated inputs, using test drivers provided by their
PBT tools. And finally, if counterexamples are discovered, they make sense of
them and determine the source of the bug.
Each of these steps presents opportunities to improve the user experience of
PBT, as we describe in the next section.

\smallskip

Why go to the trouble of PBT, when more straightforward example testing is the
de-facto standard in the software industry? First and foremost, PBT has the
potential to be much more thorough than user-defined examples. As mentioned
previously, PBT has an impressive track-record uncovering bugs that other
approaches to testing had failed to find~\cite{arts2006testing,
hughes2014mysteries, Bornholt2021, arts2015testing, hughes2016experiences}. But
PBT is more than just thorough---it is actually more general than example-based
testing. Wrenn et al.~\cite{wrenn2021using} point out that example-based testing
of programs like that implement relations (e.g., topological sort, which may
produce one of a number of potentially correct results) is impossible to do
faithfully; a property-based specification is a much better choice. PBT is also
an obvious choice if formal properties have already been written, as is the case
with tools like QuickChick~\cite{paraskevopoulou_foundational_2015}, which
implements PBT in
the Coq proof assistant. Finally, PBT can act as superior documentation:
participants in our foundational study (\participant{5}, \participant{21})
talked at length about properties being an ideal way to communicate what a
program is supposed to do.

With so many potential advantages, it is clear that PBT should be a tool should
be a tool that software developers turn to without hesitation for validating
their software. We intend to make it that way.

\iflater\todo{Double-check that we've mentioned all relevant
  ``hgih-level'' related work somewhere.}
  \hg{My current decision is that Orientation should stay high-level RE PBT.
  Specifically, I think it shouldn't get into the weeds of generation
  techniques, including fuzzing. I added a bunch of references RE that later on
  in the context sections.}
  \fi

\subsectionstar{Motivations from a Formative Study of PBT in
Industry\label{sec:motivation}
\pagebudget{2}}
%
The goals of this proposal are motivated by findings from an on-going
need-finding study we have been conducting in industry. The purpose of this
study was to understand how the research community can make PBT more valuable
for software developers. This study has consisted of semistructured interviews
with 30 stakeholders in PBT, including both developers who have used PBT and
developers and maintainers of PBT tools. The site of the study is Jane Street
Capital. A number of aspects of Jane Street make it
an attractive setting for such a study.  Most importantly, PBT is
already well established at Jane Street, so there is a large
population of people with well-informed opinions on its benefits and
challenges. Additionally, Jane Street famously builds almost all of its
software in OCaml, a mostly functional programming language for which
there is a well-engineered PBT tool. This unified
ecosystem allows us to ensure that developers have access to mature PBT tools,
experience using them in collaborative settings,
and awareness of language-level programming abstractions necessary
to advanced usage.\footnote{Because our primary need-finding study to-date has
taken place solely at Jane Street, our follow-up studies in
\sectionref{sec:foundation} will engage with broader groups of developers using
PBT to expand and generalize our findings.}

As of December 2022, a pilot study has been conducted, analyzed, and presented
at HATRA~\cite{goldstein_problems_2022}. Following this pilot study, the full
complement of 30 interviews has been completed at Jane Street, and a preliminary
round of qualitative analysis is underway. Full-scale analysis will
begin in 2023. Our findings will be disseminated in a paper submitted
to a top-tier software engineering conference.

\subsectionstar{Framework: Challenges Using PBT} Upon completing
our analysis, we will contribute a deep, qualitative description of how
developers use PBT, what they need from it, and how the research community can
help improve PBT tools. While full analysis of the data remains
to be done, several themes have arisen from our initial analysis
of interview session notes. These form the backbone of the present proposal.

\newcommand{\proptheme}[1]{{\color{nord-orange} \em #1}}
\newcommand{\gentheme}[1]{{\color{nord-green} \em #1}}
\newcommand{\evaltheme}[1]{{\color{nord-purple} \em #1}}
\newcommand{\edutheme}[1]{{\color{nord-frost4} \em #1}}
A first set of
themes revolves around \proptheme{\normalfont \bf specifications} (i.e.,
properties).
% and the kinds of programs in which they choose to test them.
Since
PBT is often described as a lightweight formal method, one
might imagine that a common challenge would be coming up with the
specifications of desired program behavior. Indeed, in our pilot need-finding
studies with users of Python's Hypothesis library, users indicated just that.
By contrast, Jane Street developers, on the whole, reported
few difficulties coming up with
properties. Rather, most participants described applying PBT in
\proptheme{High-Leverage Scenarios} where properties were already
available or straightforward to invent. For example, participant
9~(\participant{9}) pointed out that PBT was particularly easy to apply when one
has ``a really good abstraction
with a complicated implementation.''
Several participants (\participant{3}, \participant{15},
\participant{20}, \participant{22}), when asked to speculate, guessed
that 80--100\% of Jane Street
developers write programs like this, where properties are easy to find, and
where PBT is relatively easy to apply.  This suggests that an effective way to
improve education and documentation around PBT is to highlight these real-world
applications that seem to be a natural fit for PBT.

It also became clear that there were \proptheme{Opportunities for Better
Leverage} of specifications, or situations where PBT is not easy {\em yet}, but
could be with a little more research effort. For example, many developers in
both our pilot studies (Pilot-\participant{4}, Pilot-\participant{5}, and
Pilot-\participant{6}) and the Jane Street study (\participant{7}) complained
that PBT was difficult when code was poorly abstracted.  We plan to provide
better tool support to address this failure mode.  Furthermore, more than three
quarters of study participants had used a particular approach to PBT commonly
called \proptheme{Model-Based Testing}.  \participant{3}, an author of PBT tools
at Jane Street, considered better automation and tooling around model-based
testing to be one of the most significant ways improve PBT and make it easier to
pick up and use.

Another set of themes concerned the \gentheme{\normalfont \bf generation} of
random inputs for PBT. Developers often spoke
highly of \gentheme{Derived Generators} that can be automatically inferred from
the OCaml type system (\participant{5} called OCaml's tools for this
``[expletive] amazing'' and \participant{30} called them a ``game changer'').
These generators are already quite good, but they could be better: participants
identified deficiencies both small (e.g., API quirks) and large (e.g.,
derived generators
cannot enforce semantic preconditions).

When derived generators
failed, participants fell back to \gentheme{Bespoke Generators}, which
are far more flexible but proportionally more time-consuming to design and work
with. For example, \participant{20} successfully used a bespoke
generator for XML documents to find bugs in their software,
but reported spending ``at least a day'' writing it.
Improving the abstractions available for writing bespoke generators would
greatly improve both the experience of using PBT and its potential for
bug-finding.

If a generated input is determined to be a {\em counterexample} that triggers
property violation, a developer will wish to inspect that counterexample to find
and fix the root cause of the violation. Developers often implemented code for
\gentheme{Shrinking} inputs, or taking counterexamples and discovering smaller,
simpler inputs that trigger the same bug.  \participant{8}
and \participant{21}, who each implemented their own PBT libraries, both
incorporated shrinkers as key components of their libraries. That said,
shrinkers need to be customized to particular kinds of data to be most
effective, and the reality is that they are time-consuming to build; many
developers
(\participant{16}, \participant{20} \participant{21}, \participant{30})
described shrinker implementation as an opaque and difficult process.
Ideally, developers would have tools that make the process of implementing
shrinkers fast and simple.

Further themes from the interview study concerned how developers
\evaltheme{\normalfont \bf validated} their tests.
Developers described their processes for \evaltheme{Evaluating the
Effectiveness} of their tests. Many
desired
better ways to evaluate their generators and properties, including
better feedback on code coverage (\participant{9} and \participant{25}),
incorporation of mutation
testing~\cite{papadakis_mutation_2018}, and understanding the space of generated
inputs (\participant{10}, \participant{16}, \participant{16}). Problematically,
while many developers admitted they would benefit from better tools for
evaluating their tests, many seemed to
\evaltheme{Implicitly Trust the Infrastructure} that they had not thoroughly
analyzed. \participant{14} actually shipped broken code to production because
their generator missed important input examples. We propose advances in tools
for validation that help developers more clearly understand, and improve, the
effectiveness of their tests.

Finally, for PBT tools to be more usable, we need progress in
\edutheme{\normalfont \bf
education}. Developers need to be informed about the \edutheme{High-Leverage
Scenarios} in which PBT should be employed, as we discussed above.
In our pilot studies, we found that developers with less experience with PBT
sometimes struggled to \edutheme{Imagine
Properties}, or understand what properties to test (Pilot-\participant{1},
Pilot-\participant{3},
Pilot-\participant{4}, Pilot-\participant{5}). Developers in both studies
(Pilot-\participant{1}, Pilot-\participant{4}, \participant{3}, and
\participant{11}) reported a dearth of \edutheme{Documentation and Examples} for
learning about PBT. Our final aim in this project is to develop curricula,
documentation, and tools that fill this educational gap.

We discuss plans for work that addresses these themes below, \proptheme{specification
  themes} in \sectionref{sec:spec}, \gentheme{generation themes} in
\sectionref{sec:gen},
  \evaltheme{validation themes} in \sectionref{sec:val}, and \edutheme{education
  themes} in \sectionref{sec:ed}.

\iflater
\amh{Let's foreshadow other findings that
we will publish that are outside of the scope of this proposal, but which might
be motivating to other researchers, including: developers could benefit from
additional automated support for deriving generators from types, effectiveness
budget (i.e., deciding how much time to spend on running PBT tests versus fuzz
tests), integration with unit testing frameworks, and exploring the use of
properties as a source of documentation.}
\bcp{Not sure where is the right place for this discussion (or if
  there will be space)...}
\fi

\SECTION{Foundation}{Understanding Needs and Opportunities \pagebudget{1}}{sec:foundation}

% \amh{This section is around double its page budget. One reason is that I am
% probably being too heavy-handed in trying to justify the methodological choices.
% Help me figure out what to prune!}\bcp{Did some pruning! :-)}

The final results from the in-progress user study should paint a clear
picture of the benefits and challenges of PBT in the specific context
of Jane Street and other organizations with similar characteristics.  But to
fully understand the potential impact of PBT across the software
industry---and the factors that may limit its adoption---we need to
cast a wider net.
%
In this section, we describe three planned studies, drawing on mixed
methods in the service of producing a comprehensive and actionable
agenda for future research, in this research project and beyond---two
written surveys, one to assess the generality of these needs and obstacles
and one to identify potential for adoption of PBT tools
(\sectionref{sec:survey}), and an observation study to understand
particular tasks involved in PBT to guide the design of new algorithms
and interactive tools (\sectionref{sec:observations}).

\SUBSECTION{Generalizing the Jane Street Findings}{sec:survey}{2}{3}{Other}
%
Our ongoing Jane Street study has already revealed a number of
opportunities to improve tools for property-based testing. To identify
others and to better understand which of these opportunities are most
important for the research community to explore, we will conduct two
surveys with broader samples of developers. These surveys aim to
(1) determine which obstacles observed in the interview study
represent widely experienced pain points with PBT tools and
(2) characterize the potential benefits of better tools to the
software industry as a whole.

\emph{Validation survey}. The main survey we plan to conduct aims to check that
the things we learned from Jane Street generalize more broadly.
Our interview study has revealed
a number of issues with existing PBT tools, but it is not clear
how broadly these issues are experienced by developers, or
their relative severity. To find out, we will
ask developers which of the issues we have identified are
ones they have experienced, and the severity of those issues
in their experience. Respondents will also be asked to report on
other issues they encountered in their use of PBT tools.
To provide
clear usage scenarios to guide future work, respondents will
be asked to write brief anecdotes elaborating on the
most severe issues they had.

Respondents will
be recruited broadly, from three sources. First, we will recruit
users of Hypothesis, a widely used Python-based PBT
framework
(see the attached letter of support).
Second, we will distribute the survey at Jane Street, hoping to reach
a broader set of developers than we were able to interview.
And third, we will recruit users from social media by posting
survey announcements from the PIs' Twitter and Mastodon
accounts, various mailing lists, and on discussion boards for conferences like
``Yow!'' where we have been invited to speak~\cite{noauthor_when_nodate}.
% Types and types-announce mailing lists. They are mailing
% lists for the types programming. We will distribute announcements among
% attendees of conferences like ICFP or ``Yow!'' whom have
% a cumulative thousands of programmers with functional
% programming experience.

\emph{Impact survey}. Another more speculative survey we hope to conduct asks
how broadly PBT may eventually be able to reach.  To get a sense of this, we
survey ``proximal'' users of PBT---that is, developers who do not use PBT
currently, but who experience use cases where PBT methods would be particularly
useful.  Particular attention will be given to those whose work requires the
writing of validation code and public APIs with some definition of types. We
will recruit a broad sample of participants across development contexts
(professional, open source, educational) by working with our industry contacts
and by recruiting over social media. Admittedly, this survey seems more
difficult to ``get right,'' and the design will evolve over time as we gain a
better understanding of the situations where PBT is most effective
(see \sectionref{sec:whento}), but if done right this could provide invaluable
feedback to PBT researchers about how far their work could reach.

\SUBSECTION{Observing PBT in Practice}{sec:observations}{3}{4}{Other}
%
Anecdotes related in interviews do not generally, in and of
themselves, provide enough information to inform the design of
effective tools---they beg questions like: (1) How much time
are participants willing to devote to
a task like creating a generator or debugging a counterexample when in the
middle of a programming task? (2) How much space is available on a developer's
screen (amidst other tools like code editors and terminals) for
interacting with new
tools? (3) What are developers' current strategies for solving the
problems they describe
(for example, what representations of generated data currently seem most helpful
for programmers trying to under the distributions of data generated by a
generator)?.  As a basis for designing tools with a substantial novel interface
component (see \sectionref{sec:val}), we will observe developers
undertaking the respective PBT tasks we
aim to support, on the order of a small handful of developers (i.e., 2--10
observation sessions). These observation sessions will allow us to evolve
singular anecdotes from the interviews into a more comprehensive picture of
what kinds of designs will be viable for helping programmers.


\SECTION{Specification}{Widening the On-Ramp \pagebudget{2}}{sec:spec}
The projects in this section address the challenges related to
\proptheme{specification} discussed in the motivation section. One major finding
of the study is that widening the on-ramp to PBT requires making developers
aware of high-leverage use-cases for PBT. We discuss our plans to teach developers
about high-leverage scenarios in \sectionref{sec:whento}; in this section, we
discuss opportunities to expand and improve the space of scenarios where PBT
shines.  In we improve PBT's applicability to poorly abstracted code
(\sectionref{sec:outpurprop}), and we describe tools that automate
automatically in a specific high-leverage scenario: model-based testing
(\sectionref{sec:automating}).  Together, these projects will greatly improve
the on-ramp to PBT and clarify its place in the software industry.

\SUBSECTION{Properties Over Logs}{sec:outpurprop}{1}{2}{Harry}
As discussed in the motivation section above, a common problem
raised by Jane Street
developers (and even more loudly in our earlier pilot
study~\cite{ref:goldstein2022some}) was that code may sometimes not be
abstracted in a way that is amenable to testing. Indeed, any kind of
testing of software units---with PBT techniques or otherwise---requires
``units'' to test!  But PBT is
especially sensitive to issues like poorly encapsulated global state, which may
impact the repeatability of testing, and leaky or confusing
abstraction boundaries, which make it difficult to cleanly state
properties or specifications.

For example, one developer (\participant{7}) described their experience testing a
system (we'll call it \lstinline{Inner}) with messy abstraction
boundaries. In this case, \lstinline{Inner} was hard to test because it was
primarily used as a sub-component of a larger system \lstinline{Outer};
\lstinline{Outer} would take in simple data, and then at some point pass a
large, complex data structure to \lstinline{Inner}, which would do some work and
hand results back to \lstinline{Outer}. PBT of \lstinline{Inner} was therefore
hard, because generating valid examples of the large data structure that
\lstinline{Inner} takes as input required deep knowledge of the internals and
types of \lstinline{Outer} that the developers of \lstinline{Inner} did not have
access to, and because the interface was too complex to write straightforward
properties about.

We propose a novel approach to testing systems like \lstinline{Inner} that are
difficult to write properties about because they cannot be fully extracted from
a larger system.
At its core, this approach tests
\lstinline{Inner} through \lstinline{Outer} by
writing properties over intermediate values in \lstinline{Inner}. Properties like
\begin{lstlisting}
  sublist_of processed (next (processed))      (* 1. Processing is monotonic. *)
  msg.length < 100 ==> never (overflow = true) (* 2. Capacity at least 100 bytes. *)
  eventually (processed.length = msg.length)   (* 3. The whole message is processed. *)
\end{lstlisting}
which demonstrate that internal variables like \lstinline{processed},
\lstinline{msg}, and \lstinline{overflow} evolve correctly over time, let the
developer talk about the behavior of \lstinline{Innner} without talking about
its external-facing API.
These properties could theoretically be tested without ever running
\lstinline{Inner} on its own: we can generate inputs to \lstinline{Outer}, have
\lstinline{Outer} drive \lstinline{Inner}, and simply monitor the variable
values to check the desired properties.

The tricky part comes with how to actually implement this testing. One might
hope to do it through debug assertions, but that quickly becomes untenable.
Checking the property (1) with debug assertions would require noisy code to
saves the previous value every time \lstinline{processed} is updated, and the
programmer would need to use macros or some other kind of meta-programming to
remove that code when building in release mode. Even worse, there may be no way
to check properties (2) or (3) at all. Doing so would require making an
assertion at the ``end'' of the computation, which may not actually be possible
if \lstinline{Outer} is driving \lstinline{Inner} from the outside.

Instead, we propose a framework for writing properties over {\em logged values}
that can capture the interesting relationships between past and future variable
values that the above properties demand.  Concretely, we will insert lightweight
logging annotations and provide a language for writing properties over those
logs. In order to express concepts like \lstinline{next}, \lstinline{never}, and
\lstinline{eventually} the property language will need some logical connectives
that are not standard in PBT.  In particular, we need a temporal logic like {\em
linear temporal logic} (LTL) so that properties can capture a notion of time.

Using LTL with PBT is not unheard of---Quickstrom testing
framework~\cite{oconnor_quickstrom_2022} has already shown that LTL properties
can be used for specifying and testing graphical user interfaces---but LTL
properties at this level of generality have not yet been attempted. We plan to
compile the user's LTL formulae to properties over traces that respect the
structure of the log and ignore updates to irrelevant variables. Then, the
developer can provide inputs to some system containing the sub-system under
test, and the test harness will apply the trace properties in real time. In this
way, the user can get feedback about the invariants that should hold of the
intermediate values of their programs, making PBT available in a huge range of
programs where it was previously too difficult to apply.

\SUBSECTION{Models for Modules}{sec:automating}{2}{3}{Harry}
One finding that has surprised us from the Jane Street study is that
it is {very} common for developers to build (or already to have) a
{\em model
implementation} of the code they are testing and check that the two versions of
the same code produce the same results.  This is a well-documented approach to
PBT~\cite{hughes_experiences_2016}, but it is not supported as well as it could
be by existing tooling.
%
With this in mind, we propose comprehensive tooling to entirely automate
model-based testing for a large class of ML-style software
{\em modules}~\cite{macqueen_modules_1984}.

\begin{figure}[t]
  \begin{minipage}{.45\textwidth}
\begin{lstlisting}
module StringFns : sig
  val reverse : string -> string
  val drop_n  : int -> string -> string
  val split   : string -> string * string
end
\end{lstlisting}
  \end{minipage}
  \qquad\qquad
  \begin{minipage}{.45\textwidth}
\begin{lstlisting}
module Set : sig
  type 'a t     val empty : 'a t
  val mem   : 'a t -> 'a -> bool
  val add   : 'a t -> 'a -> 'a t
end
\end{lstlisting}
  \end{minipage}
  \caption{Some module interfaces we would like to test
    automatically.}\label{fig:sigs}
\end{figure}

Model-based testing is trivial in the simplest case (comparing two pure
functions), but if the code under test is a {\em collection} of functions,
organized into a module, things get more interesting. Testing even a simple
module requires a complex orchestration of calls to the different functions in
the module's signature. For example, testing \lstinline{StringFn} (in
Figure~\ref{fig:sigs}) correctly requires wiring up the functions in the
signature to satisfy their types: \lstinline{drop_n} needs a randomly generated
integer to know how much to drop, \lstinline{split} produces a pair of strings
that can each be arguments to future function calls, etc. We will build on prior
work~\cite{hughes_experiences_2016} to correctly generate graphs of function
calls that are well-typed and that can be used to compare module implementations.

Testing modules gets even hairier when they contain {\em abstract types}, as is
the case with \lstinline{Set}. Now, some of the types in the signature cannot be
generated on their own, so they must be built up from scratch (in this case by
calling \lstinline{create} and \lstinline{add}). The \lstinline{Set} module is
also {\em polymorphic}, so a type must be chosen for \lstinline{'a} before
testing. (There is significant work on this
problem~\cite{hou_favonia_logarithm_2022}, but it is not solved in general.)
To solve these problems, we will take a theoretical approach, breaking down
modules theoretically and developing automation heuristics from first
principles. This process will necessarily involve carving out some subset of
modules where automation is not possible, but highlighting that subset will
provide useful context for future work in PBT usability.

Finally, so far we have only talked about comparing implementations of the same
module signature, but what about implementations of {\em related} signatures? In
theory, it should be possible to automate test harnesses when two modules share
some subset of behaviors, or when the concrete types that they operate on are
related in some known way. We plan to work with users to understand the
important use-cases here and get the automation right.

\proposecut{With the prevalence of ML-derived languages used in practice today, and with ML
modules' similarities to other tools for defining interfaces, this project has
the potential to significantly increase the usability of model-based PBT.}

\SECTION{Generation}{Better Tools for Random Inputs \pagebudget{3}}{sec:gen}
%
The Jane Street study highlighted several \gentheme{generation themes}.
Generators were regularly cited as one of the most challenging parts of the PBT
process---the existing tools for random generation are varied and powerful, but
they are not very usable.  In this section, we discuss {\em reflective
generators}, an abstraction for random generation, currently under development,
that exposes levers for
automation that enable a host of new approaches to generator
tooling (\sectionref{sec:reflective}). Reflective generators enable new
approaches to shrinking (\sectionref{sec:shrinking}) and fuzzing
(\sectionref{sec:fuzzing}) that reduce developer effort and improve testing
effectiveness.

\subsectionstar{Context: Random Generation is Hard}
As discussed in the Orientation section above, PBT relies heavily on {\em random data
generators}. The inputs produced by the generator exercise the developer's
properties and give confidence that they hold---provided the inputs are
interesting enough.
%
However,
many properties that developers want to test have {\em preconditions}
(or {\em validity conditions} or {\em input constraints}) that
restrict the set of inputs that should be used for testing. This comes up often
when testing data structures with invariants that must hold in order to apply
the operations under test. Testing such properties can be problematic, because
many preconditions are difficult to satisfy randomly; if the developer is not
careful they may waste most of their testing time generating and
discarding invalid inputs.
%
Worse, even among valid inputs, not all are equally realistic. Often generated inputs
feel fabricated, since they are not constructed with prior knowledge of the
kinds of inputs that the program is likely to see.
%
Worse yet, even with validity and realism accounted for, there are
other ways for generators to under-perform---for example, by
generating many overly similar inputs while ignoring large parts of
the input space.

% The PBT literature partially addresses these concerns, with solutions solving
% one problem at the expense of another and trading off between programmer effort
% and generator power.

Existing approaches to these issues
fall on a spectrum from automatic to manual. The automatic approaches use
proxies for validity and general ``interestingness'' of inputs: some, like {\em
fuzzers}~\cite{afl-readme}, optimize readily available metrics like code
coverage, others ask users to provide metrics~\cite{loscher2017targetedpbt}, and
naturally some use machine learning to infer proxies for
validity~\cite{godefroid2017learn, DBLP:conf/icse/ReddyLPS20}. These approaches
are easy to apply and can obtain good distribution coverage, but they are rarely
sufficient for testing properties with complex preconditions. Slightly more
manual approaches are based on declarative representations of validity
conditions: for preconditions that are primarily structural, {\em grammar-based
fuzzing} provides a compelling solution~\cite{godefroid2008grammar,
holler2012fuzzing, veggalam2016ifuzzer, wang2019superion,
srivastava2021gramatron}, and for more complex, semantic preconditions, some
have proposed using SMT-solvers~\cite{dewey2017automated, LuckPOPL,
steinhofel2022input} to automatically seek out valid inputs. These tools are
much better at satisfying up to moderately-complex properties, but some are
still out of reach. The semi-automatic
category also includes tools for {\em example-based tuning}, a process that
improves realism of inputs by mimicking user-provided
examples~\cite{soremekun2020inputs}; the existing tools successfully generate
more realistic inputs, but are again limited in the preconditions they can
satisfy.

\subsectionstar{Context: Monadic and Free Generators}
\iflater\hg{Not sure I love this transition... I liked it better when this sentence was above}\fi
The most manual, but also the most flexible, solutions use hand-written
generators, written in a convenient domain-specific language (DSL).
In  Haskell, where PBT was
first popularized, many such DSLs are implemented using {\em
monads\/}~\cite{moggi1991notions}, an elegant design pattern for
expressing effectful (in this case, random and stateful) computations
in a pure, stateless underlying
language. While monadic DSLs are not actually necessary to express generators in
non-pure languages, some libraries (e.g., in OCaml) still use monadic
abstractions to build their generator DSLs.

Monadic generators can implement random data producers of arbitrary complexity
(e.g., it is possible to write a monadic generator for Haskell
programs~\cite{palka_testing_2011}), so they are often more expressive than
other representations like grammar-based generators.  Yet monadic generators are
syntactically constrained in a way that isolates the probabilistic code and
prevents usage errors (like passing the wrong random seed around). As we will
see, the constrained nature of monadic generators also makes them the perfect
candidates for sophisticated manipulations and interesting generalizations.

In order to improve monadic generation along the dimensions listed
above, it helps to re-frame generators as {\em parsers of randomness}. A generator
operates by making a series of random choices, but we can equivalently think of
it as being provided some random sequence of choices and then simply following
those choices to produce a value. This perspective has been used in a few
implementations of PBT
systems~\cite{maciver2019hypothesis, dolan2017testing}; we made it
formal in our paper, {\em Parsing Randomness}~\cite{goldstein2022parsing}.
%
This paper introduced {\em free generators}, which generalize the standard
monadic generator abstraction, demonstrate a formal link between parsing and
random generation, and enable new algorithms for generation that improve
generation modulo validity constraints. Free generators are written the same
way as standard monadic generators, but in reality they are more like
generator ``plans'' or syntax trees.%
\footnote{For experts: Free generators are implemented using {\em freer
monads}~\cite{kiselyov2015freer}, which have been used to great effect in recent
years to capture the structure of effectful computations
(cf.~ITrees~\cite{old:xia2019interaction}). Freer monads represent
monadic computations syntactically by reifying the monad operations
(\lstinline{return} and \lstinline{>>=}) as data constructors. Critically, this
is all implemented within the language (no macros or AST
manipulation). See \cite{goldstein2022parsing} for more details on the
free generator representation.}
In {\em Parsing Randomness} we proved that any free
generator can be interpreted
either as a standard monadic generator or as a source of
random choice strings with a parser over those strings; this formalizes the
relationship between generators and parsers.

\SUBSECTION{Reflective Generators}{sec:reflective}{1}{2}{Harry}
Building on the free generator work described above, we have begun to look at
yet more powerful generalizations of the standard monadic approach to random
generation. Our most promising ongoing project concerns monadic generators that
can be run {\em backward}.

If a generator parses a sequence of choices into a value, then running the
generator backward should take a value and produce a sequence of choices that
would produce that value. With this in mind, we have begun working on {\em reflective
generators}, an extension of monadic generators that can be run backward to
``reflect'' on the choices that they made when producing an input. The machinery
that makes reflective generators work is quite complex,%
\footnote{Reflective generators are both monads and {\em
    partial profunctors},
implementing bidirectional programming in the style of Xia et
al.~\cite{xia2019composing}. This approach to bidirectional programming is
related to lenses~\cite{foster2009bidirectional}, but it hides much of the
complexity of bidirectional program composition in the bind operation of the
monad. The result is an elegant programming experience where both directions of
the computation can be written at once, in a type-safe way.}
but, like free generators, their syntax is still close to that of normal
monadic generators.

Note that the backward direction of reflective generators is different from
remembering the choices the generator makes as it goes (which is already
commonly done~\cite{maciver2019hypothesis,hatfield-dodds_hypofuzz_nodate}). For
one, the generator can reflect on choices for inputs it did not produce itself,
and for another the choices can be given different structures (bitstrings,
higher-level choice strings, choice trees, etc.) depending on the needs of the
caller.

The bidirectional nature of reflective generators makes them
useful for more than just generation!
We already know of two other applications...

{\em Validity-Preserving Mutation.}
Many automated testing algorithms
(especially fuzzing algorithms~\cite{afl-readme}) {\em mutate} values to explore
the behavior of the program in a space ``around'' those values. This can be
difficult in PBT scenarios, where values are subject to complex validity
constraints, since mutation often produces invalid values. Reflective
generators can help: we can (1) reflect on the choices that lead to a particular
value, (2) mutate those choices, and (3) re-run the generator with the new
choices {\em while correcting any choices that
would lead to an invalid value.} Figure~\ref{fig:mutation} shows the way this
algorithm is able to mutate a binary search tree, while maintaining validity,
using no BST-specific code beyond the reflective generator itself.
\begin{figure}[h]
  \centering
  \includegraphics[width=.6\textwidth]{assets/mutate-diagram.pdf}
  \vspace{-2mm}
  \caption{Validity-preserving mutation of a binary search tree, maintaining the
  BST invariant.}\label{fig:mutation}
\end{figure}

{\em Example-Based Tuning.} Earlier we pointed out that good generators
produce ``realistic'' inputs; one way to ensure this is to tune the generator so
it produces values that are similar to some user-supplied values deemed
realistic. Existing tools make good use of this example-based approach to
tuning~\cite{soremekun2020inputs}, but they do not work with generators as
powerful as monadic generators. We implement a similar algorithm using
reflective generators: we can (1) again, reflect on the choices that lead to a
set of realistic values, and (2) run the generator with {\em new choice weights}
informed by the choices that we saw.

Both of these applications have the potential to significantly improve
testing effectiveness---example-based tuning helps users generate more
realistic inputs, while validity-preserving mutation enables more
automated approaches to improving generator distributions---and we get
{\em both} by upgrading our existing generators to reflective ones. We
will see some additional use cases for reflective generators in the
following sections.

\SUBSECTION{Reflective Shrinkers}{sec:shrinking}{2}{2}{Harry}
%
Reflective generators' ability to run backward means that they can be used as
part of any algorithm that needs to understand the structure of generated
inputs.  We therefore propose using them to implement validity-preserving {\em
shrinking} of values to find smaller counterexamples and speed up debugging
without any more effort from the user. On its face, this feels similar to
validity-preserving mutation: Can we reflect on choices, shrink the choices, and
then re-run the generator with the smaller choices? Likely yes! But there are
complications.

Shrinkers need to be more careful than mutators to keep shrunk values faithful
to the original. When mutating, it is often fine if the mutated value is
accidentally quite different from the original value, since the mutator is
trying many values and any that catch a bug are equally good. But a rogue
shrinker runs the risk of confusing the developer much more than they already
were. For example, if a reflective shrinker accidentally creates a visually larger
value, for example, because the relationship between choice sequence length and
output value size is not monotonic, the shrinker might actually make it more
difficult to find a bug. Additionally, if a reflective shrinker's shrunk choices
produce a values that are very different from the original, they may fail to
shrink at all, instead just stepping to values that are no longer
counterexamples and concluding that the original value is a local minimum.

In order to establish reflective shrinkers as a once-and-for-all solution to
validity preserving shrinking, we plan to formalize and prove the above
properties. Specifically, we want to show that, for some class of reflective
generators, the associated reflective shrinkers (1) never produce larger values
with fewer choices and (2) always produce shrunk values that are ``close'' to
the original. Formalizing sub-classes of well-behaved reflective generators and
notions of ``shrinker closeness'' will also give valuable insights into other
applications of reflective generators and shrinkers.

We mentioned in the motivation section that participants in the study thought of
shrinking as magical, but currently the abstraction is too leaky for them to
be comfortable ignoring it.  Our hope is that eventually, having a reflective
generator on hand will mean that shrinking, even in the face of complex
preconditions, is {\em actually magic}, requiring no developer intervention and
simply making counterexamples smaller and easier to understand.

\SUBSECTION{Reflective Fuzzers}{sec:fuzzing}{2}{3}{Harry}
{\em Fuzzers} like AFL~\cite{afl-readme} use principles that are similar to the
ones behind PBT: they leverage randomized testing to quickly exercise as many
program behaviors as possible. Fuzzers are compelling because they are
inherently easy to use. The developer need only point the fuzzer at a binary and
wait for it to find bugs. But without help, fuzzers are not very good at finding
bugs in programs with complex preconditions.

There are a rew existing projects that try to get the best of both worlds by
combining PBT and fuzzing.
For example, the FuzzChick library in Coq~\cite{OLDlampropoulos19fuzzchick}
uses code coverage as guidance for PBT and the HypoFuzz library uses a
similar approach in Python~\cite{hatfield-dodds_hypofuzz_nodate}. These projects
are demonstrably powerful, but neither benefits from the years of expertise
poured into industrial-strength fuzzers; Crowbar
does~\cite{dolan2017testing}. Crowbar uses
AFL~\cite{afl-readme}, one of the best-established
fuzzers, to generate random bit-strings that are later parsed into program
inputs. Crowbar does require more user effort than standard fuzzing techniques,
but the coverage-guidance means that careful tuning is often not required to get
good testing performance.

We like the model of Crowbar a lot, but think it does not quite go far enough;
we intend to replicate a version of Crowbar built on reflective generators that
is even more powerful.
We start with a classic fuzzing setup, attempting to make the system under test
crash by passing it a variety of semi-random inputs. Normally, the fuzzer is
working against the parser, in the sense that the parser's job is to reject
invalid inputs and the fuzzer's job is to ``get past the parser.'' Crowbar's
generators avoid this adversarial relationship and subsume grammar-based
generation with a powerful generator that is powerful enough to satisfy the
parser by construction; ours will as well, but with reflective generators.

Why use a reflective generator? First, we should clarify why any kind of monadic
generator is preferable to grammar-based options. Monadic generators can
straightforwardly generate context-free structures, and they can often do so
automatically with the help of type information~\cite{mista2019deriving}. If
this is all that the precondition requires, then there is no harm in using a
monadic generator, rather than a grammar-based one. But the beauty of a monadic
generator is that it can be made far more powerful, incrementally, as the
developer's testing needs change. The developer can start off thinking that they
need only consider the structure of their inputs, but they can later add more
semantic guarantees if they determine that their testing is ineffective.

Focusing on reflective generators specifically, one compelling benefit is that
their backward interpretation can be used to help seed the fuzzer.  Most fuzzers
ask for a number of {\em seeds}, input examples that the fuzzer can start from,
in order to ensure that the fuzzer does not spend ages exploring
inputs that have no hope of working out. Normally these seeds are easy enough
for the user to write down, since they are simply program inputs, but now that
we are asking the fuzzer to generate sequences of choices it becomes much more
error-prone (and tedious) to produce seeds by hand.  This is one great use for a
backward interpretation. The user can write down their seeds---either as values
in the program, or as text that can be parsed by the program's parser---and then
the reflective generator can reflect on the choices that produce those seeds.

The reflective generator also provides validity-preserving mutation. Guided by
heuristics, the system can opt to supplement the fuzzer's mutation schedule with
mutations that are obtained by the reflective generator's validity-preserving
mutation (which is more targeted than the mutators provided by the fuzzer). We
expect this to have a significant impact on performance, especially in contexts
where preconditions are relatively sparse and therefore hard for AFL to mutate
correctly.

Our ultimate goal is a unification of PBT and fuzzing tooling that combines the
powerful automation potential of reflective generators with the usability of
fuzzers. Testers will be able to test properties with complex preconditions
without giving up the power and simplicity of coverage-guided generation.

% \SUBSECTION{Benchmarking}{sec:benchmarking}{1}{3}{Other}
% \iflater\todo{Make sure this feels different from Leo's CAREER}\fi
% \bcp{Maybe we should just cut it!}
% %
% The many papers in the PBT literature demonstrate effectiveness with case
% studies, showing that certain bugs in certain systems are caught more quickly
% with one too over another. For theoretical advances, this is often sufficient
% to demonstrate that the paper is worth publishing, but this kind of evaluation
% can be hard to interpret from the perspective of a would-be user. With all of
% the new approaches to generation that we are proposing in this document, and
% considering our goals around usability, we want to do better.

% We will to develop and popularize a robust empirical evaluation framework for
% generators and other PBT techniques. Our first contribution will be an
% infrastructure for easily and extensibly running experiments.  By ``easily,'' we
% mean that we will take on the burden of collecting data and analyzing the
% results, exposing to the user library functions for their particular
% instantiations as needed. We will evaluate a given tool based on (1) the degree
% to which it is able to achieve high code coverage quickly, and (2) the speed
% with which it finds bugs that have been pre-seeded in example programs. By
% ``extensibly,'' we mean that in addition to the two languages (Haskell and
% OCaml/Coq), multiple frameworks (QuickCheck, SmallCheck, QuickChick, etc.), and
% numerous workloads that we plan to support on release, we will design the
% infrastructure so that users can easily add new things along each dimension.

% Our second contribution will codify a library of case-studies and examples as
% {\em benchmarks for PBT}. Similar suites of benchmarks already exist in the
% fuzzing literature~\cite{hazimeh_magma_2021}, but those benchmarks are not
% organized around the particular challenges that PBT tools face. In particular,
% few of the benchmarks deal with the kinds of complex preconditions that PBT
% tools are built to handle. We want to establish a set of challenging tasks that
% can serve as a north star for future improvements to PBT generators and
% bug-finding strategies (including our own!).

% Designs for this project are currently being discussed with Leonidas
% Lampropoulos and his group at the University of Maryland. PI Pierce has a long
% history of successful projects with Prof.
% Lampropoulos~\cite[etc.]{LuckPOPL,goldstein2021dojudgeatest,lampropoulos_coverage_2019,Lampropoulos&18,OLDlampropoulos19fuzzchick}.
% \iflater
% \hg{Is this everything? Probably not...}
% \fi

\SECTION{Validation}{Understanding Testing Effectiveness \pagebudget{3}}{sec:val}

Another major challenge in maximizing PBT's usability helping developers
\evaltheme{validate} that their testing is effective. In this section, we
describe a sequence of research efforts to design and evaluate usable developer
tooling for PBT. These projects will contribute new paradigms
for tools that help developers assess whether their generators are
generating sufficient and appropriate inputs
(\sectionref{sec:evaluating_distributions}) and whether those inputs
sufficiently exercise their code (\sectionref{sec:tuning}), migrate failing
property-based tests into regression tests (\sectionref{sec:counter}),
and understand testing-provoked failures involving complex
inputs (\sectionref{sec:failures}). These
projects will be pursued using human-computer interaction methodology,
integrating these tools into contemporary interactive development
environments.
\proposecut{The result of this work will be an
innovative set of design primitives for helping developers get work done in the
challenging setting of reasoning about tests with an overabundance of input-output
examples.}

\SUBSECTION{Evaluating Data Distributions}{sec:evaluating_distributions}{2}{3}{Other}
%
In comparison to testing techniques like unit testing, PBT focuses on testing
distributions of inputs, rather than individual inputs. The success
PBT depends on a developer's ability to create a distribution
of input data that is sufficiently realistic and comprehensive. We plan
to develop much-needed tooling around
understanding distributions of input data and more easily changing them.
This was a pain point of participants in our study, many of whom
did not know the kinds of input their generators were producing.

We draw inspiration from related work in HCI that has sought to better expose
the shape of input data distributions including
machine learning datasets
(e.g.,~\cite{ref:hohman2019gamut} and
~\cite{ref:hohman2020understanding}) and sequences of program values
(e.g.,~\cite{ref:kang2017omnicode}).
PBT poses a unique challenge because values produced by generators are
programmatically generated and
can be of unbounded structural
complexity (e.g., lists, trees, and other algebraic data types).
Consider an
example from a participant in our formative study, who wanted to generate
realistic logs of input data, where each log entry included at least a timestamp
and an event type. Such values are not trivially plotted in conventional
visualizations, and it would be prohibitive to review individual examples if the
logs are sufficiently long. Ideally, a developer would be able to answer
questions like: Are the generated log inputs long enough? Are the even sequences
realistic? This setting requires new kinds of views of data, and tight
developer support for easily defining meaningful views of the data.

To this end, we will design new interactive tools that provide rapid,
informative views of input data distributions. The tools will address the
challenges of visualizing generator distributions using a novel combination of
tailored, tried-and-true features for interactive programming environments.

First, the tool will support live, realtime displays of generated values.
Because PBT takes some time to run, our first goal is to provide
instant, live~\cite{ref:tanimoto1990viva} feedback on the generators. Building
in the tradition of other live functional programming environments
(e.g.,~\cite{tool:lighttable,ref:omar2019live}), our environment will provide
live feedback on the many inputs associated with the code, rather than a single
input. As the test runs, our tool will sample inputs output by the generator and
pipe them into data displays (Figure~\ref{fig:gen-vis}). These data displays
will first and foremost show aggregate data views, including aggregate
statistics (Figure~\ref{fig:gen-vis}.2), and visualizations of the distribution
of key features of the data (Figure~\ref{fig:gen-vis}.3). Visualizations will be
generated according to simple recommendation rules, similarly to other recent
exploratory data visualization tools from
HCI~\cite{ref:lee2021lux,wongsuphasawat_voyager_2016,
wongsuphasawat_voyager_2017}. Unique to our project, features to visualize will
be based on awareness of common features of alebraic data types, and extensible
through lightweight user-written code. Consider the \lstinline{log} type
described above. A developer might be interested in the log's
\lstinline{length}, field accessors like \lstinline{event_type}, \lstinline{id},
and \lstinline{timestamp}, filters like \lstinline{is_empty}, and even
aggregators like \lstinline{max_by}. These kinds of features can be generated
automatically for the most common data types.
% Then the tool will then use
% lightweight type-based program synthesis to compose and combine these functions
% to get features. It may choose to show the \lstinline{length} of a log, but also
% the \lstinline{max_by (fun l -> length l.payload)} (the maximum payload length),
% and even pairs of features like these (which could be viewed as a two
% dimensional feature).

\begin{wrapfigure}{r}{0.6\textwidth}
  \centering
  \includegraphics[width=0.6\textwidth]{assets/gen-vis.pdf}
  \caption{An envisioned tool for evaluating input data distributions.%
  \proposecut{(1) A developer invokes the tool from controls that appear directly
  next to their property in the editor. They are supported in evaluating the
  input data distribution with (2) aggregate statistics describing the generated
  values, (3) visualizations of distributions of key features of the input, and (4)
  example values, pretty-printed or represented with DOT graphs. The developer can define additional
  features and views in their functional code.}\hg{If we're going to call out
  each point in the text, we don't need to do it here too}}\label{fig:gen-vis}
\end{wrapfigure}

Second, the data displays will be easily extensible, providing lightweight hooks
for customizing aggregate data displays and previews. If
there are features that the user notices should be extracted, but that the
system cannot come up with itself (e.g., \lstinline{ids_unique}) the user can
write it themselves in companion code alongside their property specifications;
the interface will automatically load those features into the display.

Third, the interface will make it possible to drill-down into individual inputs.
In many cases, aggregate statistics will not be enough for understanding whether
the right kind of inputs are being generated. A developer will be able to access
sampled inputs from a list of samples. This list of samples can be interactively
filtered by selecting marks visualizations (e.g., a developer can choose a bar
for inputs of length ``10'' to preview individual inputs with that length). One
challenge will be to provide suitable representations of complex inputs that
will be easy to understand. The most general-purpose solution will be to
pretty-print solutions, provide interactive object browsers like those available
in JetBrains~\cite{tool:jetbrains}, and to allow a developer to explore an
object using a built-in REPL. Additionally, we will produce DOT
graph~\cite{ellson_graphviz_2002} representations of common kinds of inputs
(i.e., lists, trees) that will provide an at-a-glance understanding of inputs up
to dozens or hundreds of elements in length (Figure~\ref{fig:gen-vis}.4).

Finally, the tools will provide live feedback on completeness of the input
distribution in the form of in-situ coverage feedback. Like other HCI research
prototypes that have shown which lines of code are currently executing in a
running
program~\cite{ref:brandt2010rehearse,ref:oney2009firecrystal,ref:burg2013record},
developer's will be able to see lines of code in their editor colorized on the
basis of how frequently they have been executed while running the
tests. This will let a developer decide whether their generator is executing the
paths in the code that they believe are particularly error-prone.
\iflater
As the project evolves, and once ``Bringing Fuzzing into Focus'' from
\sectionref{sec:fuzzing} is complete, we even plan to provide users with
visualizations of code coverage feedback.
\hg{This feels weak here, let's see how
the end of the section ends up looking}
\fi
When complete, this project will be the first live tool for generator
feedback and analysis; in the next section, we take it one step farther to
consider how interactive tools can help a developer not just understand input
data distributions, but more directly change them.

\SUBSECTION{Tuning Data Distributions}{sec:tuning}{2}{3}{Harry}
%
What should developers do if their properties are testing the wrong
inputs? Tuning generators is a challenging task, often requiring significant
trial-and-error through an opaque process of changing generator parameters and
hoping that the input data distribution will be updated in the right way. This
process, however, often does not pay off. (Approaches like the one in
\sectionref{sec:fuzzing} can help, but manual tuning is still useful in cases
where fuzzing fails to find the developer's intended space of inputs.) Ideally,
developers could describe the kinds of inputs they want directly, rather than
tweaking generator parameters and hoping it leads to the right kinds of input
data distributions.

We will design tools to support the direct tuning of input data distributions
through manipulation of generated inputs and input data distributions. This
work will build on the foundation developed in the prior project
(\sectionref{sec:evaluating_distributions}). Inspired by recent tools in the PL/HCI literature
for bidirectional manipulation of programs and their
outputs~\cite{ref:hempel2019sketch,ref:kery2020mage,ref:omar2012active,ref:omar2021filling},
we explore the unique affordances of reflective generators in their ability to
support careful tuning of generators.

The first, general-purpose method for tuning input data distributions will be to
define filters on input data through interaction with aggregate data displays.
For instance, developers will be able to select ranges of values from a bar
chart showing input data features and then request that all values generated
within that range are discarded before testing. This is a particularly
compelling approach for its flexibility, though it is notably coarse-grained;
it does not influence the implementation of the underlying generator, and
therefore can only go so far in influencing the kinds of values that are
generated.

A second innovative method is to leverage the technology we have developed for
reflective generators (\sectionref{sec:reflective}) to change the underlying
generators. For some features, as a developer manipulates marks in the data
distribution visualizations, those manipulations can be mapped back to choices
in the generator (e.g., what value to place in a node, or the number of nodes to
produce, etc.). We envision building tools that show the generator code
side-by-side with visualizations, and where parameter choices in the generator
can update live as the data distribution is manipulated in the visualizations.
Furthermore, developers will be able to interact with individual data points,
expressing that they would like to see more inputs like one that has already
been generated, or that they would like an input similar to a generated input,
but different in a way that they have demonstrated. Together, this tool and the
tool for visualizing generated data distributions will have a synergistic effect
in improving developers' ability to understand and ultimately achieve more
realistic, comprehensive data distributions for their testing.

\SUBSECTION{Counterexamples as Regression Tests}{sec:counter}{2}{2}{Other}
%
After a developer has identified a failure with their tests, they
often wish to turn that failure into a regression test, to ensure that later
changes to the code will not reintroduce the failure. One pain point experienced
by informants in our interview study was that it requires considerable work to
transform a failure that was already detected by their PBT tools into a
regression test, despite the fact that much of the work involved in doing so
felt mechanical.

We plan to develop usable tooling for transforming failed PBT tests into single
regression tests. What is important to note about this project is that creation
of regression tests is \emph{mostly}, but not entirely mechanical. In reality,
the creation of regression tests will likely require judicious incorporation of
the developer's input at key decision points. This is particularly the case for
specifying acceptance criteria. For instance, consider a property that checks
that a list insertion function never produces an empty list. In the event of a
failure, a developer may want to produce a regression test checking the
exactness of the result on the failed input (e.g., checking that the insertion
produced a particular concrete list) rather than simply checking that the output
list is non-empty. The act of writing regression tests involve several such
choices, including whether to test for exact output, whether to test
intermediate results, and how to initialize inputs.

We will develop an interactive tool that assists developers in creating
regression tests from failed tests. The idea is to first develop
technology for generating sufficiently readable code for regression tests (using
approaches such as Daka et al.'s~\cite{ref:daka2015modeling}), and then provide
in-situ editing assistance along the lines of contemporary interactive
refactoring tools from the HCI
literature~\cite{ref:head2018interactive,ref:barik2016quick,ref:murphyhill2008refactoring,ref:lee2013draganddrop}.
For this unique task of transforming properties into regression tests,
our tool will provide the key features of keeping track of the failed input,
generating starter test code, substituting in correct expected values of the
output by executing corrected code, and then supporting developers in rapidly
performing likely edits to regression tests by providing suggestions to
alterations to their code like the ability to generate general property checks
with precise equality checks. We hope the results of this work can inform the
design of other approaches to counterexample extraction, including one that is
proposed for the Hypothesis library in Python~\cite{maciver2019hypothesis}.

% \ifdraft
% \SUBSECTION{Increasing Assurance with More Tests}{sec:more}{3}{4}{Other}

% The informants in our interview study admitted to heuristic
% approaches to determining how many tests to run. Many set
% rigid cutoffs such as one-thousand tests, or one minute's
% worth of tests. One reason for these heuristics was that
% developers did not want to lock up their computer's
% computational resources, or delay their other development
% tasks, by waiting for their property-based tests to
% complete. As one of our informants pointed out, this could
% lead to circumstances where bugs were not discovered until
% property-based tests were checked into the continuous
% integration system, when tests were finally conducted at a
% sufficient volume to generate failing inputs \amh{@HG can
% you fact check my paraphrase of this informant's input?}\hg{Technically this was
% a pilot study informant, but I think it's OK}.
% Unfortunately, when failures are detected in continuous
% integration, developers no longer have the context to
% address those problems easily, because they have likely
% moved on to other tasks.

% We believe that, if appropriately designed, a developer's
% PBT tools can help developers get the best of both
% world: they can both increase assurance that their software
% is correct by running more property-based tests, while not
% locking up their computer's resources. We propose to extend
% in-editor testing tools to permit options where developers
% can configure their PBT test drivers to continue testing
% software in the background by default, without requiring a
% developer's explicit input. These tools will scan a
% developer's environment for property-based tests, divide
% time proportionally between the various tests, and capture
% the results in a digested form. To reduce the likelihood of
% breaking a developer's focus, errors will be marked through
% subtle annotations in the code (e.g., icons in the line
% margins) when one has been detected. Developers can also
% request alerts that can report failures instantaneously,
% should they wish to address any discovered failures right
% away.

% \hg{This section needs honing; right now it's just what I wrote in my thesis
% proposal, and it's too high-level} \amh{Is this a user
% interface project devoted to figuring out ways of invoking
% and keeping tabs on long-running tests during solo
% programming? Or is it a project around proposing better
% software engineering process that gives the right amount of
% time to long-running PBT tests?}
% \hg{Good question. I kind of naively hoped it was a UI project that both helped
% users invoke and keep tabs of tests in the background AND pushed them towards
% software engineering processes that gave PBTs appropriate space to operate. But
% now that I think about it that seems like a lot at once. Honestly I think the
% cultural shift in SE (potentially backed by some large-scale software tools) is
% the more impactful angle, but I don't know if we can argue that we know how to /
% want to do that}
% \bcp{Delete the section?}
% \amh{I tried this section again, trying to spell out in more
% detail my updated conception about what would be useful and
% magical about this tool. Can someone else take a look at it
% and make the decision of whether to remove the section?}
% \hg{I think it's better, but it's still pretty vague and flat. I wish there was
% a specific example of an interaction that I could say ``oh yeah, why don't I
% have that?'' As written, it sort of feels like the devil is in the details and
% we didn't actually give an details}

% % It is likely that PBT tools could play a role in improving this state of
% % affairs. For example, one could take inspiration from some theorem
% % provers~\cite{berghofer2004random} and create a system in which properties are
% % checked locally but in the background, as the programmer works on other things.
% % This avoids waiting time while potentially being less frustrating than running
% % in CI, since bugs would likely be found while the programmer still had the code
% % ``paged in.'' Alternatively, one might design a PBT system with CI in mind,
% % providing automated features for deferring property failure notifications until
% % a specified time or turning failing properties into unit tests that can be saved
% % for future testing.

% \fi

\SUBSECTION{Interactive Shrinking and Debugging}{sec:failures}{3}{4}{Other}
%
One of the challenges to evaluating PBT results is understanding what a
counterexample means and why it triggers a bug.  We will design interactive
tools to help developers in this process.

First, we will design tools that build on
shrinkers~\cite{hughes_quickcheck_2007,arts_shrinking_2014} to help developers
understand counterexamples. Automatic shrinking, even when done via reflective
shrinkers as we discuss in \sectionref{sec:shrinking}, can be opaque, and
shrinkers often make suboptimal decisions that an individual developer might be
able to make better.
Drawing inspiration from approaches in recent HCI literature that support
interactive code reduction through iterative, incremental
experimentation~\cite{ref:lim2018ply,ref:head2018interactive,ref:holmes2012systematizing,ref:hibschman2016telescope},
we will design aids for rapid, incremental, interactive shrinking of complex
inputs into simpler inputs. The key feature we will develop is the ability to
shrink inputs semi-automatically by pruning the input's contents and structure in an interactive
object viewer, similar to the kinds of object viewers available in contemporary
debuggers like in the JetBrains IDE~\cite{tool:jetbrains}.
The interactive shrinker will display the \emph{valid} ways
in which an input can be pruned, but the choice of exactly what to prune will be
up to the human. We will rely on reflective generators to provide the insights
into which parts of the structure correspond to choices that can be pruned, and
these pruning points will be made
visible in the object viewer. As a developer prunes the input, they will receive
continuous feedback as to whether it still causes a test to fail or not.
Developers will ultimately be able to
reduce complex counterexamples into simpler ones that are
easier to reason about when looking for bugs.

% Developers will also be alerted if the execution path
% through the code has changed as a result of shrinking the input. In some
% circumstances, this will indicate an undesirable change in the shrunken input,
% and in other cases, such changes in execution paths may be permissible (e.g., if
% the change in the input has led to a reduction in the number of cycles through a
% loop).

% \amh{Could we indicate on top of an object explorer which
% parts of the structure, if changed, would end up leading to
% a different test result, or preserving it?}
% \hg{To a point... You'd run into exponential blowup pretty much immediately, but
% depending on the scale of the input it could be doable}

% Other related projects:
% Whyline~\cite{ref:ko2009finding}.

In addition to developing novel interaction techniques for simplifying inputs,
we will also develop systems for helping developers locate code that, if
changed, would resolve the failure. Rather than explicitly encoding
relationships between generated outputs and their dependencies on
code~\cite{ref:ko2009finding}, we will instead help the developer
understand where the execution paths of a given counterexample diverges from
successful yet very similar inputs. Leveraging the parametric nature of the
reflective generators, we will generate inputs in a space ``around'' a
counterexample and identify which ones no longer cause a failure.  Then, we will
execute the program up to the point where the traces of the programs begin to
diverge, and we will drop the programmer into a debugging environment where
they can query the state of the program and step through the remainder of the
execution. PI Head has prior work designing debugging tools that help
programmers understand trace divergences in an educational
settings~\cite{ref:suzuki2017tracediff}; the work of this project would be to
bring this technology into professional programming environments where traces
for similar inputs are abundant by the nature of reflective generators.

\SECTION{Education}{Advancing PBT in the Broader Culture \pagebudget{1}}{sec:ed}

Our goal with this project is to make PBT not only usable but {\em
  used}; \edutheme{education} will play a crucial role.  This
section describes three specific threads of activity addressing pedagogical
challenges in different domains:
course materials for instructors who want to integrate PBT into
undergraduate data structures courses (\sectionref{sec:1210}),
%
guidance for
industrial developers on how to identify low-hanging fruit (\sectionref{sec:whento}), and
%
a
tool for helping both developers and students write properties
interactively (\sectionref{sec:interactive}).

\SUBSECTION{PBT for Undergrads}{sec:1210}{1}{4}{Both}
%
\writeme{Write me.}

\begin{itemize}
\item One great way to increase industrial usage of PBT is to make it
part of the standard undergraduate curriculum.
\item There are several places where this could happen---compilers,
functional programming courses, intro courses---but we believe the
most promising place to start will be a data structures course.
(Partly because this has already happened for intro courses---cite
Brown folks, etc.)
\item Reasons:
  \begin{itemize}
  \item Data structures is often taken early in the curriculum, so
  habits can carry over to other courses even where PBT is not
  explicitly used.
  \item ``Obvious'' properties are often readily available (e.g.,
  algebraic specifications, inefficient reference implementations)
  \end{itemize}
\item Concrete plan:
\begin{itemize}
\item Create a model for how to do it by adding PBT to
the 1210 homework assignments (HW is of course the most important
place to put it) and creating some lecture material explaining it.
\item Write a cs-ed paper about how others can do it.
\end{itemize}
\end{itemize}

\ifdraft
\todo{Prior work~\cite{wrenn2021using,nelson2021automated} Shriram papers?}
\fi

\SUBSECTION{When to Specify It!}{sec:whento}{1}{2}{Harry}
%
PBT is often described as a lightweight formal method.  One
might therefore imagine that a central challenge of using PBT would be
coming up with specifications. Indeed, a preliminary interview
study before the larger Jane Street study seemed to indicate just
that~\cite{ref:goldstein2022some}. But at
Jane Street we actually heard very little about trouble
writing
properties; instead, most participants described applying PBT in scenarios when
properties were already available or quite easy to imagine. This suggests that,
while PBT educators should certainly teach developers how to
write properties, they should concentrate on
helping developers quickly recognize situations where PBT is a
natural fit because the properties are obvious!

The Jane Street study also gives us a solid start on a comprehensive list of
``no-brainer situations,'' where PBT is an obvious choice. For
example:\bcp{I think it would be easier to understand what we're
  saying if we listed the specifications themselves rather than (or in
addition to) the situations}\hg{I worry that will muddy the distinction between
WTSI and HTSI...}
\begin{enumerate*}[label=(\arabic{enumi})]
\item code that is already formally or semi-formally
specified,\bcp{still don't find this one compelling---when would it
  arise? isn't it kind of tautological?  (a function with a
specification has a specification...)}\hg{We're looking for cases where
properties are easy to find, right? Well they're {\em very} easy to find if you
already have them! This is essentially the QuickChick case---PBT is a great
choice because you already had to write down the spec for something else}
\item two functions (e.g., a parser and a printer) that should ``round-trip,''
\item a pure data structure (sets, maps, etc.) with obvious algebraic laws,
\item a module with a natural invariant,
\item related versions of the same code (e.g., one fast, one slow; one old, one
new),
and
\item programs that may fail catastrophically.
\end{enumerate*}
These patterns cover a range of
scenarios that real software developers regularly find themselves in, but they
are likely not exhaustive. To flesh out the list, we will continue to
gather examples; specifically, we will include
questions about natural use-cases on the survey of the Hypothesis user
community described in~\sectionref{sec:survey}; we will also mine
research papers and experience reports in the literature,
examine open-source software projects, and leverage our connections with various
PBT communities (QuickCheck/Haskell, Hypothesis/Python, Quickcheck/OCaml, etc.)
to ensure that we have a comprehensive understanding of the best uses for PBT

These results will be presented in a survey-style paper (with an
accompanying talk), tentatively entitled {\em When to Specify It!} in
homage to John Hughes's {\em How to Specify
  It!}~\cite{HowToSpecifyIt}, a famous tutorial on the many different
kinds of properties that can be written or a given piece of code.

% Clearly it is important to ask {\em How}, but our developer interviews
% suggest that it may be even more important to have a clear sense of {\em When}
% to use PBT. Our paper on {\em When to Specify It!} will combine examples from developer
% interviews with ones from our our years of experience studying and applying PBT
% into the definitive guide for the most impactful opportunities to apply PBT.

\SUBSECTION{Interactive Property Specification}{sec:interactive}{3}{4}{Other}
\hg{NEW! Thoughts? Feel free to just change it.}
The central idea of {\em When to Specify It!} is that developers should focus on
applying PBT in the highest-leverage scenarios because those scenarios have the
most obvious and useful properties. But what if, even when properties are
readily available, an inexperienced developer still has trouble seeing them?
This was a strong signal in the pilot study and a great opportunity for new
educational technologies.

We propose an interactive tool to help beginners find the properties in their
code. Prior research has demonstrated that automated tools can extract
specifications of a program's behavior~\cite{ref:ammons2002mining,
ref:le2018deep, ref:claessen2010quickspec}, but such tools have been primarily
evaluated in terms of their {\em abilities} rather than their {\em usability}.
Our tool will apply specification extraction in the context of education,
with the hope of demonstrating that these tools can help students use PBT in
their own code without direct guidance from instructors.

The tool will use a mixed-initiative approach~\cite{ref:allen1999mixed}, where
properties are determined in a ``conversation'' between the student and the
specification extractor.  The student might guide the extractor through
mechanisms such as (1) identifying regions of code that are likely to lead to an
adverse behavior such as an exception or a logical error; (2) providing unit
test cases that focus on a special case of a generalized property; and (3)
indicating aspects of interest on input and output data during exploration in a
debugging REPL.  If a generated property is too strict, the student will able to
mark a counterexample that was generated by the PBT tool as spurious; if too
relaxed, they can provide a counterexample that {\em should} be
marked as a failure.

We will develop this tool as a VSCode extension, and evaluate it across the
University of Pennsylvania CIS curriculum.  We will start by generating
candidate properties using QuickSpec~\cite{ref:claessen2010quickspec}, a tool
that builds on Haskell's QuickCheck library to synthesize a series of plausible
properties about a given function, and evaluating our tool in the context of the
main Haskell course, CIS 5520.
Future iterations will require us to either find tools in other languages used
commonly at Penn (e.g., Bach~\cite{smith_discovering_2017} in Python) or create
our own, with the ultimate goal of making specification extraction usable by any Penn
computer science student.

PI Head has extensive experience with designing and developing tools involving
program analysis~\cite{ref:head2018interactive,ref:head2019managing} and program
synthesis~\cite{ref:head2017writing} and with extending the VSCode
environment~\cite{ref:head2020composing}.
\bcp{There must be more work on automatically generating
  specifications, etc...  Need more citations...}

\immediate\closeout\workplanfile
\SIMPLESECTION{Plan of Work \pagebudget{.7}}{sec:plan-of-work}

% \begin{figure}[ht]
%   \centering
%   \vspace*{-1in}
%   \hspace*{-.4in}\includegraphics[width=1.1\textwidth]{assets/workplan.pdf}
%   \vspace*{-1.3in}
%   \caption{Plan of work.}\label{fig:workplan}
% \end{figure}

\begin{figure}[ht]
  \centering
\begin{ganttchart}[
      expand chart=\textwidth,
      y unit chart=.4cm,
      %   vgrid
    ]{1}{4}
% \gantttitle{Ongoing}{1}
  \gantttitle{Year 1}{1}
  \gantttitle{Year 2}{1}
  \gantttitle{Year 3}{1}
  \gantttitle{Year 4}{1}
  % \\ \ganttbar
  %      [bar/.append
  %         style={fill=green}]
  %      {\hbox to 3in{\bf Foundation \hfill \footnotesize Task}}
  %      {1}{3}
  \input{main.workplan}
\end{ganttchart}
  \caption{Plan of work.
   % \bcp{The timeline needs to be brought into correspondence with our
   %   actual expectations of timing, and explained.  Harry's projects
   %   should end by year 3 latest!  :-)}
   % \hg{How's that?}
   % \bcp{Hmm---now it looks like there's not much happening in year 4!
   % Maybe we need to remove this constraint.  Also, there's not much
   % blue at the moment, and I'm not certain how to talk about the
   % distinction between the two students' domains of work.  Can we try
   % to make it more coherent (even at the cost of some accuracy
   % wrt. our expectations about what you're going to do)?}
}\label{fig:workplan}
\end{figure}

The work of the project will be carried out by two PhD students, with
help from several undergraduates or masters students, over the course
of four years.  All the work will be collaborative across the group,
but the specific threads can be roughly divided into two broader
thrusts, one focused on PL techniques (advised by PI Pierce) and the
other emphasizing HCI methods and tools (advised by PI Head).

The PL thrust, likely led by PhD candidate Harrison Goldstein, will be
concentrated in years 1--3 (see the blue bars in
Figure~\ref{fig:workplan}).  This student will first finish the work on
reflective generators and write the {\em When to Specify It!} survey;
later on, they will consider reflective shrinkers and fuzzers,
collaborate with the other PhD student on tools for tuning data
distributions, and advise undergraduates and masters students on
languages for writing properties over logs and on automation for
testing ML modules.  (Goldstein has a proven track record of advising
and co-advising junior students---e.g.,~\cite{pytestmutagen}.)

The HCI thrust will likely be the responsibility of a new PhD student,
hired in year 1 or 2; this thread of work will thus be concentrated in
years 2--4 (shown in yellow in Figure~\ref{fig:workplan}). This thread
will include formative HCI research--- surveys to generalize our
motivating study and observations of PBT in practice---as well as
human-centered design projects including tools to validate data
distributions, interactive shrinking and debugging, and interactive
property specification. The second student will also co-advise an
undergraduate or masters project centered around turning
counterexamples into regression tests.

The work on incorporating PBT in the undergraduate curriculum (shown
in green) will involve both students equally and will evolve over the
course of the four year project.

\SIMPLESECTION{Broader Impacts of the Proposed Work \pagebudget{.5}}{sec:broader-impacts}

\paragraph*{Educational Benefits}
%
The most immediate broad impact of the proposed work will be the
development of educational materials. Educational activities will be
coordinated with the rest of the work, and they are integral to the
project's overall goal of making PBT a standard testing methodogy,
readily available to every industrial developer.  Specific pedagogical
threads within the project are described in~\sectionref{sec:ed}.

The majority of requested funding will support graduate students.  We
also plan to work with undergraduate students during this project;
they, too will benefit from the research experience. Each graduate
student will have leadership responsibility for multiple facets of the
project, including co-supervising interested undergraduate
researchers.

Our research group is already relatively diverse---PI Pierce has one
female Ph.D.{} student (who may, indeed, be interested in this
project) and PI Head has two---and we will continue pushing to make it
more so.  Pierce (along with Penn colleagues Zdancewic and Weirich)
also recently received funding for an NSF REU program that will
involve eight undergraduates in research, selected specifically with
an eye to diversity.  See the appendix on Broadening Participation in
Computing for more details.

\paragraph*{Benefits to Society}
%
The project's goals will also be served by open-source distribution of
all of the tools built during the project.  We hope to engineer these
to a standard that will make them directly useful to engineers and
students; more importantly, though, they will serve as models for
similar tools for other programming languages and environments.

The project also represents an excellent opportunity for collaboration
between university researchers and industrial advocates of PBT.  Our
ongoing user study at Jane Street has been carried out with the
enthusiastic support of their developers and management, and we hope
to continue using them as a testbed for prototypes of the tools we
will build.  Similarly, we are in active discussions with the
developers of the Hypothesis testing tool for Python, who are keenly
interested in the results of the survey study described in
\sectionref{sec:survey}.

Longer term, better testing means better software.  As software
systems have grown to the gigantic scale seen today, good testing
methodologies and tools (unit testing tools, test-first design
methods, etc.) have come to play an ever more crucial role.  Adding a
powerful new testing tool to programmers' arsenals will further boost
this part of the development process, leading to software of every
sort that is more robust, more reliable, and less expensive to build.


% \proposecut{See the Collaboration Plan for
% more.}

% The Project Description must contain, as a separate section within the narrative, a section labeled ``Broader
% Impacts of the Proposed Work". This section should provide a discussion of the broader impacts of the proposed
% activities. Broader impacts may be accomplished through the research itself, through the activities that are
% directly related to specific research projects, or through activities that are supported by, but are complementary to
% the project. NSF values the advancement of scientific knowledge and activities that contribute to the
% achievement of societally relevant outcomes. Such outcomes include, but are not limited to: full
% participation of women, persons with disabilities, and underrepresented minorities in science, technology, engineering, and
% mathematics (STEM); improved STEM education and educator development at any level; increased public
% scientific literacy and public engagement with science and technology; improved well-being of individuals in
% society; development of a diverse,globally competitive STEM workforce; increased partnerships between
% academia, industry, and others; improved national security; increased economic competitiveness of the United
% States; and enhanced infrastructure for research and education.

\SIMPLESECTION{Results from Prior NSF Support \pagebudget{.5}}{sec:prior}

% If any PI or co-PI identified on the project has received NSF funding (including any current
% funding) in the past five years, in formation on the award(s) is required,
% irrespective of whether the support was directly related to the proposal or not.
% In cases where the PI or co-PI has received more than one award (excluding amendments),
% they need only report on the one award most closely related to the proposal. Funding includes not just salary
% support, but any funding awarded by NSF. The following information must be provided:\\

\emph{\underline{PI Pierce}}: (NSF 1955565) ``Collaborative Research:
SHF: Medium: Bringing Python Up to Speed'' (\$437,999,
7/2020--6/2023), with co-PIs Michael Hicks (Maryland) and Emery Berger
(Amherst).
The project aimed to dramatically increase the performance and
correctness of applications written in Python by developing novel
techniques for performance analysis, optimization, run-time systems,
property-based random testing, concolic execution, and program
synthesis. It developed both
novel performance analysis tools and optimizations and novel automatic
testing frameworks. These were largely tailored to and implemented for
Python, but applicable in other, similar languages.
%
{\bf Intellectual Merit.} The project involved work on both
performance measurement (mostly at Amherst and Maryland) and PBT (mostly at Penn
and Maryland).  Specific threads of work involving Penn included
building an early
version~\cite{Frohlich2022} of the Reflective Generators described in
\sectionref{sec:reflective}, carrying out the pilot study of PBT in Python
mentioned in the motivation section
above~\cite{goldstein_problems_2022}, and building on the idea of
freer monads from functional programming to develop ``free
generators,'' which unify parsing and
generation~\cite{goldstein2022parsing}, presented a principled
automatic testing framework for application-layer
protocols~\cite{Li2021:MBToNA}, and developed and released a freely
available mutation testing framework for Python, called {\tt
  pytest-mutagen}~\cite{pytestmutagen}, and applied ideas from
combinatorial testing, a widely studied testing methodology, to modify
the distributions of random test-case generators so as to find bugs
with fewer tests~\cite{DBLP:conf/esop/GoldsteinHLP21}.
%
{\bf Broader Impacts.} Project results and open-source software
products are being iused to increase the
performance and correctness of Python applications.
Educational impact has included training both graduate and
undergraduate students, including a female PhD student at Penn, Jessica
Shi.
%
{\bf Publications (involving Penn).} \cite{Frohlich2022,DBLP:conf/esop/GoldsteinHLP21,
  goldstein2022parsing, goldstein_problems_2022, Li2021:MBToNA}.
{\bf Research Products (from Penn).} \cite{pytestmutagen}.

\emph{\underline{PI Head}} has not previously received NSF support.

% \SUBSECTION{Proposed Study}
% The Project Description should provide a clear statement of the work to be undertaken and must include:
% objectives for the period of the proposed work and expected significance; relation to longer-term goals of the PI's
% project; and relation to the present state of knowledge in the field, to work in progress by the PI under other
% support and to work in progress elsewhere.
%
% The Project Description should outline the general plan of work, including the broad design of activities to be
% undertaken, and, where appropriate, provide a clear description of experimental methods and procedures.
% Proposers should address what they want to do, why they want to do it, how they plan to do it, how they will
% know if they succeed, and what benefits could accrue if the project is successful. The project activities may be
% based on previously established and/or innovative methods and approaches, but in either case must be well
% justified. These issues apply to both the technical aspects of the proposal and the way in which the project may
% make broader contributions.

\ifdraft
\subsubsection*{BPC stuff}

Penn's plan is here: \url{https://docs.google.com/document/d/1HXriYvTZPZhuGBtlvJEgSAsP0ilRt4wChl88NriSTbY/edit#}

More materials are here: \url{https://drive.google.com/drive/folders/1a-yzunRx1mj1uHavtD9cbbPyCsFg3avz?usp=sharing}

Instructions are here: \url{https://www.nsf.gov/cise/bpc/}

bpc.net - source materials for NSF BPC plans (they also vet plans).

\fi